{
  "hash": "3119e946d1328c8b733ee9ff46330ea9",
  "result": {
    "markdown": "---\ndate: 2023-11-21\nlesson: Stat8\nthema: Clusteranalysen\nindex: 3\nformat:\n  html:\n    code-tools:\n      source: true\n---\n\n\n# Stat8: Lösung\n\n- Download dieses Lösungsscript via \"\\</\\>Code\" (oben rechts)\n- [Lösungstext als Download](Statistik_Loesungstext_8.1.pdf)\n\n## Musterlösung Aufgabe 8.1: Clusteranalysen\n\n### Übungsaufgabe \n\n*(hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)*\n\n- Ladet den Datensatz crime2.csv. Dieser enthält Raten von 7 Kriminatlitätsformen pro 100000 Einwohner und Jahr für die Bundesstaaten der USA.\n- Führt eine k-means- und eine agglomerative Clusteranalyse eurer Wahl durch. Bitte beachet, dass wegen der sehr ungleichen Varianzen in jedem Fall eine Standardisierung stattfinden muss, damit die Distanzen zwischen den verschiedenen Kriminalitätsraten sinnvoll berechnet werden können.\n- **Überlegt in beiden Fällen, wie viele Cluster sinnvoll sind** (k-means: z. B. visuelle Betrachtung einer PCA, agglomertive Clusteranalyse: z. B. Silhoutte-Plot).\n- Entscheidet euch dann für eine der beiden Clusterungen und vergleicht dann die\nerhaltenen Cluster bezüglich der Kriminalitätsformen und interpretiert die Cluster entsprechend.\n- Bitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu diesem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in das ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\n- Formuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\n- **Zu erstellen sind (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).**\n\n### Lösung\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"readr\")\n\ncrime <- read_delim(\"datasets/stat5-8/crime2.csv\", \";\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncrime\n## # A tibble: 50 × 8\n##    ...1  Murder  Rape Robbery Assault Burglary Theft Vehicle\n##    <chr>  <dbl> <dbl>   <dbl>   <dbl>    <dbl> <dbl>   <dbl>\n##  1 ME       2    14.8      28     102      803  2347     164\n##  2 NH       2.2  21.5      24      92      755  2208     228\n##  3 VT       2    21.8      22     103      949  2697     181\n##  4 MA       3.6  29.7     193     331     1071  2189     906\n##  5 RI       3.5  21.4     119     192     1294  2568     705\n##  6 CT       4.6  23.8     192     205     1198  2758     447\n##  7 NY      10.7  30.5     514     431     1221  2924     637\n##  8 NJ       5.2  33.2     269     265     1071  2822     776\n##  9 PA       5.5  25.1     152     176      735  1654     354\n## 10 OH       5.5  38.6     142     235      988  2574     376\n## # ℹ 40 more rows\n```\n:::\n\n\nIm mitgelieferten R-Skript habe ich die folgenden Analysen zunächst mit untransformierten, dann mit standardisierten Kriminalitätsraten berechnet. Ihr könnt die Ergebnisse vergleichen und seht, dass sie sehr unterschiedlich ausfallen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrimez <- crime\ncrimez[, c(2:8)] <- lapply(crime[, c(2:8)], scale)\ncrimez\n## # A tibble: 50 × 8\n##    ...1  Murder[,1] Rape[,1] Robbery[,1] Assault[,1] Burglary[,1] Theft[,1]\n##    <chr>      <dbl>    <dbl>       <dbl>       <dbl>        <dbl>     <dbl>\n##  1 ME        -1.38   -1.32     -1.05         -1.25       -0.939    -0.760  \n##  2 NH        -1.32   -0.853    -1.08         -1.32       -1.05     -0.945  \n##  3 VT        -1.38   -0.832    -1.10         -1.24       -0.591    -0.294  \n##  4 MA        -0.919  -0.287     0.467         0.398      -0.300    -0.970  \n##  5 RI        -0.948  -0.860    -0.212        -0.601       0.232    -0.466  \n##  6 CT        -0.630  -0.694     0.458        -0.508       0.00320  -0.213  \n##  7 NY         1.14   -0.232     3.41          1.12        0.0580    0.00774\n##  8 NJ        -0.456  -0.0452    1.16         -0.0766     -0.300    -0.128  \n##  9 PA        -0.369  -0.604     0.0908       -0.716      -1.10     -1.68   \n## 10 OH        -0.369   0.328    -0.000917     -0.292      -0.498    -0.458  \n## # ℹ 40 more rows\n## # ℹ 1 more variable: Vehicle <dbl[,1]>\n```\n:::\n\n\n„scale“ führt eine Standardisierung (z-Transformation) durch, so dass alle Variablen anschiessen einen Mittelwert von 0 und eine SD von 1 haben, ausgenommen natürlich die 1. Spalte mit den Kürzeln der Bundesstaaten. Anschliessend wird das SSI-Kriterium getestet und zwar für Partitionierungen von 2 bis 6 Gruppen (wie viele Gruppen man maximal haben will, muss man pragmatisch nach der jeweiligen Fragestelltung entscheiden).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"vegan\")\ncrimez.KM.cascade <- cascadeKM(crimez[, c(2:8)],\n  inf.gr = 2, sup.gr = 6, iter = 100, criterion = \"ssi\"\n)\nsummary(crimez.KM.cascade)\n##           Length Class  Mode     \n## partition 250    -none- numeric  \n## results    10    -none- numeric  \n## criterion   1    -none- character\n## size       30    -none- numeric\n\ncrimez.KM.cascade$results\n##       2 groups   3 groups   4 groups   5 groups  6 groups\n## SSE 174.959159 144.699605 124.437221 108.119280 95.316398\n## ssi   1.226057   1.304674   1.555594   1.539051  1.351146\ncrimez.KM.cascade$partition\n##    2 groups 3 groups 4 groups 5 groups 6 groups\n## 1         1        3        3        1        2\n## 2         1        3        3        1        2\n## 3         1        3        3        1        2\n## 4         2        2        1        3        6\n## 5         1        2        2        3        6\n## 6         1        2        2        3        3\n## 7         2        2        1        2        4\n## 8         2        2        1        3        6\n## 9         1        3        2        4        3\n## 10        1        2        2        4        3\n## 11        1        3        2        4        3\n## 12        2        2        1        2        4\n## 13        2        1        4        2        1\n## 14        1        3        3        1        2\n## 15        1        3        3        1        2\n## 16        1        3        3        1        2\n## 17        2        2        2        4        4\n## 18        1        3        3        1        2\n## 19        1        3        3        1        2\n## 20        1        3        3        1        2\n## 21        1        3        2        4        3\n## 22        1        2        2        4        3\n## 23        2        2        1        2        4\n## 24        1        3        2        4        3\n## 25        1        3        3        1        2\n## 26        1        2        2        4        3\n## 27        2        2        2        4        4\n## 28        2        2        4        2        4\n## 29        2        1        4        2        1\n## 30        1        3        2        4        3\n## 31        2        2        1        4        4\n## 32        1        2        2        4        3\n## 33        1        3        2        4        3\n## 34        1        3        2        4        3\n## 35        2        1        4        2        4\n## 36        2        2        4        5        5\n## 37        2        1        4        2        1\n## 38        1        3        3        1        2\n## 39        1        3        3        1        2\n## 40        1        3        3        1        2\n## 41        2        1        4        5        5\n## 42        2        1        4        5        5\n## 43        2        1        4        5        5\n## 44        1        3        3        1        2\n## 45        2        1        4        2        1\n## 46        2        1        4        5        5\n## 47        2        1        4        5        5\n## 48        2        1        4        2        1\n## 49        2        1        4        5        5\n## 50        1        3        2        1        3\n\n# k-means visualisation\nlibrary(\"cclust\")\nplot(crimez.KM.cascade, sortg = TRUE)\n```\n\n::: {.cell-output-display}\n![](Statistik8_Loesung_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nNach SSI ist die 4-Gruppenlösung die beste, mit dieser wird also weitergerechnet.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4 Kategorien sind nach SSI offensichtlich besonders gut\nmodelz <- kmeans(crimez[, c(2:8)], 4)\nmodelz\n## K-means clustering with 4 clusters of sizes 10, 9, 9, 22\n## \n## Cluster means:\n##        Murder        Rape     Robbery     Assault   Burglary      Theft\n## 1 -0.07699665 -0.02720392  0.17975176  0.08506429 -0.1091087 -0.5091183\n## 2  0.47973352  0.96904551  0.01436791  0.70492557  1.1511357  1.0830163\n## 3  1.31273822  0.94679752  1.65699740  1.27728944  0.9185274  0.6804530\n## 4 -0.69828542 -0.77138855 -0.76544570 -0.84957173 -0.7970855 -0.4900018\n##      Vehicle\n## 1  0.5329062\n## 2  0.2569235\n## 3  1.1463189\n## 4 -0.8162838\n## \n## Clustering vector:\n##  [1] 4 4 4 1 1 1 3 1 4 1 4 3 3 4 4 4 1 4 4 4 4 1 3 4 4 1 2 2 3 4 1 1 4 4 3 2 3 4\n## [39] 4 4 2 2 2 4 3 2 2 3 2 4\n## \n## Within cluster sum of squares by cluster:\n## [1] 28.83534 24.92365 27.50024 44.59027\n##  (between_SS / total_SS =  63.3 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n\n# File für ANOVA (Originaldaten der Vorfälle, nicht die ztransformierten)\ncrime.KM4 <- data.frame(crime, modelz[1])\ncrime.KM4$cluster <- as.factor(crime.KM4$cluster)\ncrime.KM4\n##    ...1 Murder Rape Robbery Assault Burglary Theft Vehicle cluster\n## 1    ME    2.0 14.8      28     102      803  2347     164       4\n## 2    NH    2.2 21.5      24      92      755  2208     228       4\n## 3    VT    2.0 21.8      22     103      949  2697     181       4\n## 4    MA    3.6 29.7     193     331     1071  2189     906       1\n## 5    RI    3.5 21.4     119     192     1294  2568     705       1\n## 6    CT    4.6 23.8     192     205     1198  2758     447       1\n## 7    NY   10.7 30.5     514     431     1221  2924     637       3\n## 8    NJ    5.2 33.2     269     265     1071  2822     776       1\n## 9    PA    5.5 25.1     152     176      735  1654     354       4\n## 10   OH    5.5 38.6     142     235      988  2574     376       1\n## 11   IN    6.0 25.9      90     186      887  2333     328       4\n## 12   IL    8.9 32.4     325     434     1180  2938     628       3\n## 13   MI   11.3 67.4     301     424     1509  3378     800       3\n## 14   WI    3.1 20.1      73     162      783  2802     254       4\n## 15   MN    2.5 31.8     102     148     1004  2785     288       4\n## 16   IA    1.8 12.5      42     179      956  2801     158       4\n## 17   MO    9.2 29.2     170     370     1136  2500     439       1\n## 18   ND    1.0 11.6       7      32      385  2049     120       4\n## 19   SD    4.0 17.7      16      87      554  1939      99       4\n## 20   NE    3.1 24.6      51     184      748  2677     168       4\n## 21   KS    4.4 32.9      80     252     1188  3008     258       4\n## 22   DE    4.9 56.9     124     241     1042  3090     272       1\n## 23   MD    9.0 43.6     304     476     1296  2978     545       3\n## 24   VA    7.1 26.5     106     167      813  2522     219       4\n## 25   WV    5.9 18.9      41      99      625  1358     169       4\n## 26   NC    8.1 26.4      88     354     1225  2423     208       1\n## 27   SC    8.6 41.3      99     525     1340  2846     277       2\n## 28   GA   11.2 43.9     214     319     1453  2984     430       2\n## 29   FL   11.7 52.7     367     605     2221  4373     598       3\n## 30   KY    6.7 23.1      83     222      824  1740     193       4\n## 31   TN   10.4 47.0     208     274     1325  2126     544       1\n## 32   AL   10.1 28.4     112     408     1159  2304     267       1\n## 33   MS   11.2 25.8      65     172     1076  1845     150       4\n## 34   AR    8.1 28.9      80     278     1030  2305     195       4\n## 35   LA   12.8 40.1     224     482     1461  3417     442       3\n## 36   OK    8.1 36.4     107     285     1787  3142     649       2\n## 37   TX   13.5 51.6     240     354     2049  3987     714       3\n## 38   MT    2.9 17.3      20     118      783  3314     215       4\n## 39   ID    3.2 20.0      21     178     1003  2800     181       4\n## 40   WY    5.3 21.9      22     243      817  3078     169       4\n## 41   CO    7.0 42.3     145     329     1792  4231     486       2\n## 42   NM   11.5 46.9     130     538     1845  3712     343       2\n## 43   AZ    9.3 43.0     169     437     1908  4337     419       2\n## 44   UT    3.2 25.3      59     180      915  4074     223       4\n## 45   NV   12.6 64.9     287     354     1604  3489     478       3\n## 46   WA    5.0 53.4     135     244     1861  4267     315       2\n## 47   OR    6.6 51.1     206     286     1967  4163     402       2\n## 48   CA   11.3 44.9     343     521     1696  3384     762       3\n## 49   AK    8.6 72.7      88     401     1162  3910     604       2\n## 50   HI    4.8 31.0     106     103     1339  3759     328       4\nstr(crime.KM4)\n## 'data.frame':\t50 obs. of  9 variables:\n##  $ ...1    : chr  \"ME\" \"NH\" \"VT\" \"MA\" ...\n##  $ Murder  : num  2 2.2 2 3.6 3.5 4.6 10.7 5.2 5.5 5.5 ...\n##  $ Rape    : num  14.8 21.5 21.8 29.7 21.4 23.8 30.5 33.2 25.1 38.6 ...\n##  $ Robbery : num  28 24 22 193 119 192 514 269 152 142 ...\n##  $ Assault : num  102 92 103 331 192 205 431 265 176 235 ...\n##  $ Burglary: num  803 755 949 1071 1294 ...\n##  $ Theft   : num  2347 2208 2697 2189 2568 ...\n##  $ Vehicle : num  164 228 181 906 705 447 637 776 354 376 ...\n##  $ cluster : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 4 4 4 1 1 1 3 1 4 1 ...\n```\n:::\n\n\n**Von den agglomerativen Clusterverfahren habe ich mich für Ward’s minimum variance clustering entschieden, da dieses allgemein als besonders geeignet gilt.**\n\nVor der Berechnung von crime.norm und crime.ch muss man die Spalte mit den Bundesstaatenkürzeln entfern.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"cluster\")\n\n# Add rownames to the dataframe\ncrime <- as.data.frame(crime)\nrownames(crime) <- crime$...1\ncrime2 <- crime[, -1]\n\n# Agglomerative Clusteranalyse\ncrime.norm <- decostand(crime2, \"normalize\")\ncrime.ch <- vegdist(crime.norm, \"euc\")\n# Attach site names to object of class 'dist'\nattr(crime.ch, \"Labels\") <- rownames(crime2)\n\n# Ward's minimum variance clustering\ncrime.ch.ward <- hclust(crime.ch, method = \"ward.D2\")\npar(mfrow = c(1, 1))\nplot(crime.ch.ward, labels = rownames(crime2), main = \"Chord - Ward\")\n```\n\n::: {.cell-output-display}\n![](Statistik8_Loesung_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Choose and rename the dendrogram (\"hclust\" object)\nhc <- crime.ch.ward\n# hc <- spe.ch.beta2\n# hc <- spe.ch.complete\ndev.new(title = \"Optimal number of clusters\", width = 12, height = 8, noRStudioGD = TRUE)\ndev.off()\n## png \n##   2\npar(mfrow = c(1, 2))\n\n# Average silhouette widths (Rousseeuw quality index)\nlibrary(\"cluster\")\nSi <- numeric(nrow(crime))\nfor (k in 2:(nrow(crime) - 1))\n{\n  sil <- silhouette(cutree(hc, k = k), crime.ch)\n  Si[k] <- summary(sil)$avg.width\n}\nk.best <- which.max(Si)\nplot(1:nrow(crime), Si,\n  type = \"h\",\n  main = \"Silhouette-optimal number of clusters\",\n  xlab = \"k (number of clusters)\", ylab = \"Average silhouette width\"\n)\n\naxis(1, k.best, paste(\"optimum\", k.best, sep = \"\\n\"),\n  col = \"red\",\n  font = 2, col.axis = \"red\"\n)\npoints(k.best, max(Si), pch = 16, col = \"red\", cex = 1.5)\n```\n\n::: {.cell-output-display}\n![](Statistik8_Loesung_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\nDemnach wären beim Ward’s-Clustering nur zwei Gruppen die optimale Lösung.\n\n**Für die Vergleiche der Bundesstaatengruppen habe ich mich im Folgenden für die k-means Clusterung mit 4 Gruppen entschieden.**\n\nDamit die Boxplots und die ANOVA direkt interpretierbar sind, werden für diese, anders als für die Clusterung, die untransformierten Incidenz-Werte verwendet (also crime statt crimez). Die Spalte mit der Clusterzugehörigkeit im Fall von k-means mit 4 Clustern hängt man als Spalte an (Achtung: muss als Faktor definiert werden!).\n\nAnschliessend kann man die 7 ANOVAs rechnen, die Posthoc-Vergleiche durchführen und die zugehörigen Boxplots mit Buchstaben für die homogenen Gruppen erzeugen. Sinnvollerweise gruppiert man die Abbildungen gleich, z. B. je 2 x 2. Das Skript ist hier simple für jede Verbrechensart wiederholt. Erfahrenere R-Nutzer können das Ganze hier natürlich durch eine Schleife abkürzen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"multcomp\")\npar(mfrow = c(3, 3))\n\nANOVA.Murder <- aov(Murder ~ cluster, data = crime.KM4)\nsummary(ANOVA.Murder)\n##             Df Sum Sq Mean Sq F value   Pr(>F)    \n## cluster      3  338.6  112.85   21.08 9.73e-09 ***\n## Residuals   46  246.2    5.35                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters <- cld(glht(ANOVA.Murder, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Murder ~ cluster, xlab = \"Cluster\", ylab = \"Murder\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Rape <- aov(Rape ~ cluster, data = crime.KM4)\nsummary(ANOVA.Rape)\n##             Df Sum Sq Mean Sq F value   Pr(>F)    \n## cluster      3   6213  2070.9   23.43 2.36e-09 ***\n## Residuals   46   4066    88.4                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters <- cld(glht(ANOVA.Rape, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Rape ~ cluster, xlab = \"Cluster\", ylab = \"Rape\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Robbery <- aov(Robbery ~ cluster, data = crime.KM4)\nsummary(ANOVA.Robbery)\n##             Df Sum Sq Mean Sq F value   Pr(>F)    \n## cluster      3 450920  150307   52.51 6.79e-15 ***\n## Residuals   46 131669    2862                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters <- cld(glht(ANOVA.Robbery, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Robbery ~ cluster, xlab = \"Cluster\", ylab = \"Robbery\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Assault <- aov(Assault ~ cluster, data = crime.KM4)\nsummary(ANOVA.Assault)\n##             Df Sum Sq Mean Sq F value   Pr(>F)    \n## cluster      3 680144  226715   38.75 1.21e-12 ***\n## Residuals   46 269160    5851                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters <- cld(glht(ANOVA.Assault, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Assault ~ cluster, xlab = \"Cluster\", ylab = \"Assault\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Burglary <- aov(Burglary ~ cluster, data = crime.KM4)\nsummary(ANOVA.Burglary)\n##             Df  Sum Sq Mean Sq F value   Pr(>F)    \n## cluster      3 5912868 1970956   33.51 1.23e-11 ***\n## Residuals   46 2705987   58826                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters <- cld(glht(ANOVA.Burglary, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Burglary ~ cluster, data = crime.KM4, xlab = \"Cluster\", ylab = \"Burglary\")\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Theft <- aov(Theft ~ cluster, data = crime.KM4)\nsummary(ANOVA.Theft)\n##             Df   Sum Sq Mean Sq F value   Pr(>F)    \n## cluster      3 12773961 4257987   13.12 2.55e-06 ***\n## Residuals   46 14924590  324448                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters <- cld(glht(ANOVA.Theft, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Theft ~ cluster, xlab = \"Cluster\", ylab = \"Theft\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Vehicle <- aov(Vehicle ~ cluster, data = crime.KM4)\nsummary(ANOVA.Vehicle)\n##             Df  Sum Sq Mean Sq F value   Pr(>F)    \n## cluster      3 1316376  438792   24.04 1.65e-09 ***\n## Residuals   46  839495   18250                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters <- cld(glht(ANOVA.Vehicle, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Vehicle ~ cluster, data = crime.KM4, xlab = \"Cluster\", ylab = \"Vehicle\")\nmtext(letters$mcletters$Letters, at = 1:6)\n```\n\n::: {.cell-output-display}\n![](Statistik8_Loesung_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nDie Boxplots erlauben jetzt auch eine Beurteilung der Modelldiagnostik: sind die Residuen hinreichen normalverteilt (symmetrisch) und sind die Varianzen zwischen den Kategorien einigermassen ähnlich. Mit der Symmetrie/Normalverteilung sieht es OK aus. Die Varianzhomogenität ist nicht optimal – meist deutlich grössere Varianz bei höheren Mittelwerten. Eine log-Transformation hätte das verbessert und könnte hier gut begründet werden. Da die p-Werte sehr niedrig waren und die Varianzheterogenität noch nicht extrem war, habe ich aber von einer Transformation abgesehen, da jede Transformation die Interpretation der Ergebnisse erschwert. Jetzt muss man nur noch herausfinden, welche Bundesstaaten überhaupt zu welchem der vier\nCluster gehören, sonst ist das ganze schöne Ergebnis nutzlos. Z. B. kann man in R auf den Dataframe clicken und ihn nach cluster sortieren.\n",
    "supporting": [
      "Statistik8_Loesung_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}