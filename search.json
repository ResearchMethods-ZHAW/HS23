[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods HS22",
    "section": "",
    "text": "Willkommen\nDas Modul „Research Methods” vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen” auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen”. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverarbeitung und Statistik).\nHier werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.\nDiese Website wurde am 2023-08-09 15:26:21 zum letzten Mal aktualisiert."
  },
  {
    "objectID": "PrePro.html",
    "href": "PrePro.html",
    "title": "Pre-Processing",
    "section": "",
    "text": "Die Datenkunde 2.0 gibt den Studierenden das Wissen und die Fertigkeiten an die Hand, selbst erhobene und bezogene Daten für Ihre eigenen Analysen vorzubereiten und anzureichern (preprocessing). Die Einheit vermittelt zentrale Datenverarbeitungskompetenzen und thematisiert bekannte Problemzonen der umweltwissenschaftlichen Datenverarbeitung – immer mit einer „hands-on” Perspektive auf die begleitenden R-Übungen. Die Studierenden lernen die Eigenschaften ihrer Datensätze in der Fachsprache korrekt zu beschreiben. Sie lernen ausserdem Metadaten zu verstehen und die Implikationen derselben für ihre eigenen Analyseprojekte kritisch zu beurteilen. Zentrale Konzepte der lesson sind Skalenniveaus, Datentypen, Zeitdaten und Typumwandlungen.\nDie Lesson vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape”, „split-apply-combine”). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser lesson lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden.\n\n\n\n\n\n\n\n\n\n\nTitel\n\n\nDatum\n\n\nLesson\n\n\nThema\n\n\n\n\n\n\nVorbereitung\n\n\n2022-10-11\n\n\nPrePro1\n\n\nVorbereitung\n\n\n\n\nPrepro 1: Demo\n\n\n2022-10-11\n\n\nPrePro1\n\n\nDatentypen\n\n\n\n\nPrePro 1: Übung\n\n\n2022-10-11\n\n\nPrePro1\n\n\nDatentypen\n\n\n\n\nPrepro 2: Demo\n\n\n2022-10-17\n\n\nPrePro2\n\n\nPiping / Joins\n\n\n\n\nPrepro 2: Übung A\n\n\n2022-10-17\n\n\nPrePro2\n\n\nPiping / Joins\n\n\n\n\nPrepro 2: Übung B\n\n\n2022-10-17\n\n\nPrePro2\n\n\nPiping / Joins\n\n\n\n\nPrepro 3: Demo\n\n\n2022-10-18\n\n\nPrePro3\n\n\nSplit-Apply-Combine\n\n\n\n\nPrepro 3: Übung\n\n\n2022-10-18\n\n\nPrePro3\n\n\nSplit-Apply-Combine\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "prepro/Prepro1_Vorbereitung.html",
    "href": "prepro/Prepro1_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "Im Rahmen von Prepro 1 - 3 werden wir folgende Packages brauchen: dplyr, ggplot2, lubridate, readr und tidyr. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Einzelne Packages werden typischerweise folgendermassen installiert:\n\ninstall.packages(\"dplyr\")    # Anführungs- und Schlusszeichen sind zwingend\ninstall.packages(\"ggplot2\")\n...                          # usw.\n\nMit nachstehendem Code werden alle noch nicht installierten packages automatisch installiert.\n\n\nipak &lt;- function(pkg){\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if(length(new.pkg)){install.packages(new.pkg, dependencies = TRUE)}\n}\n\npackages &lt;- c(\"dplyr\", \"ggplot\", \"lubridate\", \"readr\", \"tidyr\")\n\nipak(packages)"
  },
  {
    "objectID": "prepro/Prepro1_Demo.html#footnotes",
    "href": "prepro/Prepro1_Demo.html#footnotes",
    "title": "Prepro 1: Demo",
    "section": "",
    "text": "ordered = T kann nur bei der Funktion factor() spezifiziert werden, nicht bei as.factor(). Ansonsten sind factor() und as.factor() sehr ähnlich.↩︎"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#arbeiten-mit-rstudio-project",
    "href": "prepro/Prepro1_Uebung.html#arbeiten-mit-rstudio-project",
    "title": "PrePro 1: Übung",
    "section": "Arbeiten mit RStudio “Project”",
    "text": "Arbeiten mit RStudio “Project”\nWir empfehlen die Verwendung von “Projects” innerhalb von RStudio. RStudio legt für jedes Projekt dann einen Ordner an, in welches die Projekt-Datei abgelegt wird (Dateiendung .Rproj). Sollen innerhalb des Projekts dann R-Skripts geladen oder erzeugt werden, werden diese dann auch im angelegten Ordner abgelegt. Mehr zu RStudio Projects findet ihr hier.\nDas Verwenden von Projects bringt verschiedene Vorteile, wie zum Beispiel:\n\nFestlegen der Working Directory ohne die Verwendung des expliziten Pfades (setwd()). Das ist sinnvoll, da sich dieser Pfad ändern kann (Zusammenarbeit mit anderen Usern, Ausführung des Scripts zu einem späteren Zeitpunkt)\nAutomatisches Zwischenspeichern geöffneter Scripts und Wiederherstellung der geöffneten Scripts bei der nächsten Session\nFestlegen verschiedener projektspezifischer Optionen\nVerwendung von Versionsverwaltungssystemen (z.B. git)"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#arbeiten-mit-libraries-packages",
    "href": "prepro/Prepro1_Uebung.html#arbeiten-mit-libraries-packages",
    "title": "PrePro 1: Übung",
    "section": "Arbeiten mit Libraries / Packages",
    "text": "Arbeiten mit Libraries / Packages\nR ist ohne Zusatzpackete nicht mehr denkbar. Die allermeisten Packages werden auf CRAN gehostet und können leicht mittels install.packages() installiert werden. Eine sehr wichtige Sammlung von Packages wird von RStudio entwickelt. Unter dem Namen Tidyverse werden eine Reihe von Packages angeboten, den R-Alltag enorm erleichtert. Wir werden später näher auf das “Tidy”-Universum eingehen, an dieser Stelle können wir einfach mal die wichtigsten Packages aus tidyverse installieren (heute werden wir davon nur einen kleinen Teil benutzen).\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"tidyr\")      # ← MIT Anführungs-/Schlusszeichen\ninstall.packages(\"readr\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"ggplot2\")\n\nUm ein package in R verwenden zu können, gibt es zwei Möglichkeiten:\n\nentweder man lädt es zu Beginn der R-session mittles library(\"dplyr\") (ohne Anführungs- und Schlusszeichen).\noder man ruft eine function mit vorangestelltem Packetname sowie zwei Doppelpunkten auf. dplyr::filter() ruft die Funktion filter() des Packets dplyr auf.\n\nLetztere Notation ist vor allem dann sinnvoll, wenn sich zwei unterschiedliche Funktionen mit dem gleichen namen in verschiedenen pacakges existieren. filter() existiert als Funktion einersits im package dplyr sowie in stats. Dieses Phänomen nennt man “masking”.\nZu Beginn laden wir die nötigen Pakete :\n\nlibrary(\"readr\")                 # ← OHNE Anführungs-/Schlusszeichen\nlibrary(\"lubridate\")\n\n\n\n\n\n\n\nHinweis\n\n\n\nWir nutzen readr um csvs zu importieren und verwenden die Funktion read_csv (mit underscore) als alternative zu read.csv (mit Punkt). Das ist eine persönliche Präferenz1, es ist euch überlassen welche Funktion ihr verwendet. Beachtet, dass die beiden Funktionen leicht andere Parameter erwarten."
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-1",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-1",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nErstelle eine data.frame mit nachstehenden Daten.\n\n\nCode\ndf &lt;- data.frame(\n  Tierart = c(\"Fuchs\",\"Bär\",\"Hase\",\"Elch\"),\n  Anzahl = c(2,5,1,3),\n  Gewicht = c(4.4, 40.3,1.1,120),\n  Geschlecht = c(\"m\",\"f\",\"m\",\"m\"),\n  Beschreibung = c(\"Rötlich\",\"Braun, gross\", \"klein, mit langen Ohren\",\"Lange Beine, Schaufelgeweih\")\n  )\n\n\n\n\n\n\n\nTierart\nAnzahl\nGewicht\nGeschlecht\nBeschreibung\n\n\n\n\nFuchs\n2\n4.4\nm\nRötlich\n\n\nBär\n5\n40.3\nf\nBraun, gross\n\n\nHase\n1\n1.1\nm\nklein, mit langen Ohren\n\n\nElch\n3\n120.0\nm\nLange Beine, Schaufelgeweih"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-2",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-2",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nWas für Datentypen wurden in der letzten Aufgabe automatisch angenommen? Ermittle diese mit str() und prüfe, ob diese sinnvoll sind und wandle um wo nötig.\n\n\nCode\nstr(df)\n## 'data.frame':    4 obs. of  5 variables:\n##  $ Tierart     : chr  \"Fuchs\" \"Bär\" \"Hase\" \"Elch\"\n##  $ Anzahl      : num  2 5 1 3\n##  $ Gewicht     : num  4.4 40.3 1.1 120\n##  $ Geschlecht  : chr  \"m\" \"f\" \"m\" \"m\"\n##  $ Beschreibung: chr  \"Rötlich\" \"Braun, gross\" \"klein, mit langen Ohren\" \"Lange Beine, Schaufelgeweih\"\ntypeof(df$Anzahl)\n## [1] \"double\"\n# Anzahl wurde als `double` interpretiert, ist aber eigentlich ein `integer`. \n\ndf$Anzahl &lt;- as.integer(df$Anzahl)"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-3",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-3",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nNutze die Spalte Gewicht um die Tiere in 3 Gewichtskategorien einzuteilen:\n\nleicht: &lt; 5kg\nmittel: 5 - 100 kg\nschwer: &gt; 100kg\n\n\n\nCode\ndf$Gewichtsklasse[df$Gewicht &gt; 100] &lt;- \"schwer\"\ndf$Gewichtsklasse[df$Gewicht &lt;= 100 & df$Gewicht &gt; 5] &lt;- \"mittel\"\ndf$Gewichtsklasse[df$Gewicht &lt;= 5] &lt;- \"leicht\"\n\n\nDas Resultat:\n\n\n\n\n\n\n\n\n\n\n\n\n\nTierart\nAnzahl\nGewicht\nGeschlecht\nBeschreibung\nGewichtsklasse\n\n\n\n\nFuchs\n2\n4.4\nm\nRötlich\nleicht\n\n\nBär\n5\n40.3\nf\nBraun, gross\nmittel\n\n\nHase\n1\n1.1\nm\nklein, mit langen Ohren\nleicht\n\n\nElch\n3\n120.0\nm\nLange Beine, Schaufelgeweih\nschwer"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-4",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-4",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nImportiere den Datensatz weather.csv (Quelle MeteoSchweiz). Es handelt sich dabei um die stündlich gemittelten Temperaturdaten an verschiedenen Standorten in der Schweiz.\n\n\n\n\n\n\nWarnung\n\n\n\nWenn du read_csv verwenden möchtest: Diese Funktion erwartet leicht andere inputs als read.csv, konsultiere dazu die Hilfe zu read_csv (mit ?read_csv).\n\n\n\n\nCode\nwetter &lt;- read_csv(\"datasets/prepro/weather.csv\")\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000010100\n-2.6\n\n\nABO\n2000010101\n-2.5\n\n\nABO\n2000010102\n-3.1\n\n\nABO\n2000010103\n-2.4\n\n\nABO\n2000010104\n-2.5\n\n\nABO\n2000010105\n-3.0\n\n\nABO\n2000010106\n-3.7\n\n\nABO\n2000010107\n-4.4\n\n\nABO\n2000010108\n-4.1\n\n\nABO\n2000010109\n-4.1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-5",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-5",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nSchau dir die Rückmeldung von read_csv()an. Sind die Daten korrekt interpretiert worden?\n\n\nCode\n# Die Spalte 'time' wurde als 'integer' interpretiert. Dabei handelt es\n# sich offensichtlich um Zeitangaben."
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-6",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-6",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nDie Spalte time ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe meta.txt). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (POSIXct) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit as.POSIXct() ein und spezifiziere sowohl format wie auch tz.\n\n\n\n\n\n\nTipp\n\n\n\n\nWenn keine Zeitzone festgelegt wird, trifft as.POSIXct() eine Annahme (basierend auf Sys.timezone()). In unserem Fall handelt es sich aber um Werte in UTC (siehe metadata.csv)\nas.POSIXcterwartet character: Wenn du eine Fehlermeldung hast die 'origin' must be supplied (o.ä) heisst, hast du der Funktion vermutlich einen Numeric übergeben.\n\n\n\n\n\nCode\nwetter$time &lt;- as.POSIXct(as.character(wetter$time), format = \"%Y%m%d%H\",tz = \"UTC\")\n\n\n\n\n\nDie neue Tabelle sollte so aussehen\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\n\n\nABO\n2000-01-01 01:00:00\n-2.5\n\n\nABO\n2000-01-01 02:00:00\n-3.1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\n\n\nABO\n2000-01-01 04:00:00\n-2.5\n\n\nABO\n2000-01-01 05:00:00\n-3.0\n\n\nABO\n2000-01-01 06:00:00\n-3.7\n\n\nABO\n2000-01-01 07:00:00\n-4.4\n\n\nABO\n2000-01-01 08:00:00\n-4.1\n\n\nABO\n2000-01-01 09:00:00\n-4.1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-7",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-7",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nErstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte POSIXct-Spalte sowie eine geeignete Funktion aus lubridate.\n\n\nCode\nwetter$wochentag &lt;- wday(wetter$time,label = T)\nwetter$kw &lt;- week(wetter$time)\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nwochentag\nkw\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSa.\n1\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSa.\n1\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSa.\n1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSa.\n1\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSa.\n1\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSa.\n1\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSa.\n1\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSa.\n1\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSa.\n1\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSa.\n1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-8",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-8",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nErstelle eine neue Spalte basierend auf die Temperaturwerte mit der Einteilung “kalt” (unter Null Grad) und “warm” (über Null Grad)\n\n\nCode\nwetter$temp_kat[wetter$tre200h0&gt;0] &lt;- \"warm\"\nwetter$temp_kat[wetter$tre200h0&lt;=0] &lt;- \"kalt\"\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nwochentag\nkw\ntemp_kat\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSa.\n1\nkalt"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#footnotes",
    "href": "prepro/Prepro1_Uebung.html#footnotes",
    "title": "PrePro 1: Übung",
    "section": "",
    "text": "Vorteile von read_csv gegenüber read.csv: https://stackoverflow.com/a/60374974/4139249↩︎"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#piping",
    "href": "prepro/Prepro2_Demo.html#piping",
    "title": "Prepro 2: Demo",
    "section": "Piping",
    "text": "Piping\nGegeben ist ein character string (diary). Wir wollen aus diesem Text die Temperaturangabe aus dem String extrahieren und danach den Wert von Kelvin in Celsius nach der folgenden Formel umwandeln und zum Schluss den Mittelwert über all diese Werte berechnen.\n\\[°C = K - 273.15\\]\n\ndiary &lt;- c(\n  \"The temperature is 310° Kelvin\",\n  \"The temperature is 322° Kelvin\",\n  \"The temperature is 410° Kelvin\"\n)\n\ndiary\n## [1] \"The temperature is 310° Kelvin\" \"The temperature is 322° Kelvin\"\n## [3] \"The temperature is 410° Kelvin\"\n\nDazu brauchen wir die Funktion substr(), welche aus einem character einen Teil “raus schnipseln” kann.\n\n# Wenn die Buchstaben einzelne _Elemente_ eines Vektors wären, würden wir diese\n# folgendermassen subsetten:\n\ncharvec1 &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\")\ncharvec1[4:6]\n## [1] \"d\" \"e\" \"f\"\n\n# Aber wenn diese in einem einzigen character gespeichert sind, brauchen wir substr:\ncharvec2 &lt;- \"abcdefgh\"\nsubstr(charvec2, 4, 6)\n## [1] \"def\"\n\nZudem nutzen haben wir eine Hilfsfunktion subtrahieren, welche zwei Werte annimmt, den minuend und den subtrahend:\n\nsubtrahieren &lt;- function(minuend, subtrahend){\n  minuend - subtrahend\n}\n\nsubtrahieren(10, 4)\n## [1] 6\n\nÜbersetzt in R-Code entsteht folgende Operation:\n\noutput &lt;- mean(subtrahieren(as.numeric(substr(diary, 20, 22)),273.15))\n#                                             \\_1_/\n#                                      \\________2__________/\n#                           \\___________________3___________/\n#              \\________________________________4__________________/\n#         \\_____________________________________5____________________/\n\n# 1. Nimm diary\n# 2. Extrahiere auf jeder Zeile die Werte 20 bis 22\n# 3. Konvertiere \"character\" zu \"numeric\"\n# 4. Subtrahiere 273.15\n# 5. Berechne den Mittlwert\n\nDie ganze Operation liest sich etwas leichter, wenn diese sequentiell notiert wird:\n\ntemp &lt;- substr(diary, 20, 22)       # 2\ntemp &lt;- as.numeric(temp)            # 3\ntemp &lt;- subtrahieren(temp, 273.15)  # 4\noutput &lt;- mean(temp)                # 5\n\nUmständlich ist dabei einfach, dass die Zwischenresultate immer abgespeichert und in der darauf folgenden Operation wieder abgerufen werden müssen. Hier kommt “piping” ins Spiel: Mit “piping” wird der Output der einen Funktion der erste Parameter der darauf folgenden Funktion.\n\ndiary |&gt;                            # 1\n  substr(20, 22) |&gt;                 # 2\n  as.numeric() |&gt;                   # 3 \n  subtrahieren(273.15) |&gt;           # 4\n  mean()                            # 5\n## [1] 74.18333\n\n\n\n\n\n\n\nWichtig\n\n\n\n\nder |&gt; Pipe Operator wurde erst in R 4.1 eingeführt\nNeben dem base R Pipe Operator existiert im Package magrittr ein sehr ähnlicher1 Pipe Operator: |&gt;\nDie Tastenkombination Ctrl+Shift+M in RStudio fügt einen Pipe Operator ein.\nWelcher Pipe Operator |&gt; oder |&gt; mit der obigen Tastenkombination eingeführt wird, kann über die RStudio Settings Tools → Global Options → Code → Häckchen setzen bei Use nativ pipe operator"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#joins",
    "href": "prepro/Prepro2_Demo.html#joins",
    "title": "Prepro 2: Demo",
    "section": "Joins",
    "text": "Joins\n\nstudierende &lt;- data.frame(\n  Matrikel_Nr = c(100002, 100003, 200003),\n  Studi = c(\"Patrick\", \"Manuela\", \"Eva\"),\n  PLZ = c(8006, 8001, 8820)\n)\n\nstudierende\n##   Matrikel_Nr   Studi  PLZ\n## 1      100002 Patrick 8006\n## 2      100003 Manuela 8001\n## 3      200003     Eva 8820\n\nortschaften &lt;- data.frame(\n  PLZ = c(8003, 8006, 8810, 8820),\n  Ortsname = c(\"Zürich\", \"Zürich\", \"Horgen\", \"Wädenswil\")\n)\n\nortschaften\n##    PLZ  Ortsname\n## 1 8003    Zürich\n## 2 8006    Zürich\n## 3 8810    Horgen\n## 4 8820 Wädenswil\n\n\n#Load library\nlibrary(\"dplyr\")\n\ninner_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      200003     Eva 8820 Wädenswil\n\nleft_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      100003 Manuela 8001      &lt;NA&gt;\n## 3      200003     Eva 8820 Wädenswil\n\nright_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      200003     Eva 8820 Wädenswil\n## 3          NA    &lt;NA&gt; 8003    Zürich\n## 4          NA    &lt;NA&gt; 8810    Horgen\n\nfull_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      100003 Manuela 8001      &lt;NA&gt;\n## 3      200003     Eva 8820 Wädenswil\n## 4          NA    &lt;NA&gt; 8003    Zürich\n## 5          NA    &lt;NA&gt; 8810    Horgen\n\n\nstudierende &lt;- data.frame(\n  Matrikel_Nr = c(100002, 100003, 200003),\n  Studi = c(\"Patrick\", \"Manuela\", \"Pascal\"),\n  Wohnort = c(8006, 8001, 8006)\n)\n\nleft_join(studierende, ortschaften, by = c(\"Wohnort\" = \"PLZ\"))\n##   Matrikel_Nr   Studi Wohnort Ortsname\n## 1      100002 Patrick    8006   Zürich\n## 2      100003 Manuela    8001     &lt;NA&gt;\n## 3      200003  Pascal    8006   Zürich"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#footnotes",
    "href": "prepro/Prepro2_Demo.html#footnotes",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "siehe https://stackoverflow.com/q/67633022/4139249↩︎"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-1",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-1",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nLade die Wetterdaten von letzer Woche runter (weather.csv, Quelle MeteoSchweiz) und importiere sie in R. Sorge dafür, dass die Spalten korrekt formatiert sind (stn als factor, time als POSIXct, tre200h0 als numeric.)\n\n\nCode\n# Variante 1\nwetter &lt;- read_csv(\"datasets/prepro/weather.csv\")\nwetter$stn &lt;- as.factor(wetter$stn)\nwetter$time &lt;- as.POSIXct(as.character(wetter$time), format = \"%Y%m%d%H\", tz = \"UTC\")\n\n\n\n\nCode\n# Variate 2 (für Profis)\nwetter &lt;- read_csv(\"datasets/prepro/weather.csv\",\n                  col_types = cols(\n                    col_factor(levels = NULL),    \n                    col_datetime(format = \"%Y%m%d%H\"),\n                    col_double()\n                    )\n                  )"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-2",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-2",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nLade metadata herunter und lese es ebenfalls als csv ein.\n\n\n\n\n\n\nTipp\n\n\n\nWenn Umlaute und Sonderzeichen nicht korrekt dargestellt werden (z.B. das è in Genève), hat das vermutlich mit der Zeichencodierung zu tun. Das File ist aktuell in UTF-8 codiert. Wenn Umlaute nicht korrekt dargestellt werden, hat R diese Codierung nicht erkannt und sie muss in der Import-Funktion spezifitiert werden. Dies wird je nach verwendete import Funktion unterschiedlich gemacht:\n\nFunktionen aus dem Package readr: locale = locale(encoding = \"UTF-8\")\nBase-R Funktionen: fileEncoding = \"UTF-8\"\n\nWenn ihr die codierung eines Files nicht kennt, könnt wie folgt vorgehen: Anleitung für Windows, für Mac und für Linux.\n\n\n\n\nCode\nmetadata &lt;- read_delim(\"datasets/prepro/metadata.csv\", delim = \";\", locale = locale(encoding = \"UTF-8\"))"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-3",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-3",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nNun wollen wir den Datensatz wettermit den Informationen aus metadata anreichern. Uns interessiert aber nur das Stationskürzel, der Name, die x/y Koordinaten sowie die Meereshöhe, selektiere diese Spalten.\n\n\nCode\nmetadata &lt;- metadata[,c(\"stn\", \"Name\", \"x\",\"y\",\"Meereshoehe\")]"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-4",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-4",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nJetzt kann metadata mit dem Datensatz wetter verbunden werden. Überlege dir, welcher Join dafür sinnvoll ist und mit welchem Attribut wir “joinen” können.\nNutze die Join-Möglichkeiten von dplyr (Hilfe via ?dplyr::join) um die Datensätze wetter und metadata zu verbinden.\n\n\nCode\nwetter &lt;- left_join(wetter,metadata,by = \"stn\")\n\n# Jointyp: Left-Join auf 'wetter', da uns nur die Stationen im Datensatz 'wetter' interessieren.\n# Attribut: \"stn\""
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-5",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-5",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nErstelle eine neue Spalte month welche den jeweiligen Monat (aus time) beinhaltet. Nutze dafür die Funktion lubridate::month().\n\n\nCode\nwetter$month &lt;- month(wetter$time)"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-6",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-6",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nBerechne mit der Spalte month die Durchschnittstemperatur pro Monat.\n\n\nCode\nmean(wetter$tre200h0[wetter$month == 1])\n## [1] -1.963239\nmean(wetter$tre200h0[wetter$month == 2])\n## [1] 0.3552632\nmean(wetter$tre200h0[wetter$month == 3])\n## [1] 2.965054\n\n# usw. für alle 12 Monate"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-1",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-1",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nGegeben sind die Daten von drei Sensoren (sensor1.csv, sensor2.csv, sensor3.csv). Lade die Datensätze runter und lese sie ein.\n\n\nCode\nsensor1 &lt;- read_delim(\"datasets/prepro/sensor1.csv\",\";\")\nsensor2 &lt;- read_delim(\"datasets/prepro/sensor2.csv\",\";\")\nsensor3 &lt;- read_delim(\"datasets/prepro/sensor3.csv\",\";\")"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-2",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-2",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nErstelle aus den 3 Dataframes eine einzige Dataframe, die aussieht wie unten dargestellt. Nutze dafür zwei joins aus dplyr um 3 data.frames miteinander zu verbinden. Bereinige im Anschluss die Spaltennamen (wie geht das?).\n\n\nCode\nsensor1_2 &lt;- full_join(sensor1, sensor2, \"Datetime\")\n\nsensor1_2 &lt;- rename(sensor1_2, sensor1 = Temp.x, sensor2 = Temp.y)\n\nsensor_all &lt;- full_join(sensor1_2, sensor3, by = \"Datetime\")\n\nsensor_all &lt;- rename(sensor_all, sensor3 = Temp)\n\n\n\n\n\n\n\nDatetime\nsensor1\nsensor2\nsensor3\n\n\n\n\n16102017_1800\n23.5\n13.5\n26.5\n\n\n17102017_1800\n25.4\n24.4\n24.4\n\n\n18102017_1800\n12.4\n22.4\n13.4\n\n\n19102017_1800\n5.4\n12.4\n7.4\n\n\n23102017_1800\n23.5\n13.5\nNA\n\n\n24102017_1800\n21.3\n11.3\nNA"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-3",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-3",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nImportiere die Datei sensor_fail.csv in R.\n\n\nCode\nsensor_fail &lt;- read_delim(\"datasets/prepro/sensor_fail.csv\", delim = \";\")\n\n\nsensor_fail.csv hat eine Variabel SensorStatus: 1 bedeutet der Sensor misst, 0 bedeutet der Sensor misst nicht. Fälschlicherweise wurde auch dann der Messwert Temp = 0 erfasst, wenn Sensorstatus = 0. Richtig wäre hier NA (not available). Korrigiere den Datensatz entsprechend.\n\n\n\n\n\nSensor\nTemp\nHum_%\nDatetime\nSensorStatus\n\n\n\n\nSen102\n0.6\n98\n16102017_1800\n1\n\n\nSen102\n0.3\n96\n17102017_1800\n1\n\n\nSen102\n0.0\n87\n18102017_1800\n1\n\n\nSen102\n0.0\n86\n19102017_1800\n0\n\n\nSen102\n0.0\n98\n23102017_1800\n0\n\n\nSen102\n0.0\n98\n24102017_1800\n0\n\n\nSen102\n0.0\n96\n25102017_1800\n1\n\n\nSen103\n-0.3\n87\n26102017_1800\n1\n\n\nSen103\n-0.7\n98\n27102017_1800\n1\n\n\nSen103\n-1.2\n98\n28102017_1800\n1\n\n\n\n\n\n\n\nCode\n# mit base-R: \nsensor_fail$Temp_correct[sensor_fail$SensorStatus == 0] &lt;- NA\n\n# das gleiche mit dplyr:\nsensor_fail &lt;- sensor_fail |&gt;\n  mutate(Temp_correct = ifelse(SensorStatus == 0, NA, Temp))"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-4",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-4",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nWarum spielt das es eine Rolle, ob 0 oder NA erfasst wird? Berechne die Mittlere der Temperatur / Feuchtigkeit nach der Korrektur.\n\n\nCode\n\n# Mittelwerte der falschen Sensordaten: 0 fliesst in die Berechnung \n# ein und verfälscht den Mittelwert\nmean(sensor_fail$Temp)\n## [1] -0.13\n\n# Mittelwerte der korrigierten Sensordaten: mit na.rm = TRUE werden \n# NA-Werte aus der Berechnung entfernt. \nmean(sensor_fail$Temp_correct, na.rm = TRUE)\n## [1] -0.1857143"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#split-apply-combine",
    "href": "prepro/Prepro3_Demo.html#split-apply-combine",
    "title": "Prepro 3: Demo",
    "section": "Split-Apply-Combine",
    "text": "Split-Apply-Combine\n\nDaten Laden\nWir laden die Wetterdaten (Quelle MeteoSchweiz) von der letzten Übung.\n\nwetter &lt;- read_csv(\"datasets/prepro/weather.csv\")\n\nwetter &lt;- wetter  |&gt;\n  mutate(\n    stn = as.factor(stn),\n    time = as.POSIXct(as.character(time), format = \"%Y%m%d%H\")\n  )\n\n\n\nKennwerte berechnen\nWir möchten den Mittelwert aller gemessenen Temperaturwerte berechnen. Dazu könnten wir folgenden Befehl verwenden:\n\nmean(wetter$tre200h0, na.rm = TRUE) \n## [1] 6.324744\n\nDie Option na.rm = T bedeutet, dass NA Werte von der Berechnung ausgeschlossen werden sollen.\nMit der selben Herangehensweise können diverse Werte berechnet werden (z.B. das Maximum (max()), Minimum (min()), Median (median()) u.v.m.).\nDiese Herangehensweise funktioniert nur dann gut, wenn wir die Kennwerte über alle Beobachtungen für eine Variable (Spalte) berechnen wollen. Sobald wir die Beobachtungen gruppieren wollen, wird es schwierig. Zum Beispiel, wenn wir die durchschnittliche Temperatur pro Monat berechnen wollen.\n\n\nConvenience Variablen\nUm diese Aufgabe zu lösen, muss zuerst der Monat extrahiert werden (der Monat ist die convenience variable). Hierfür brauchen wir die Funktion lubridate::month().\nNun kann kann die convenience Variable “Month” erstellt werden. Ohne dpylr wird eine neue Spalte folgendermassen hinzugefügt.\n\nwetter$month &lt;- month(wetter$time)\n\nMit dplyr (siehe 3) sieht der gleiche Befehl folgendermassen aus:\n\nwetter &lt;- mutate(wetter,month = month(time))\n\nDer grosse Vorteil von dplyr ist an dieser Stelle noch nicht ersichtlich. Dieser wird aber später klar.\n\n\nKennwerte nach Gruppen berechnen\nUm mit base R den Mittelwert pro Monat zu berechnen, kann man zuerst ein Subset mit [] erstellen und davon den Mittelwert berechnen, z.B. folgendermassen:\n\nmean(wetter$tre200h0[wetter$month == 1], na.rm = TRUE)\n## [1] -1.963239\n\nDies müssen wir pro Monat wiederholen, was natürlich sehr umständlich ist. Deshalb nutzen wir das package dplyr. Damit geht die Aufgabe (Temperaturmittel pro Monat berechnen) folgendermassen:\n\nsummarise(group_by(wetter,month),temp_mittel = mean(tre200h0, na.rm = TRUE))\n## # A tibble: 13 × 2\n##    month temp_mittel\n##    &lt;dbl&gt;       &lt;dbl&gt;\n##  1     1      -1.96 \n##  2     2       0.355\n##  3     3       2.97 \n##  4     4       4.20 \n##  5     5      11.0  \n##  6     6      12.4  \n##  7     7      13.0  \n##  8     8      15.0  \n##  9     9       9.49 \n## 10    10       8.79 \n## 11    11       1.21 \n## 12    12      -0.898\n## 13    NA       2.95\n\n\n\nVerketten vs. verschachteln\nAuf Deutsch übersetzt heisst die obige Operation folgendermassen:\n\nnimm den Datensatz wetter\nBilde Gruppen pro Jahr (group_by(wetter,year))\nBerechne das Temperaturmittel (mean(tre200h0))\n\nDiese Übersetzung R-&gt; Deutsch unterscheidet sich vor allem darin, dass die Operation auf Deutsch verkettet ausgesprochen wird (Operation 1-&gt;2-&gt;3) während der Computer verschachtelt liest 3(2(1)). Um R näher an die gesprochene Sprache zu bringen, kann man den |&gt;-Operator verwenden (siehe 4).\n\n# 1 nimm den Datensatz \"wetter\"\n# 2 Bilde Gruppen pro Monat\n# 3 berechne das Temperaturmittel \n\nsummarise(group_by(wetter,month),temp_mittel = mean(tre200h0))\n#                  \\_1_/\n#         \\__________2_________/\n#\\___________________3_______________________________________/\n\n# wird zu:\n\nwetter |&gt;                                 # 1\n  group_by(month) |&gt;                      # 2\n  summarise(temp_mittel = mean(tre200h0)) # 3\n\nDieses Verketten mittels |&gt; (genannt “pipe”) macht den Code einiges schreib- und leserfreundlicher, und wir werden ihn in den nachfolgenden Übungen verwenden. Die “pipe” wird mit dem package magrittr bereitgestellt und mit dplyr mitinstalliert.\nZu dplyr gibt es etliche Tutorials online (siehe5), deshalb werden wir diese Tools nicht in allen Details erläutern. Nur noch folgenden wichtigen Unterschied zu zwei wichtigen Funktionen in dpylr: mutate() und summarise().\n\nsummarise() fasst einen Datensatz zusammen. Dabei reduziert sich die Anzahl Beobachtungen (Zeilen) auf die Anzahl Gruppen (z.B. eine zusammengefasste Beobachtung (Zeile) pro Jahr). Zudem reduziert sich die Anzahl Variablen (Spalten) auf diejenigen, die in der “summarise” Funktion spezifiziert wurde (z.B. temp_mittel).\nmit mutate wird ein data.frame vom Umfang her belassen, es werden lediglich zusätzliche Variablen (Spalten) hinzugefügt (siehe Beispiel unten).\n\n\n# Maximal und minimal Temperatur pro Kalenderwoche\nweather_summary &lt;- wetter |&gt;               #1) nimm den Datensatz \"wetter\"\n  filter(month == 1) |&gt;                    #2) filter auf den Monat Januar\n  mutate(day = day(time)) |&gt;               #3) erstelle eine neue Spalte \"day\"\n  group_by(day) |&gt;                         #4) Nutze die neue Spalte um Gruppen zu bilden\n  summarise(\n    temp_max = max(tre200h0, na.rm = TRUE), #5) Berechne das Maximum \n    temp_min = min(tre200h0, na.rm = TRUE)  #6) Berechne das Minimum\n    )   \n\nweather_summary\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # ℹ 21 more rows"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#reshaping-data",
    "href": "prepro/Prepro3_Demo.html#reshaping-data",
    "title": "Prepro 3: Demo",
    "section": "Reshaping data",
    "text": "Reshaping data\n\nBreit → lang\nDie Umformung von Tabellen breit→lang erfolgt mittels tidyr(siehe 6). Auch dieses Package funktioniert wunderbar mit piping (|&gt;).\n\nweather_summary |&gt;\n  pivot_longer(c(temp_max,temp_min))\n## # A tibble: 62 × 3\n##      day name     value\n##    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n##  1     1 temp_max   5.8\n##  2     1 temp_min  -4.4\n##  3     2 temp_max   2.8\n##  4     2 temp_min  -4.3\n##  5     3 temp_max   4.2\n##  6     3 temp_min  -3.1\n##  7     4 temp_max   4.7\n##  8     4 temp_min  -2.8\n##  9     5 temp_max  11.4\n## 10     5 temp_min  -0.6\n## # ℹ 52 more rows\n\nIm Befehl pivot_longer() müssen wir festlegen, welche Spalten zusammengefasst werden sollen (hier: temp_max,temp_min,temp_mean). Alternativ können wir angeben, welche Spalten wir nicht zusammenfassen wollen:\n\nweather_summary |&gt;\n  pivot_longer(-day)\n## # A tibble: 62 × 3\n##      day name     value\n##    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n##  1     1 temp_max   5.8\n##  2     1 temp_min  -4.4\n##  3     2 temp_max   2.8\n##  4     2 temp_min  -4.3\n##  5     3 temp_max   4.2\n##  6     3 temp_min  -3.1\n##  7     4 temp_max   4.7\n##  8     4 temp_min  -2.8\n##  9     5 temp_max  11.4\n## 10     5 temp_min  -0.6\n## # ℹ 52 more rows\n\nWenn wir die Namen neuen Spalten festlegen wollen (anstelle von name und value) erreichen wir dies mit names_to bzw. values_to:\n\nweather_summary_long &lt;- weather_summary |&gt;\n  pivot_longer(-day, names_to = \"Messtyp\", values_to = \"Messwert\")\n\nDie ersten 6 Zeilen von weather_summary_long:\n\n\n\n\n\nday\nMesstyp\nMesswert\n\n\n\n\n1\ntemp_max\n5.8\n\n\n1\ntemp_min\n-4.4\n\n\n2\ntemp_max\n2.8\n\n\n2\ntemp_min\n-4.3\n\n\n3\ntemp_max\n4.2\n\n\n3\ntemp_min\n-3.1\n\n\n\n\n\nDie ersten 6 Zeilen von wetter_sry:\n\n\n\n\n\nday\ntemp_max\ntemp_min\n\n\n\n\n1\n5.8\n-4.4\n\n\n2\n2.8\n-4.3\n\n\n3\n4.2\n-3.1\n\n\n4\n4.7\n-2.8\n\n\n5\n11.4\n-0.6\n\n\n6\n6.7\n-1.6\n\n\n\n\n\nBeachte: weather_summary_long umfasst 62 Beobachtungen (Zeilen), das sind doppelt soviel wie weather_summary, da wir ja zwei Spalten zusammengefasst haben.\n\nnrow(weather_summary)\n## [1] 31\nnrow(weather_summary_long)\n## [1] 62\n\nLange Tabellen sind in verschiedenen Situationen praktischer. Beispielsweise ist das Visualisieren mittels ggplot2 (dieses Package werdet ihr im Block “InfoVis” kennenlernen) mit long tables wesentlich einfacher.\n\nggplot(weather_summary_long, aes(day,Messwert, colour = Messtyp)) +\n  geom_line()\n\n\n\n\n\n\nLang → breit\nDas Gegenstück zu pivot_longer ist pivot_wider. Mit dieser Funktion können wir eine lange Tabelle in eine breite überführen. Dazu müssen wir in names_from angeben, aus welcher Spalte die neuen Spaltennamen erstellt werden sollen (names_from) und aus welcher Spalte die Werte entstammen sollen (values_from):\n\nweather_summary_long |&gt;\n  pivot_wider(names_from = Messtyp, values_from = Messwert)\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # ℹ 21 more rows\n\nZum Vergleich: mit einer wide table müssen wir in ggplot2 jede Spalte einzeln plotten. Dies ist bei wenigen Variabeln wie hier noch nicht problematisch, aber bei einer hohen Anzahl wird dies schnell mühsam.\n\nggplot(weather_summary) +\n  geom_line(aes(day, temp_max)) +\n  geom_line(aes(day, temp_min))\n\n\n\n\n\n\n\n\nWickham, Hadley, und Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093."
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#footnotes",
    "href": "prepro/Prepro3_Demo.html#footnotes",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "http://r4ds.had.co.nz/↩︎\nhttps://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093↩︎\nWickham und Grolemund (2017), Kapitel 10 / http://r4ds.had.co.nz/transform.html↩︎\nWickham und Grolemund (2017), Kapitel 14 / http://r4ds.had.co.nz/pipes.html↩︎\nWickham und Grolemund (2017), Kapitel 10 / http://r4ds.had.co.nz/transform.html, oder Hands-on dplyr tutorial..↩︎\nhttps://r4ds.had.co.nz/tidy-data.html#pivoting↩︎"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-1",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-1",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nGegeben sei ein Datensatz “sensors_combined.csv”, mit den Temperaturwerten von drei verschiedenen Sensoren. Lade diesen Datensatz herunter, importiere ihn als csv in R (als sensors_combined).\nFormatiere die Datetime Spalte in POSIXct um. Verwende dazu die Funktion as.POSIXct (lies mit ?strftime() nochmal nach wie du das spezfische Format (die “Schablone”) festlegen kannst.\n\n\nCode\nlibrary(\"readr\")\n\nsensors_combined &lt;- read_csv(\"datasets/prepro/sensors_combined.csv\")\n\nsensors_combined$Datetime &lt;- as.POSIXct(sensors_combined$Datetime, format = \"%d%m%Y_%H%M\")"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-2",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-2",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nÜberführe die Tabelle in ein langes Format (verwende dazu die Funktion pivot_longer aus tidyr) und speichere den output als sensors_long.\nTipp:\n\nim Argument cols kannst du entweder die Spalten auflisten, die “pivotiert” werden sollen.\nAlternativ kannst du (mit vorangestelltem Minuszeichen, -) die Spalte, bezeichnen, die nicht pivotiert werden soll.\nIn beiden Fällen musst du die Spalten weder mit Anführungs- und Schlusszeichen noch mit dem $-Zeichen versehen.\n\n\n\nCode\nlibrary(\"tidyr\")\n\n# Variante 1 (Spalten abwählen)\nsensors_long &lt;- pivot_longer(sensors_combined, -Datetime) \n\n# Variante 2 (Spalten anwählen)\nsensors_long &lt;- pivot_longer(sensors_combined, c(sensor1:sensor3))"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-3",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-3",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nGruppiere sensors_long nach der neuen Spalte wo die Sensor-Information enthalten ist (default: name) mit group_by und berechne die mittlere Temperatur pro Sensor (summarise). Hinweis: Beide Funktionen sind Teil des Packages dplyr.\nDer Output sieht folgendermassen aus:\n\n\nCode\nlibrary(\"dplyr\")\n\nsensors_long |&gt;\n  group_by(name) |&gt;\n  summarise(temp_mean = mean(value, na.rm = TRUE))\n## # A tibble: 3 × 2\n##   name    temp_mean\n##   &lt;chr&gt;       &lt;dbl&gt;\n## 1 sensor1      14.7\n## 2 sensor2      12.0\n## 3 sensor3      14.4"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-4",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-4",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nErstelle für sensors_long eine neue convenience Variabel month welche den Monat beinhaltet (Tipp: verwende dazu die Funktion month aus lubridate). Gruppiere nun nach month und Sensor und berechne die mittlere Temperatur.\n\n\nCode\nlibrary(\"lubridate\")\n\nsensors_long |&gt;\n  mutate(month = month(Datetime)) |&gt;\n  group_by(month, name) |&gt;\n  summarise(temp_mean = mean(value, na.rm = TRUE))"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-5",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-5",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nLade jetzt nochmal den Datensatz weather.csv (Quelle MeteoSchweiz) herunter und importiere ihn als CSV mit den korrekten Spaltentypen (stn als factor, time als POSIXct, tre200h0 als double).\n\n\nCode\nweather &lt;- read_csv(\"datasets/prepro/weather.csv\", col_types = cols(col_factor(), col_datetime(\"%Y%m%d%H\"), col_double()))"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-6",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-6",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nErstelle nun eine convenience Variable für die Kalenderwoche pro Messung (lubridate::isoweek). Berechne im Anschluss den mittleren Temperaturwert pro Kalenderwoche.\n\n\nCode\nweather_summary &lt;- weather |&gt;\n  mutate(week = isoweek(time)) |&gt;\n  group_by(week) |&gt;\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\n\nVisualisiere im Anschluss das Resultat:\n\nCode\nplot(weather_summary$week, weather_summary$temp_mean, type = \"l\")"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-7",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-7",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nIn der vorherigen Aufgabe haben wir die mittlere Temperatur pro Kalenderwoche über alle Jahre (2000 und 2001) berechnet. Wenn wir die Jahre aber miteinander vergleichen wollen, müssen wir das Jahr als zusätzliche convenience Variable erstellen und danach gruppieren. Versuche dies mit den Wetterdaten und visualisiere den Output anschliessend.\n\n\nCode\nweather_summary2 &lt;- weather |&gt;\n  mutate(\n    week = week(time),\n    year = year(time)\n    ) |&gt;\n  group_by(year, week) |&gt;\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\n\n\n\nCode\nplot(weather_summary2$week, weather_summary2$temp_mean, type = \"l\")\n\n\n\n\n\nAbbildung 8.1: baseplot mag keine long tables und macht aus den beiden Jahren eine kontinuierliche Linie"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-8",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-8",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nÜberführe den Output aus der letzten Übung in eine wide table. Nun lassen sich die beiden Jahre viel besser miteinander vergleichen.\n\n\nCode\nweather_summary2 &lt;- weather_summary2 |&gt;\n  pivot_wider(names_from = year, values_from = temp_mean,names_prefix = \"year\")\n\n\n\n\nCode\nplot(weather_summary2$week, weather_summary2$year2000, type = \"l\",col = \"blue\")\nlines(weather_summary2$week, weather_summary2$year2001, type = \"l\",col = \"red\")"
  },
  {
    "objectID": "InfoVis.html#infovis-1",
    "href": "InfoVis.html#infovis-1",
    "title": "InfoVis",
    "section": "Infovis 1",
    "text": "Infovis 1\nDie konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics."
  },
  {
    "objectID": "InfoVis.html#infovis-2",
    "href": "InfoVis.html#infovis-2",
    "title": "InfoVis",
    "section": "Infovis 2",
    "text": "Infovis 2\nDie Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser lesson eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen."
  },
  {
    "objectID": "infovis/Infovis1_Vorbereitung.html",
    "href": "infovis/Infovis1_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "Im Rahmen von InfoVis 1 - 2 werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog Kapitel 1 könnt ihr mit nachstehendem Code alle noch nicht installierten packages automatisch installieren.\n\n\nipak &lt;- function(pkg){\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if(length(new.pkg)){install.packages(new.pkg, dependencies = TRUE)}\n}\n\npackages &lt;- c(\"dplyr\", \"ggplot\", \"lubridate\", \"readr\", \"scales\", \"tidyr\")\n\nipak(packages)"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#base-plot-vs.-ggplot",
    "href": "infovis/Infovis1_Demo.html#base-plot-vs.-ggplot",
    "title": "Infovis 1: Demo A",
    "section": "Base-plot vs. ggplot",
    "text": "Base-plot vs. ggplot\nUm in “base-R” einen Scatterplot zu erstellen wo Datum der Temperatur gegenübersteht, gehen wir wie folgt vor:\n\nplot(temperature$time, temperature$SHA, type = \"l\", col = \"red\")\nlines(temperature$time, temperature$ZER, col = \"blue\")\n\n\n\n\nIn ggplot sieht das etwas anders und auf den ersten Blick etwas komplizierter aus: Ein plot wird durch den Befehl ggplot() initiiert. Hier wird einerseits der Datensatz festgelegt, auf dem der Plot beruht (data =), sowie die Variablen innerhalb des Datensatzes, die Einfluss auf den Plot ausüben (mapping = aes()).\n\n# Datensatz: \"temperature\" | Beeinflussende Variabeln: \"time\" und \"temp\"\nggplot(data = temperature, mapping = aes(time,SHA))             \n\n\n\n\nWeiter braucht es mindestens ein “Layer” der beschreibt, wie die Daten dargestellt werden sollen (z.B. geom_point()). Anders als bei “Piping” (|&gt;) wird ein Layer mit + hinzugefügt.\n\nggplot(data = temperature, mapping = aes(time,SHA)) +         \n  # Layer: \"geom_point\" entspricht Punkten in einem Scatterplot \n  geom_point()                                    \n\n\n\n\nDa ggplot die Eingaben in der Reihenfolge data = und dann mapping =erwartet, können wir diese Spezifizierungen auch weglassen.\n\nggplot(temperature, aes(time,SHA)) +\n  geom_point()"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "href": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "title": "Infovis 1: Demo A",
    "section": "Long vs. wide",
    "text": "Long vs. wide\nWie wir in PrePro 2 bereits erwähnt haben, ist ggplot2 auf long tables ausgelegt. Wir überführen deshalb an dieser Stelle die breite in eine lange Tabelle:\n\ntemperature_long &lt;- pivot_longer(temperature, -time, names_to = \"station\", values_to = \"temp\")\n\nNun wollen wir die unterschiedlichen Stationen unterschiedlich einfärben. Da wir Variablen definieren wollen, welche Einfluss auf die Grafik haben sollen, gehört diese Information in aes().\n\nggplot(temperature_long, aes(time,temp, colour = station)) +\n  geom_point()\n\n\n\n\nWir können noch einen Layer mit Linien hinzufügen:\n\nggplot(temperature_long, aes(time,temp, colour = station)) +\n  geom_point()+\n  geom_line()"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#beschriftungen-labels",
    "href": "infovis/Infovis1_Demo.html#beschriftungen-labels",
    "title": "Infovis 1: Demo A",
    "section": "Beschriftungen (labels)",
    "text": "Beschriftungen (labels)\nWeiter können wir die Achsen beschriften und einen Titel hinzufügen. Zudem lasse ich die Punkte (geom_point()) nun weg, da mir diese nicht gefallen.\n\nggplot(temperature_long, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    )"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#split-apply-combine",
    "href": "infovis/Infovis1_Demo.html#split-apply-combine",
    "title": "Infovis 1: Demo A",
    "section": "Split Apply Combine",
    "text": "Split Apply Combine\nIm obigen Plot fällt auf, dass stündliche Werte eine zu hohe Auflösung ist, wenn wir daten über 2 Jahre visualisieren. Mit Split Apply Combine (PrePro 3) können wir die Auflösung unserer Daten verändern:\n\ntemperature_day &lt;- temperature_long |&gt;\n  mutate(time = as.Date(time)) \n\ntemperature_day\n\n# A tibble: 35,088 × 3\n   time       station  temp\n   &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n 1 2000-01-01 SHA       0.2\n 2 2000-01-01 ZER      -8.8\n 3 2000-01-01 SHA       0.3\n 4 2000-01-01 ZER      -8.7\n 5 2000-01-01 SHA       0.3\n 6 2000-01-01 ZER      -9  \n 7 2000-01-01 SHA       0.3\n 8 2000-01-01 ZER      -8.7\n 9 2000-01-01 SHA       0.4\n10 2000-01-01 ZER      -8.5\n# ℹ 35,078 more rows\n\ntemperature_day &lt;- temperature_day |&gt;\n  group_by(station, time) |&gt;\n  summarise(temp = mean(temp))\n\ntemperature_day  \n\n# A tibble: 1,462 × 3\n# Groups:   station [2]\n   station time        temp\n   &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;\n 1 SHA     2000-01-01  1.25\n 2 SHA     2000-01-02  1.73\n 3 SHA     2000-01-03  1.59\n 4 SHA     2000-01-04  1.78\n 5 SHA     2000-01-05  4.66\n 6 SHA     2000-01-06  3.49\n 7 SHA     2000-01-07  3.87\n 8 SHA     2000-01-08  3.28\n 9 SHA     2000-01-09  3.24\n10 SHA     2000-01-10  3.24\n# ℹ 1,452 more rows"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#xy-achse-anpassen",
    "href": "infovis/Infovis1_Demo.html#xy-achse-anpassen",
    "title": "Infovis 1: Demo A",
    "section": "X/Y-Achse anpassen",
    "text": "X/Y-Achse anpassen\nMan kann auch Einfluss auf die x-/y-Achsen nehmen. Dabei muss man zuerst festlegen, was für ein Achsentyp der Plot hat (vorher hat ggplot eine Annahme auf der Basis der Daten getroffen).\nBei unserer y-Achse handelt es sich um numerische Daten, ggplot nennt diese: scale_y_continuous(). Unter ggplot2.tidyverse.org findet man noch andere x/y-Achsentypen (scale_x_irgenwas bzw. scale_y_irgendwas).\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    ) +    \n  scale_y_continuous(limits = c(-30,30))    # y-Achsenabschnitt bestimmen\n\n\n\n\nDas gleiche Spiel kann man für die y-Achse betreiben. Bei unserer y-Achse handelt es sich ja um Datumsangaben. ggplot nennt diese: scale_x_date().\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    ) +    \n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#themes",
    "href": "infovis/Infovis1_Demo.html#themes",
    "title": "Infovis 1: Demo A",
    "section": "Themes",
    "text": "Themes\nMit theme verändert man das allgmeine Layout der Plots. Beispielsweise kann man mit theme_classic() ggplot-Grafiken etwas weniger “Poppig” erscheinen lassen: so sind sie besser für Bachelor- / Masterarbeiten sowie Publikationen geeignet. theme_classic() kann man indiviudell pro Plot anwenden, oder für die aktuelle Session global setzen (s.u.)\nIndividuell pro Plot:\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  theme_classic()\n\nGlobal (für alle nachfolgenden Plots der aktuellen Session):\n\ntheme_set(theme_classic())"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "href": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "title": "Infovis 1: Demo A",
    "section": "Facets / Small Multiples",
    "text": "Facets / Small Multiples\nSehr praktisch sind auch die Funktionen für “Small multiples”. Dies erreicht man mit facet_wrap() (oder facet_grid(), mehr dazu später). Man muss mit einem Tilde-Symbol “~” nur festlegen, welche Variable für das Aufteilen des Plots in kleinere Subplots verantwortlich sein soll.\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    ) +    \n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\") +\n  facet_wrap(station~.)\n\n\n\n\nAuch facet_wrap kann man auf seine Bedürfnisse anpassen: Beispielweise kann man mit ncol = die Anzahl facets pro Zeile bestimmen.\nZudem brauchen wir die Legende nicht mehr, da der Stationsnamen über jedem Facet steht. Ich setze deshalb theme(legend.position=\"none\")\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\"\n    ) +  \n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\") +\n  facet_wrap(~station,ncol = 1) +\n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#in-variabel-abspeichern-und-exportieren",
    "href": "infovis/Infovis1_Demo.html#in-variabel-abspeichern-und-exportieren",
    "title": "Infovis 1: Demo A",
    "section": "In Variabel abspeichern und Exportieren",
    "text": "In Variabel abspeichern und Exportieren\nGenau wie data.frames und andere Objekte, kann man einen ganzen Plot auch in einer Variabel speichern. Dies kann nützlich sein um einen Plot zu exportieren (als png, jpg usw.) oder sukzessive erweitern wie in diesem Beispiel.\n\np &lt;- ggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\"\n    ) +\n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\") +\n  facet_wrap(~station,ncol = 1)\n  # ich habe an dieser Stelle theme(legend.position=\"none\") entfernt\n\nFolgendermassen kann ich den Plot als png-File abspeichern (ohne Angabe von “plot =” wird einfach der letzte Plot gespeichert)\n\nggsave(filename = \"plot.png\",plot = p)\n\n.. und so kann ich einen bestehenden Plot (in einer Variabel) mit einem Layer / einer Option erweitern\n\np +\n  theme(legend.position=\"none\")\n\nWie üblich wurde diese Änderung nicht gespeichert, sondern nur das Resultat davon ausgeben. Wenn die Änderung in meinem Plot (in der Variabel) abspeichern will, muss ich die Variabel überschreiben:\n\np &lt;- p +\n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#smoothing",
    "href": "infovis/Infovis1_Demo.html#smoothing",
    "title": "Infovis 1: Demo A",
    "section": "Smoothing",
    "text": "Smoothing\nMit geom_smooth() kann ggplot eine Trendlinie auf der Baiss von Punktdaten berechnen. Die zugrunde liegende statistische Methode kann selbst gewählt werden (ohne Angabe verwendet ggplot bei &lt; 1’000 Messungen stats::loess, ansonsten mgcv::gam)\n\np &lt;- p +\n  geom_smooth(colour = \"black\")\n\np"
  },
  {
    "objectID": "infovis/Infovis1_Script_eda.html",
    "href": "infovis/Infovis1_Script_eda.html",
    "title": "Infovis 1: Demo B",
    "section": "",
    "text": "library(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"scales\")\n\n# create some data about age and height of people\npeople &lt;- data.frame(\n  ID = c(1:30),\n  \n  age = c(5.0, 7.0, 6.5 ,9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0,\n          63.5 ,65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3,\n          15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3),\n  \n  height = c(0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13,\n             1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69,\n             1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80),\n  \n  weight = c(45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6,\n             84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5,\n             48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3)\n)\n\n# build a scatterplot for a first inspection\nggplot(people, aes(x=age, y=height)) + \n  geom_point() \n\n\n\n\n\nggplot(people, aes(x=age, y=height)) + \n  geom_point() +\n  scale_y_continuous(limits = c(0.75, 2))\n\n\n\n\n# Go to help page: http://docs.ggplot2.org/current/ -&gt; Search for icon of fit-line\n# http://docs.ggplot2.org/current/geom_smooth.html\n\n\n# build a scatterplot for a first inspection, with regression line\nggplot(people, aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n# stem and leaf plot\nstem(people$height)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 25593\n##   10 | 037\n##   12 | 523\n##   14 | 19556\n##   16 | 255789916\n##   18 | 04774\nstem(people$height, scale=2)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 2559\n##    9 | 3\n##   10 | \n##   11 | 037\n##   12 | 5\n##   13 | 23\n##   14 | 19\n##   15 | 556\n##   16 | 2557899\n##   17 | 16\n##   18 | 0477\n##   19 | 4\n\n\n# explore the two variables with box-whiskerplots\nsummary(people$age)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    5.00    8.70   30.20   59.14   65.15  512.30\nboxplot(people$age)\n\n\n\n\n\nsummary(people$height)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.820   1.190   1.555   1.455   1.690   1.940\nboxplot(people$height)\n\n\n\n\n\n# explore data with a histgram\nggplot(people, aes(x=age)) + \n  geom_histogram(binwidth=20)  \n\n\n\n\n\ndensity(x = people$height)\n## \n## Call:\n##  density.default(x = people$height)\n## \n## Data: people$height (30 obs.);   Bandwidth 'bw' = 0.1576\n## \n##        x                y           \n##  Min.   :0.3472   Min.   :0.001593  \n##  1st Qu.:0.8636   1st Qu.:0.102953  \n##  Median :1.3800   Median :0.510601  \n##  Mean   :1.3800   Mean   :0.483553  \n##  3rd Qu.:1.8964   3rd Qu.:0.722660  \n##  Max.   :2.4128   Max.   :1.216350\n\n# re-expression: use log or sqrt axes\n#\n# Find here guideline about scaling axes \n# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/\n# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html\n\n\n# logarithmic axis: respond to skewness in the data, e.g. log10 \nggplot(people, aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth() +\n  scale_x_log10()\n\n\n\n\n\n# outliers: Remove very small and very old people\n\npeopleClean &lt;- people |&gt;\n  filter(ID != 27) |&gt;    # Diese Person war zu klein.\n  filter(age &lt; 100)       # Fehler in der Erhebung des Alters\n\n\nggplot(peopleClean, aes(x=age)) + \n  geom_histogram(binwidth=10)\n\n\n\n\n\nggplot(peopleClean, aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n# with custom binwidth\nggplot(peopleClean, aes(x=age)) + \n  geom_histogram(binwidth=10) + \n  theme_bw() # specifying the theme\n\n\n\n\n\n# quadratic axis\nggplot(peopleClean, aes(x=age, y=height)) + \n  geom_point() + scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth(method=\"lm\", fill='lightblue', size=0.5, alpha=0.5) + \n  scale_x_sqrt()\n\n\n\n\n\n# filter \"teenies\": No trend\nfilter(peopleClean, age &lt; 15) |&gt;\n  ggplot(aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth(method=\"lm\", fill='lightblue', size=0.5, alpha=0.5)\n\n\n\n\n\n# filter \"teenies\": No trend\npeopleClean |&gt;\n  filter(age &gt; 55) |&gt;\n  ggplot(aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth(method=\"lm\", fill='lightblue', size=0.5, alpha=0.5)\n\n\n\n\n\n# Onwards towards multidimensional data\n\n# Finally, make a scatterplot matrix\npairs(peopleClean[,2:4], panel=panel.smooth)\n\n\n\n\n\npairs(peopleClean[,2:4], panel=panel.smooth)"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-1",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-1",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_kanton.csv Datensatz:\nTipp:\n\nNutze ggplot(kanton, aes(auslanderanteil, ja_anteil)) um den ggplot zu initiieren. Füge danach ein einen Punkte Layer hinzu (geom_point())\nNutze coord_fixed() um die beiden Achsen in ein fixes Verhältnis zu setzen (1:1).\nOptional:\n\nSetze die Achsen Start- und Endwerte mittels scale_y_continuous bzw. scale_x_continuous.\nSetze analog Kovic (2014) die breaks (0.0, 0.1…0.7) manuell (innerhalb scale_*_continuous)\nNutze labs() für die Beschriftung der Achsen\n\n\n\n\nCode\n# Lösung zu Aufgabe 1\n\nplot1 &lt;- ggplot(kanton, aes(auslanderanteil, ja_anteil)) +\n  geom_point() +\n  coord_fixed(1) +\n  scale_y_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits =  c(0,0.7)) +\n  scale_x_continuous(breaks = c(0,0.1,0.3,0.5,0.7),limits =  c(0,0.7)) +\n  labs(y = \"Anteil Ja-Stimmen\", x = \"Ausländeranteil\")\n\nplot1"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-2",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-2",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot:\nTipp:\n\nNutze geom_smooth\n\n\n\nCode\n# Lösung zu Aufgabe 2\n\nplot1 +\n  geom_smooth()"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-3",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-3",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nImportiere die Gemeindedaten tagi_data_gemeinden.csv.\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_point()\nNutze labs()\nNutze coord_fixed()\n\n\n\nCode\n# Lösung zu Aufgabe 3\n\ngemeinde &lt;- read_csv(\"datasets/infovis/tagi_data_gemeinden.csv\")\n\nplot2 &lt;- ggplot(gemeinde, aes(anteil_ausl, anteil_ja)) +\n  geom_point() +\n  labs(x = \"Ausländeranteil\",y = \"Anteil Ja-Stimmen\") +\n  coord_fixed(1) +\n  lims(x = c(0,1), y = c(0,1))\n\nplot2"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-4",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-4",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth\n\n\n\nCode\n# Lösung zu Aufgabe 4\n\nplot2 +\n  geom_smooth()"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-5",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-5",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze facet_wrap um einen Plot pro Kanton darzustellen.\n\n\n\nCode\n# Lösung zu Aufgabe 5\n\nplot3 &lt;- plot2 +\n  facet_wrap(~kanton)\nplot3"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-6",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-6",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth\n\n\n\nCode\n# Lösung zu Aufgabe 6\n\nplot3 +\n  geom_smooth()"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-7",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-7",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nRekonstrukturieren folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze facet_wrap\n\n\n\nCode\n# Lösung zu Aufgabe 7\n\nplot4 &lt;- plot2 +\n  facet_wrap(~quantile)\nplot4"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-8",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-8",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth\n\n\n\nCode\n# Lösung zu Aufgabe 8\n\nplot4 +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nKovic, Marko. 2014. „Je weniger Ausländer, desto mehr Ja-Stimmen? Wirklich?“ Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-1",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-1",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nMache aus der wide table eine long table die wie folgt aussieht.\n\n\nCode\ntemperature_long &lt;- pivot_longer(temperature, -time, names_to = \"station\", values_to = \"temperature\")\n\nknitr::kable(head(temperature_long))\n\n\nLade anschliessend temperature_2005_metadata.csv herunter und verbinde die beiden Datensätze mit einem left_join via station (bzw. stn).\n\n\nCode\nmetadata &lt;- read_csv(\"datasets/infovis/temperature_2005_metadata.csv\")\n\ntemperature_long &lt;- left_join(temperature_long, metadata, by = c(station = \"stn\"))"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-2",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-2",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nErstelle ein Scatterplot (time vs. temperature) wobei die Punkte aufgrund ihrer Meereshöhe eingefärbt werden sollen. Tiefe Werte sollen dabei blau eingefärbt werden und hohe Werte rot (scale_color_gradient). Verkleinere die Punkte um übermässiges Überplotten der Punkten zu vermeiden (size =). Weiter sollen auf der x-Achse im Abstand von 3 Monaten der jeweilige Monat vermerkt sein (date_breaks bzw. date_labels von scale_x_datetime()).\n\n\nCode\n# Musterlösung\n\nggplot(temperature_long, aes(time,temperature, color = Meereshoehe)) +\n  geom_point(size = 0.5) +\n  labs(x = \"\", y = \"Temperatur in ° Celsius\") +\n  scale_x_datetime(date_breaks = \"3 months\", date_labels = \"%b\")  +\n  scale_color_gradient(low = \"blue\", high = \"red\")"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-3",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-3",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nErstelle eine Zusatzvariabel Date mit dem Datum der jeweiligen Messung ( mit as.Date). Nutze diese Spalte um die Tagesmitteltemperatur pro Station zu berechnen (mit summarise()).\nUm die Metadaten (Name, Meereshoehe, x, y) nicht zu verlieren kannst du den Join aus der ersten Übung wieder ausführen. Alternativ (schneller aber auch schwerer zu verstehen) kannst du diese Variabeln innerhalb deines group_by verwenden.\n\n\nCode\ntemperature_long &lt;- temperature_long |&gt;\n  mutate(time = as.Date(time)) |&gt;\n  group_by(time, station, Name, Meereshoehe, x, y) |&gt;\n  summarise(temperature = mean(temperature))"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-4",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-4",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nWiederhole nun den Plot aus der ersten Aufgabe mit den aggregierten Daten aus der vorherigen Aufgabe. Um die labels korrekt zu setzen musst du scale_x_datetime mit scale_x_date ersetzen.\n\n\nCode\np &lt;- ggplot(temperature_long, aes(time,temperature, color = Meereshoehe)) +\n  geom_point(size = 0.5) +\n  labs(x = \"\", y = \"Temperatur in ° Celsius\") +\n  scale_x_date(date_breaks = \"3 months\", date_labels = \"%b\")  +\n  scale_color_gradient(low = \"blue\", high = \"red\")\n\np"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-5",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-5",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nFüge am obigen Plot eine schwarze, gestrichelte Trendlinie hinzu.\n\n\nCode\n# Musterlösung\n\np &lt;- p +\n  stat_smooth(colour = \"black\",lty = 2)\n\np"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-6",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-6",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nPositioniere die Legende oberhalb des Plots (nutze dazu theme() mit legend.position).\n\n\nCode\n# Musterlösung\n\np &lt;- p + \n  theme(legend.position = \"top\")\n\np"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-7-optional-fortgeschritten",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-7-optional-fortgeschritten",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 7 (optional, fortgeschritten)",
    "text": "Aufgabe 7 (optional, fortgeschritten)\nFüge den Temperaturwerten auf der y-Ache ein °C hinzu (siehe unten und studiere diesen Tipp zur Hilfe).\n\n\nCode\n# Musterlösung\n\np &lt;- p +\n  scale_y_continuous(labels = function(x)paste0(x,\"°C\")) +\n  labs(x = \"Kalenderwoche\", y = \"Temperatur\")\n\np"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-8",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-8",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nJetzt verlassen wir den Scatterplot und machen einen Boxplot mit den Temperaturdaten. Färbe die Boxplots wieder in Abhängigkeit der Meereshöhe ein.\n\nBeachte den Unterschied zwischen colour = und fill =\nBeachte den Unterschied zwischen facet_wrap() und facet_grid()\nfacet_grid() braucht übrigens noch einen Punkt (.) zur Tilde (~).\nBeachte den Unterschied zwischen “.~” und “~.” bei facet_grid()\nverschiebe nach Bedarf die Legende\n\n\n\nCode\n# Musterlösung\n\ntemperature_long &lt;- mutate(temperature_long, monat = month(time,label = T,abbr = F))\n\nggplot(temperature_long, aes(monat,temperature, fill = Meereshoehe)) +\n  geom_boxplot() +\n  labs(x = \"Station\", y = \"Temperatur\") +\n  facet_wrap(~station) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-9",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-9",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 9",
    "text": "Aufgabe 9\nAls letzter wichtiger Plottyp noch zwei Übungen zum Histogramm. Erstelle ein Histogramm geom_histogram() mit den Temperaturwerten. Teile dazu die Stationen in verschiedene Höhenlagen ein (Tieflage [&lt; 400 m], Mittellage [400 - 600 m] und Hochlage [&gt; 600 m]). Vergleiche die Verteilung der Temperaturwerte in den verschiedenen Lagen mit einem Histogramm.\nTip: Nutze cut um die Stationen in die drei Gruppen aufzuteilen\n\n\nCode\n# Musterlösung\n\ntemperature_long &lt;- temperature_long |&gt;\n  mutate(lage = cut(Meereshoehe, c(0, 400, 600,1000),labels = c(\"Tieflage\", \"Mittellage\", \"Hochlage\")))\n\nggplot(temperature_long, aes(temperature)) +\n  geom_histogram() +\n  facet_grid(~lage) +\n  labs(x = \"Lage\", y = \"Temperatur\") +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1))"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-1-parallel-coordinate-plots",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-1-parallel-coordinate-plots",
    "title": "Infovis 2: Übung B",
    "section": "Aufgabe 1: Parallel coordinate plots",
    "text": "Aufgabe 1: Parallel coordinate plots\nErstelle einen parallel coordinate plot. Dafür eignet sich der integrierte Datensatz mtcars. Extrahiere die Fahrzeugnamen mit rownames_to_column.\nZudem müssen die Werte jeweiles auf eine gemeinsame Skala normalisiert werden. Hierfür kannst du die Funktion scales::rescale verwenden.\n\n\nCode\nmtcars2 &lt;- mtcars |&gt;\n  tibble::rownames_to_column(\"car\") |&gt;\n  pivot_longer(-car)\n\nmtcars2 &lt;- mtcars2 |&gt;\n  group_by(name) |&gt;\n  mutate(value_scaled = scales::rescale(value))\n\n\nSo sieht der fertige Plot aus:\n\n\nCode\nmtcars2 &lt;- mtcars2 |&gt;\n  group_by(car) |&gt;\n  mutate(gear = value[name == \"gear\"])\n\nggplot(mtcars2, aes(name, value_scaled, group = car, color = factor(gear))) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.title.y = element_blank())"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-2-polar-plot-mit-biber-daten",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-2-polar-plot-mit-biber-daten",
    "title": "Infovis 2: Übung B",
    "section": "Aufgabe 2: Polar Plot mit Biber Daten",
    "text": "Aufgabe 2: Polar Plot mit Biber Daten\nPolar Plots eignen sich unter anderem für Daten, die zyklischer Natur sind, wie zum Beispiel zeitlich geprägte Daten (Tages-, Wochen-, oder Jahresrhythmen). Aus den Beispiels-Datensätzen habe ich zwei Datensätze gefunden, die zeitlich geprägt sind:\n\nbeaver1 und beaver2\nAirPassenger\n\nBeide Datensätze müssen noch etwas umgeformt werden, bevor wir sie für einen Radialplot verwenden können. In Aufgabe 2 verwenden wir die Biber-Datensätze, in der nächsten Aufgabe (3) die Passagier-Daten.\nWenn wir die Daten von beiden Bibern verwenden wollen, müssen wir diese noch zusammenfügen:\n\n\nCode\nbeaver1_new &lt;- beaver1 |&gt;\n  mutate(beaver = \"nr1\")\n\nbeaver2_new &lt;- beaver2 |&gt;\n  mutate(beaver = \"nr2\")\n\nbeaver_new &lt;- rbind(beaver1_new,beaver2_new)\n\n\nZudem müssen wir die Zeitangabe noch anpassen: Gemäss der Datenbeschreibung handelt es sich bei der Zeitangabe um ein sehr programmier-unfreundliches Format. 3:30 wird als “0330” notiert. Wir müssen diese Zeitangabe, noch in ein Dezimalsystem umwandeln:\n\n\nCode\nbeaver_new &lt;- beaver_new |&gt;\n  mutate(\n    hour_dec = (time/100)%/%1,         # Ganze Stunden (mittels ganzzaliger Division)\n    min_dec = (time/100)%%1/0.6,       # Dezimalminuten (15 min wird zu 0.25, via Modulo)\n    hour_min_dec = hour_dec+min_dec    # Dezimal-Zeitangabe (03:30 wird zu 3.5)\n    ) \n\n\nSo sieht der fertige Plot aus:\n\n\nCode\n# Lösung Aufgabe 2\n\nbeaver_new |&gt;\n  ggplot(aes(hour_min_dec, temp, color = beaver)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0,23,2)) +\n  coord_polar() +\n  theme_minimal() +\n  theme(axis.title =  element_blank())"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-3-raster-visualisierung-mit-flugpassagieren",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-3-raster-visualisierung-mit-flugpassagieren",
    "title": "Infovis 2: Übung B",
    "section": "Aufgabe 3: Raster Visualisierung mit Flugpassagieren",
    "text": "Aufgabe 3: Raster Visualisierung mit Flugpassagieren\nAnalog Aufgabe 2, dieses Mal mit dem Datensatz AirPassanger\nAirPassengers kommt in einem Format daher, das ich selbst noch gar nicht kannte. Es sieht zwar aus wie ein data.frame oder eine matrix, ist aber von der Klasse ts.\n\n\nCode\nAirPassengers\n\nclass(AirPassengers)\n\n\nDamit wir den Datensatz verwenden können, müssen wir ihn zuerst in eine matrix umwandeln. Wie das geht habe ich hier erfahren.\n\n\nCode\nAirPassengers2 &lt;- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c)\n\nAirPassengers2\n\n\nAus der matrix muss noch ein Dataframe her, zudem müssen wir aus der breiten Tabelle eine lange Tabelle machen:\n\n\nCode\nAirPassengers3 &lt;- AirPassengers2 |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"year\") |&gt;\n  pivot_longer(-year, names_to = \"month\", values_to = \"n\") |&gt;\n  mutate(\n    # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln\n    month = factor(month, levels = month.abb,ordered = T),\n    month_numb = as.integer(month),\n    year = as.integer(year)\n  )\n\n\nSo sieht der fertige Plot aus:\n\n\nCode\nggplot(AirPassengers3, aes(month, year, fill = n)) +\n  geom_raster() +\n  scale_y_reverse() +\n  scale_fill_viridis_c(guide = guide_colourbar(barwidth = 15, title.position = \"top\")) +\n  theme_minimal() +\n  labs(fill = \"Anzahl Passagiere\") +\n  coord_equal() +\n  theme(axis.title = element_blank(), legend.position = \"bottom\")"
  },
  {
    "objectID": "Stat1-4.html",
    "href": "Stat1-4.html",
    "title": "Statistik 1 - 4",
    "section": "",
    "text": "Statistik 1\nIn Statistik 1 lernen die Studierenden, was (Inferenz-) Statistik im Kern leistet und warum sie für wissenschaftliche Erkenntnis (in den meisten Disziplinen) unentbehrlich ist. Nach einer Wiederholung der Rolle von Hypothesen wird erläutert, wie Hypothesentests in der frequentist-Statistik umgesetzt werden, einschliesslich p-Werten und Signifikanz-Levels. Die praktische Statistik beginnt mit den beiden einfachsten Fällen, dem Chi-Quadrat-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Abschliessend beschäftigen wir uns damit, wie man Ergebnisse statistischer Analysen am besten in Abbildungen, Tabellen und Text darstellt.\n\n\n\n\n\n\nStatistik 2\nIn Statistik 2 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R (sowie teilweise ihrer „nicht-parametrischen“ bzw. „robusten“ Äquivalente). Am Anfang steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind. Dann beschäftigen wir uns mit Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten. Abschliessend besprechen wir, was die grosse Gruppe linearer Modelle (Befehl lm in R) auszeichnet.\n\n\n\n\nStatistik 3\nStatistik 3 fassen wir zu Beginn den generellen Ablauf inferenzstatistischer Analysen in einem Flussdiagramm zusammen. Dann wird die ANCOVA als eine Technik vorgestellt, die eine ANOVA mit einer linearen Regression verbindet. Danach geht es um komplexere Versionen linearer Regressionen. Hier betrachten wir polynomiale Regressionen, die z. B. einen Test auf unimodale Beziehungen erlaubt, indem man dieselbe Prädiktorvariable linear und quadriert einspeist. Multiple Regressionen versuchen dagegen, eine abhängige Variable durch zwei oder mehr verschieden Prädiktorvariablen zu erklären. Wir thematisieren verschiedene dabei auftretende Probleme und ihre Lösung, insbesondere den Umgang mit korrelierten Prädiktoren und das Aufspüren des besten unter mehreren möglichen statistischen Modellen. Hieran wird auch der informatian theoretician-Ansatz der Statistik und die multimodel inference eingeführt.\n\n\nStatistik 4\nHeute geht es hauptsächlich um generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Indem sie Fehler- und Varianzstrukturen explizit modellieren, ist man nicht mehr an Normalverteilung der Residuen und Varianzhomogenität gebunden. Bei generalized linear regressions muss man sich zwischen verschiedenen Verteilungen und link-Strukturen entscheiden. Spezifisch werden wir uns die Poisson-Regressionen für Zähldaten und die logistische Regression für ja/nein-Daten anschauen. Danach folgt ein Einstieg in nicht-lineare Regressionen, die es erlauben, etwa Potenzgesetze oder Sättigungsfunktionen direkt zu modellieren. Zum Abschluss gibt es einen Ausblick auf Glättungsverfahren (LOWESS) und general additive models (GAMs).\n\n\n\n\n\n\n\n\n\n\n\nTitel\n\n\nDatum\n\n\nLesson\n\n\nThema\n\n\n\n\n\n\nStat1: Demo\n\n\n2022-10-31\n\n\nStat1\n\n\nGrundlagen der Statistik\n\n\n\n\nStat1: NOVANIMAL\n\n\n2022-10-31\n\n\nStat1\n\n\nGrundlagen der Statistik\n\n\n\n\nStat1: Übung\n\n\n2022-10-31\n\n\nStat1\n\n\nGrundlagen der Statistik\n\n\n\n\nStat1: Lösung\n\n\n2022-10-31\n\n\nStat1\n\n\nGrundlagen der Statistik\n\n\n\n\nStat2: Demo\n\n\n2022-11-01\n\n\nStat2\n\n\nEinführung in lineare Modelle\n\n\n\n\nStat2: Übung\n\n\n2022-11-01\n\n\nStat2\n\n\nEinführung in lineare Modelle\n\n\n\n\nStat2: Lösung Beispiel\n\n\n2022-11-01\n\n\nStat2\n\n\nEinführung in lineare Modelle\n\n\n\n\nStat2: Lösung 2.1\n\n\n2022-11-01\n\n\nStat2\n\n\nEinführung in lineare Modelle\n\n\n\n\nStat2: Lösung 2.2 & 2.3S\n\n\n2022-11-01\n\n\nStat2\n\n\nEinführung in lineare Modelle\n\n\n\n\nStat2: Lösung 2.3N\n\n\n2022-11-01\n\n\nStat2\n\n\nEinführung in lineare Modelle\n\n\n\n\nStat3: Demo\n\n\n2022-11-07\n\n\nStat3\n\n\nLineare Modelle II\n\n\n\n\nStat3: Übung\n\n\n2022-11-07\n\n\nStat3\n\n\nLineare Modelle II\n\n\n\n\nStat3: Lösung\n\n\n2022-11-07\n\n\nStat3\n\n\nLineare Modelle II\n\n\n\n\nStat4: Demo\n\n\n2022-11-08\n\n\nStat4\n\n\nKomplexere Regressionsmethoden\n\n\n\n\nStat4: Übung\n\n\n2022-11-08\n\n\nStat4\n\n\nKomplexere Regressionsmethoden\n\n\n\n\nStat4: Lösung 4.1\n\n\n2022-11-08\n\n\nStat4\n\n\nKomplexere Regressionsmethoden\n\n\n\n\nStat4: Lösung 4.2N\n\n\n2022-11-08\n\n\nStat4\n\n\nKomplexere Regressionsmethoden\n\n\n\n\nStat4: Lösung 4.2S\n\n\n2022-11-08\n\n\nStat4\n\n\nKomplexere Regressionsmethoden\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#daten-generieren-und-anschauen",
    "href": "stat1-4/Statistik1_Demo.html#daten-generieren-und-anschauen",
    "title": "Stat1: Demo",
    "section": "Daten generieren und anschauen",
    "text": "Daten generieren und anschauen\n\na &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14)\nb &lt;- c(12, 15, 16, 7, 8, 10, 12, 11, 13, 10)\nblume &lt;- data.frame(a,b)\nblume\n##     a  b\n## 1  20 12\n## 2  19 15\n## 3  25 16\n## 4  10  7\n## 5   8  8\n## 6  15 10\n## 7  13 12\n## 8  18 11\n## 9  11 13\n## 10 14 10\nsummary(blume)\n##        a               b        \n##  Min.   : 8.00   Min.   : 7.00  \n##  1st Qu.:11.50   1st Qu.:10.00  \n##  Median :14.50   Median :11.50  \n##  Mean   :15.30   Mean   :11.40  \n##  3rd Qu.:18.75   3rd Qu.:12.75  \n##  Max.   :25.00   Max.   :16.00\nboxplot(blume$a, blume$b)\n\n\n\nboxplot(blume)\n\n\n\nhist(blume$a)\n\n\n\nhist(blume$b)"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#zweiseitiger-t-test",
    "href": "stat1-4/Statistik1_Demo.html#zweiseitiger-t-test",
    "title": "Stat1: Demo",
    "section": "Zweiseitiger t-Test",
    "text": "Zweiseitiger t-Test\n\nt.test(blume$a, blume$b) # Zweiseitig \"Test auf a ≠ b\" (default)\n## \n##  Welch Two Sample t-test\n## \n## data:  blume$a and blume$b\n## t = 2.0797, df = 13.907, p-value = 0.05654\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.1245926  7.9245926\n## sample estimates:\n## mean of x mean of y \n##      15.3      11.4"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#einseitiger-t-test",
    "href": "stat1-4/Statistik1_Demo.html#einseitiger-t-test",
    "title": "Stat1: Demo",
    "section": "Einseitiger t-Test",
    "text": "Einseitiger t-Test\n\nt.test(blume$a, blume$b, alternative = \"greater\") # Einseitig \"Test auf a &gt; b\"\n## \n##  Welch Two Sample t-test\n## \n## data:  blume$a and blume$b\n## t = 2.0797, df = 13.907, p-value = 0.02827\n## alternative hypothesis: true difference in means is greater than 0\n## 95 percent confidence interval:\n##  0.5954947       Inf\n## sample estimates:\n## mean of x mean of y \n##      15.3      11.4\nt.test(blume$a, blume$b, alternative = \"less\") # Einseitig \"Test auf a &lt; b\"\n## \n##  Welch Two Sample t-test\n## \n## data:  blume$a and blume$b\n## t = 2.0797, df = 13.907, p-value = 0.9717\n## alternative hypothesis: true difference in means is less than 0\n## 95 percent confidence interval:\n##      -Inf 7.204505\n## sample estimates:\n## mean of x mean of y \n##      15.3      11.4"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#klassischer-t-test-vs.-welch-test",
    "href": "stat1-4/Statistik1_Demo.html#klassischer-t-test-vs.-welch-test",
    "title": "Stat1: Demo",
    "section": "Klassischer t-Test vs. Welch Test",
    "text": "Klassischer t-Test vs. Welch Test\n\n# Varianzen gleich: klassischer t-Test\nt.test(blume$a, blume$b, var.equal = TRUE) \n## \n##  Two Sample t-test\n## \n## data:  blume$a and blume$b\n## t = 2.0797, df = 18, p-value = 0.05212\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.03981237  7.83981237\n## sample estimates:\n## mean of x mean of y \n##      15.3      11.4\n\n# Varianzen ungleich: Welch's t-Test (siehe Titelzeile des R-Outputs!)\nt.test(blume$a, blume$b) # dasselbe wie var.equal = FALSE\n## \n##  Welch Two Sample t-test\n## \n## data:  blume$a and blume$b\n## t = 2.0797, df = 13.907, p-value = 0.05654\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.1245926  7.9245926\n## sample estimates:\n## mean of x mean of y \n##      15.3      11.4"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#gepaarter-t-test",
    "href": "stat1-4/Statistik1_Demo.html#gepaarter-t-test",
    "title": "Stat1: Demo",
    "section": "Gepaarter t-Test",
    "text": "Gepaarter t-Test\n\n# Gepaarter t-Test: erster Wert von a wird mit erstem Wert von\n# b gepaart, zweiter Wert von a mit zweitem von b ect.\nt.test(blume$a, blume$b, paired = TRUE)\n## \n##  Paired t-test\n## \n## data:  blume$a and blume$b\n## t = 3.4821, df = 9, p-value = 0.006916\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  1.366339 6.433661\n## sample estimates:\n## mean difference \n##             3.9\nt.test(blume$a, blume$b, paired = TRUE, alternative = \"greater\")\n## \n##  Paired t-test\n## \n## data:  blume$a and blume$b\n## t = 3.4821, df = 9, p-value = 0.003458\n## alternative hypothesis: true mean difference is greater than 0\n## 95 percent confidence interval:\n##  1.846877      Inf\n## sample estimates:\n## mean difference \n##             3.9\n\nDasselbe mit einer “long table”\n\n# \"Long table\" erstellen \ncultivar &lt;- c(rep(\"a\", 10), rep(\"b\", 10))\nsize &lt;- c(a, b)\nblume.long &lt;- data.frame(cultivar, size)\n\n# nicht mehr benötitgten Objekte entfernen\nrm(size)\nrm(cultivar)\n\n# Daten anschauen\nsummary(blume.long)             \n##    cultivar              size      \n##  Length:20          Min.   : 7.00  \n##  Class :character   1st Qu.:10.00  \n##  Mode  :character   Median :12.50  \n##                     Mean   :13.35  \n##                     3rd Qu.:15.25  \n##                     Max.   :25.00\nhead(blume.long)\n##   cultivar size\n## 1        a   20\n## 2        a   19\n## 3        a   25\n## 4        a   10\n## 5        a    8\n## 6        a   15\nboxplot(size~cultivar, data = blume.long)\n\n\n\n\n# Tests durchführen\nt.test(size~cultivar, blume.long, var.equal = TRUE)\n## \n##  Two Sample t-test\n## \n## data:  size by cultivar\n## t = 2.0797, df = 18, p-value = 0.05212\n## alternative hypothesis: true difference in means between group a and group b is not equal to 0\n## 95 percent confidence interval:\n##  -0.03981237  7.83981237\n## sample estimates:\n## mean in group a mean in group b \n##            15.3            11.4\nt.test(size~cultivar, blume.long, paired = TRUE)\n## \n##  Paired t-test\n## \n## data:  size by cultivar\n## t = 3.4821, df = 9, p-value = 0.006916\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  1.366339 6.433661\n## sample estimates:\n## mean difference \n##             3.9"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#base-r-vs.-ggplot2",
    "href": "stat1-4/Statistik1_Demo.html#base-r-vs.-ggplot2",
    "title": "Stat1: Demo",
    "section": "Base R vs. ggplot2",
    "text": "Base R vs. ggplot2\n\nlibrary(tidyverse)\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot()\n\n\n\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot() + theme_classic()\n\n\n\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot(size = 1) + theme_classic()+\ntheme(axis.line = element_line(size = 1)) + theme(axis.title = element_text(size = 14))+\ntheme(axis.text = element_text(size = 14))\n\n\n\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot(size=1) + theme_classic()+\n  theme(axis.line = element_line(size = 1), axis.ticks = element_line(size = 1), \n       axis.text = element_text(size = 20), axis.title = element_text(size = 20))\n\n\n\n\nDefinieren von mytheme mit allen gewünschten Settings, das man zu Beginn einer Sitzung einmal laden und dann immer wieder ausführen kann (statt des langen Codes)\n\nmytheme &lt;- theme_classic() + \n  theme(axis.line = element_line(color = \"black\", size=1), \n        axis.text = element_text(size = 20, color = \"black\"), \n        axis.title = element_text(size = 20, color = \"black\"), \n        axis.ticks = element_line(size = 1, color = \"black\"), \n        axis.ticks.length = unit(.5, \"cm\"))\n\n\n# Schöne Boxplots erstellen\n\nggplot(blume.long, aes(cultivar, size)) + \n  geom_boxplot(size = 1) +\n  mytheme\n\n\n\n\nt_test &lt;- t.test(size~cultivar, blume.long)\n\n# Mit p-Wert im Plot\nggplot(blume.long, aes(cultivar, size)) + \n  geom_boxplot(size = 1) + \n  mytheme +\n  annotate(\"text\", x = \"b\", y = 24, \n  label = paste0(\"italic(p) == \", round(t_test$p.value, 3)), parse = TRUE, size = 8)\n\n\n\n\n# Ohne p-Wert im Plot (da dieser &gt; 0.05)\nggplot (blume.long, aes(cultivar,size)) + \n  geom_boxplot(size = 1) + \n  mytheme +\n  labs(x=\"Cultivar\",y=\"Size (cm)\")"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#binomialtest",
    "href": "stat1-4/Statistik1_Demo.html#binomialtest",
    "title": "Stat1: Demo",
    "section": "Binomialtest",
    "text": "Binomialtest\nIn Klammern übergibt man die Anzahl der Erfolge und die Stichprobengrösse\n\nbinom.test(84, 200) # Anzahl Frauen im Nationalrat (≙ 42.0 %; Stand 2019) \n## \n##  Exact binomial test\n## \n## data:  84 and 200\n## number of successes = 84, number of trials = 200, p-value = 0.02813\n## alternative hypothesis: true probability of success is not equal to 0.5\n## 95 percent confidence interval:\n##  0.3507439 0.4916638\n## sample estimates:\n## probability of success \n##                   0.42\nbinom.test(116, 200) # Anzahl Männer im Nationalrat (≙ 58.0 %; Stand 2019) \n## \n##  Exact binomial test\n## \n## data:  116 and 200\n## number of successes = 116, number of trials = 200, p-value = 0.02813\n## alternative hypothesis: true probability of success is not equal to 0.5\n## 95 percent confidence interval:\n##  0.5083362 0.6492561\n## sample estimates:\n## probability of success \n##                   0.58\nbinom.test(3, 7) # Anzahl Frauen im Bundesrat (≙ 42.9 %; Stand 2019)\n## \n##  Exact binomial test\n## \n## data:  3 and 7\n## number of successes = 3, number of trials = 7, p-value = 1\n## alternative hypothesis: true probability of success is not equal to 0.5\n## 95 percent confidence interval:\n##  0.09898828 0.81594843\n## sample estimates:\n## probability of success \n##              0.4285714"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#chi-quadrat-test-fishers-test",
    "href": "stat1-4/Statistik1_Demo.html#chi-quadrat-test-fishers-test",
    "title": "Stat1: Demo",
    "section": "Chi-Quadrat-Test & Fishers Test",
    "text": "Chi-Quadrat-Test & Fishers Test\nErmitteln des kritischen Wertes\n\nqchisq(0.95, 1)\n## [1] 3.841459"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#direkter-test-in-r-dazu-werte-als-matrix-nötig",
    "href": "stat1-4/Statistik1_Demo.html#direkter-test-in-r-dazu-werte-als-matrix-nötig",
    "title": "Stat1: Demo",
    "section": "Direkter Test in R (dazu Werte als Matrix nötig)",
    "text": "Direkter Test in R (dazu Werte als Matrix nötig)\n\n# Matrix mit Haarfarbe&Augenfarbe-Kombidnationen erstellen\n# 38 blond&blau, 14 dunkel&blau, 11 blond&braun,, 51 dunkel&braun\ncount &lt;- matrix(c(38, 14, 11, 51), nrow = 2)\ncount # Check\n##      [,1] [,2]\n## [1,]   38   11\n## [2,]   14   51\nrownames(count) &lt;- c(\"blond\", \"dunkel\") # Benennen für Übersicht\ncolnames(count) &lt;- c(\"blau\", \"braun\") #  Benennen für Übersicht\ncount # Check \n##        blau braun\n## blond    38    11\n## dunkel   14    51\n\n# Tests durchführen\nchisq.test(count)\n## \n##  Pearson's Chi-squared test with Yates' continuity correction\n## \n## data:  count\n## X-squared = 33.112, df = 1, p-value = 8.7e-09\nfisher.test(count)\n## \n##  Fisher's Exact Test for Count Data\n## \n## data:  count\n## p-value = 2.099e-09\n## alternative hypothesis: true odds ratio is not equal to 1\n## 95 percent confidence interval:\n##   4.746351 34.118920\n## sample estimates:\n## odds ratio \n##   12.22697"
  },
  {
    "objectID": "stat1-4/Statistik1_Novanimal.html",
    "href": "stat1-4/Statistik1_Novanimal.html",
    "title": "Stat1: NOVANIMAL",
    "section": "",
    "text": "Im Forschungsprojekt NOVANIMAL wird u.a. der Frage nachgegangen, was es braucht, damit Menschen freiwillig weniger tierische Produkte konsumieren? Ein interessanter Ansatzpunkt ist die Ausser-Haus-Verpflegung. Gemäss der ersten in den Jahren 2014/2015 durchgeführten nationalen Ernährungserhebung menuCH essen 70 % der Bevölkerung zwischen 18 und 75 Jahren am Mittag auswärts (Bochud u. a. 2017). Daher rückt die Gastronomie als zentraler Akteur einer innovativen und nachhaltigen Ernährungswirtschaft ins Blickfeld. Welche Innovationen in der Gastronomie könnten dazu beitragen, den Pro-Kopf-Verbrauch an tierischen Nahrungsmitteln zu senken?\nDazu wurde u.a. ein Experiment in zwei Hochschulmensen durchgeführt. Forschungsleitend war die Frage, wie die Gäste dazu bewogen werden können, häufiger vegetarische oder vegane Gerichte zu wählen. Konkret wurde untersucht, wie die Gäste auf ein verändertes Menü-Angebot mit einem höheren Anteil an vegetarischen und veganen Gerichten reagieren. Das Experiment fand während 12 Wochen statt und bestand aus zwei Mensazyklen à 6 Wochen. Über den gesamten Untersuchungszeitraum werden insgesamt 90 verschiedene Gerichte angeboten. In den 6 Referenz- bzw. Basiswochen wurden zwei fleisch- oder fischhaltige Menüs und ein vegetarisches Menü angeboten. In den 6 Interventionswochen wurde das Verhältnis umgekehrt und es wurden ein veganes, ein vegetarisches und ein fleisch- oder fischhaltiges Gericht angeboten. Basis- und Interventionsangebote wechselten wöchentlich ab. Während der gesamten 12 Wochen konnten die Gäste jeweils auf ein Buffet ausweichen und ihre Mahlzeit aus warmen und kalten Komponenten selber zusammenstellen. Die Gerichte wurden über drei vorgegebene Menülinien (F, K, W) randomisiert angeboten.\n\n\n\nDie Abbildung zeigt das Versuchsdesign der ersten 6 Experimentwochen (Kalenderwoche 40 bis 45).\n\n\nMehr Informationen über das Forschungsprojekt NOVANIMAL findet ihr auf der Webpage.\n\n\n\n\nBochud, Murielle, Angéline Chatelan, Juan-Manuel Blanco, und Sigrid Maria Beer-Borst. 2017. „Anthropometric characteristics and indicators of eating and physical activity behaviors in the Swiss adult population: results from menuCH 2014-2015“."
  },
  {
    "objectID": "stat1-4/Statistik1_Uebung.html#aufgabe-1.1-assoziationstest",
    "href": "stat1-4/Statistik1_Uebung.html#aufgabe-1.1-assoziationstest",
    "title": "Stat1: Übung",
    "section": "Aufgabe 1.1: Assoziationstest",
    "text": "Aufgabe 1.1: Assoziationstest\nFührt einen Assoziationstest zweier kategorialer Variablen (mit je zwei Ausprägungen) mit Chi-Quadrat und Fishers exaktem Test durch. Dazu erhebt ihr selbst die Daten (wozu ihr euch auch in Teams zusammenschliessen könnt). Ihr könnt z.B. eine Datenerhebung unter Mitstudierenden durchführen (etwa Nutzung Mac/Windows vs. männlich/weiblich). Bitte formuliert vor der Datenerhebung eine Hypothese, d.h. eine Erwartungshaltung, ob und welche Assoziation vorliegt und wenn ja warum. Beachtet, dass ihr für diese Form des Assoziationstests genau zwei binäre Variablen benötigt. Wenn ihr also kategoriale Variablen mit mehr als zwei Ausprägungen habt, so könnt ihr entweder Ausprägungen sinnvoll zusammenfassen oder seltene Ausprägungen im Test unberücksichtigt lassen."
  },
  {
    "objectID": "stat1-4/Statistik1_Uebung.html#aufgabe-1.2-t-test",
    "href": "stat1-4/Statistik1_Uebung.html#aufgabe-1.2-t-test",
    "title": "Stat1: Übung",
    "section": "Aufgabe 1.2: t-Test",
    "text": "Aufgabe 1.2: t-Test\nT-Test mit Datensatz_novanimal_Uebung_Statistik1.2.csv\nWerden in den Basis- und Interventionswochen unterschiedlich viele Gerichte verkauft?\n\nSchau die Daten an: Verstehen und ggf. plotten.\nDefiniere die Null- (\\(H_0\\)) und die Alternativhypothese (\\(H_1\\)).\nWelche Form von t-Test musst Du anwenden: einseitig/zweiseitig resp. gepaart/ungepaart?\nFühre einen t-Test durch.\nWie gut sind die Voraussetzungen für einen t-Test erfüllt (z.B. Normalverteilung der Residuen und Varianzhomogenität)?\nStelle deine Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle"
  },
  {
    "objectID": "stat1-4/Statistik1_Loesung.html#musterlösung-übung-1.1-assoziationstest",
    "href": "stat1-4/Statistik1_Loesung.html#musterlösung-übung-1.1-assoziationstest",
    "title": "Stat1: Lösung",
    "section": "Musterlösung Übung 1.1: Assoziationstest",
    "text": "Musterlösung Übung 1.1: Assoziationstest\nIn diesem Beispiel soll einem Klischee auf den Grund gegagen werden: Sind Aargauer überdurchschnittlich mit weissen Socken assoziiert? Die Datenerhebung basiert auf männlichen Studenten bei denen folgende beiden binären Variablen erhoben wurden:\n\nSockenfarbe: weiss, nicht-weiss\nSelbstdeklarierte Kantonsangehörigkeit: AG, nicht-AG\n\nDie Hypothese ist: Das Klischee trifft zu, weisse Socken sind überdurchschnittlich häufig mit Aargauer Studenten assoziiert.\nDie Datenerhebung unter 35 Studenten ergab folgende Datengrundlage:\n\nWeisssockige Aargauer: 4\nNicht-weisssockige Aargauer: 2\nWeissockige nicht-Aargauer: 7\nNicht-weisssockige nicht-Aargauer: 22\n\n\n# Matrix erstellen\nAargauer &lt;- c(4, 2)\nnames(Aargauer) &lt;- c(\"Weiss\", \"NotWeiss\")\nNotAargauer &lt;- c(7, 22)\nnames(NotAargauer) &lt;- c(\"Weiss\", \"NotWeiss\")\nAGsocks &lt;- data.frame(Aargauer, NotAargauer)\nAGsocks &lt;- as.matrix(AGsocks)\nAGsocks\n##          Aargauer NotAargauer\n## Weiss           4           7\n## NotWeiss        2          22\n\n# Daten anschauen mit erstem Google-Ergebnis für 'Assoziation Plot r'\nassocplot(AGsocks)  # Interpretation des Plots mit dem Befehl ?assocplot\n\n\n\n\nDer Assoziationsplot zeigt, dass in den Daten weisse Socken bei den Aargauern überverterten und bei den Nicht-Aargauern untervertreten sind.\nFür kleine Erwartungswerte in den Zellen (&lt; 5) ist der Chi-Quadrat-Test nicht zuverlässig (siehe “Warning message”). Darum wird mit Fishers exaktem Test gearbeitet.\n\n# Tests durchführen\nchisq.test(AGsocks)  # Chi-Quadrat-Test nur zum anschauen.\n## \n##  Pearson's Chi-squared test with Yates' continuity correction\n## \n## data:  AGsocks\n## X-squared = 2.4323, df = 1, p-value = 0.1189\nfisher.test(AGsocks)  # 'Fisher's Exact Test for Count Data'\n## \n##  Fisher's Exact Test for Count Data\n## \n## data:  AGsocks\n## p-value = 0.06323\n## alternative hypothesis: true odds ratio is not equal to 1\n## 95 percent confidence interval:\n##   0.6811227 78.4336189\n## sample estimates:\n## odds ratio \n##   5.897263\n\n\nErgebnisse\nIn den erhobenen Daten konnte keine signifikante Assoziation zwischen Kantonangehörigkeit (AG, nicht-AG) und Sockenfarbe (weiss, nicht-weiss) festgestellt werden. Der p-Wert von Fishers exaktem Test war nur marginal signifikant (p = 0.063). Das nicht-signfikante Resultat überrascht auf den ersten Blick, denn der “odds ratio” im Datensatz ist mit 5.9 relativ hoch und 67 % der Aargauer trugen weisse Socken während nur 24 % der Nicht-Aargauer weisse Socken trugen. Doch war der Anteil von nur 6 Aargauer in der nur 35 Männer umfassenden Stichprobe relativ klein, um ein verlässliches Bild der Sockenpräferenzen der Aargauer zu machen. Insofern leuchtet es ein, das bei dieser kleinen und unausgewogenen Stichprobe die “Power” des satistischen Tests (um die Nullhypothese zu verwerfen) relativ klein ist."
  },
  {
    "objectID": "stat1-4/Statistik1_Loesung.html#musterlösung-übung-1.2-t-test",
    "href": "stat1-4/Statistik1_Loesung.html#musterlösung-übung-1.2-t-test",
    "title": "Stat1: Lösung",
    "section": "Musterlösung Übung 1.2: t-Test",
    "text": "Musterlösung Übung 1.2: t-Test\n\nLeseempfehlung Kapitel 2 von Manny Gimond\n\n\nNull- und Alternativhypothese\n\\(H_0\\): Es gibt keine Unterschiede in den Verkaufszahlen zwischen Basis- und Interventionswochen.\n\\(H_1\\): Es gibt Unterschiede in den Verkaufszahlen zwischen Basis- und Interventionswochen.\n\n#lade Daten\ndf &lt;- readr::read_csv2(\"datasets/statistik/Datensatz_novanimal_Uebung_Statistik1.2.csv\")\n\n# überprüft die Voraussetzungen für einen t-Test\nggplot(df, aes(x = condit, y= tot_sold)) + # achtung 0 Punkt fehlt\n    geom_boxplot(fill = \"white\", color = \"black\", size = 1) + \n    labs(x=\"\\nBedingungen\", y=\"Durchschnittlich verkaufte Gerichte pro Woche\\n\") + \n    mytheme\n\n\n\n\n# Auf den ersten Blick scheint es keine starken Abweichungen zu einer \n#Normalverteilung zu geben resp. es sind keine extremen schiefen Verteilungen\n# ersichtlich (vgl. Skript Statistik 2)\n\n\n# führt einen t-Tests durch; es wird angenommen, dass die Verkaufszahlen\n# zwischen den Bedingungen unabhängig sind\n\nt_test &lt;- t.test(tot_sold ~ condit, data = df, var.equl = T)\nt_test\n## \n##  Welch Two Sample t-test\n## \n## data:  tot_sold by condit\n## t = 0.27168, df = 9.9707, p-value = 0.7914\n## alternative hypothesis: true difference in means between group Basis and group Intervention is not equal to 0\n## 95 percent confidence interval:\n##  -115.2743  147.2743\n## sample estimates:\n##        mean in group Basis mean in group Intervention \n##                       2203                       2187\n\n# alternative Formulierung\nt.test(df[df$condit == \"Basis\", ]$tot_sold, df[df$condit == \"Intervention\", ]$tot_sold)\n## \n##  Welch Two Sample t-test\n## \n## data:  df[df$condit == \"Basis\", ]$tot_sold and df[df$condit == \"Intervention\", ]$tot_sold\n## t = 0.27168, df = 9.9707, p-value = 0.7914\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -115.2743  147.2743\n## sample estimates:\n## mean of x mean of y \n##      2203      2187\n\n\n\nMethoden\nZiel war es die aggregierten Verkaufszahlen zwischen den Interventions- und Basiswochen zu vergleichen. Die Annahme ist, dass die wöchentlichen Verkaufszahlen unabhängig sind. Daher können die Unterschiede zwischen den Verkaufszahlen pro Woche zwischen den beiden Bedingungen mittels t-Test geprüft werden. Obwohl die visuelle Inspektion keine schwerwiegenden Verletzungen der Modelvoraussetzung zeigte (mit Ausnahme eines Ausreissers), wurde einen Welch t-Test gerechnet. Zudem muss gesagt werden, dass die Gruppengrösse hier jeweils mit n = 6 (Anzahl Wochen) eher klein ist. T-test liefern dennoch relativ reliable Resultate. Für mehr Infos dazu hier eine Studie.\n\n\nErgebnisse\nIn den Basiswochen werden mehr Gerichte pro Woche verkauft als in den Interventionswochen (siehe Abbildung 1). Die wöchentlichen Verkaufszahlen zwischen den Bedingungen (Basis oder Intervention) unterscheiden sich gemäss Welch t-Test jedoch nicht signifikant (t(10) = 0.272 , p = 0.791). Die Ergebnisse könnten mit einem \\(\\chi^2\\)-Test nochmals validiert werden, da die Gruppengrösse mit n = 6 doch eher klein ist.\n\n\n\n\n\nAbbildung 18.1: Die wöchentlichen Verkaufszahlen für die Interventions- und Basiswochen unterscheiden sich nicht signifikant."
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#t-test-als-anova",
    "href": "stat1-4/Statistik2_Demo.html#t-test-als-anova",
    "title": "Stat2: Demo",
    "section": "t-test als ANOVA",
    "text": "t-test als ANOVA\n\n# Daten generieren als 'long table' (2 Kategorien)\na &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14)\nb &lt;- c(12, 15, 16, 7, 8, 10, 12, 11, 13, 10)\nblume &lt;- data.frame(cultivar = c(rep(\"a\", 10), rep(\"b\", 10)), size = c(a, b))\n\n# Daten anschauen\npar(mfrow = c(1, 1))\nboxplot(size ~ cultivar, xlab = \"Sorte\", ylab = \"Bluetengroesse [cm]\", data = blume)\n\n\n\n\n# Klassischer t-Test ausführen\nt.test(size ~ cultivar, blume, var.equal = TRUE)\n## \n##  Two Sample t-test\n## \n## data:  size by cultivar\n## t = 2.0797, df = 18, p-value = 0.05212\n## alternative hypothesis: true difference in means between group a and group b is not equal to 0\n## 95 percent confidence interval:\n##  -0.03981237  7.83981237\n## sample estimates:\n## mean in group a mean in group b \n##            15.3            11.4\n\n# ANOVA ausführen\naov(size ~ cultivar, data = blume)\n## Call:\n##    aov(formula = size ~ cultivar, data = blume)\n## \n## Terms:\n##                 cultivar Residuals\n## Sum of Squares     76.05    316.50\n## Deg. of Freedom        1        18\n## \n## Residual standard error: 4.193249\n## Estimated effects may be unbalanced\nsummary(aov(size ~ cultivar, data = blume))\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## cultivar     1   76.0   76.05   4.325 0.0521 .\n## Residuals   18  316.5   17.58                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary.lm(aov(size ~ cultivar, data = blume))\n## \n## Call:\n## aov(formula = size ~ cultivar, data = blume)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -7.300 -2.575 -0.350  2.925  9.700 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   15.300      1.326   11.54 9.47e-10 ***\n## cultivarb     -3.900      1.875   -2.08   0.0521 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.193 on 18 degrees of freedom\n## Multiple R-squared:  0.1937, Adjusted R-squared:  0.1489 \n## F-statistic: 4.325 on 1 and 18 DF,  p-value: 0.05212"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#echte-anova",
    "href": "stat1-4/Statistik2_Demo.html#echte-anova",
    "title": "Stat2: Demo",
    "section": "Echte ANOVA",
    "text": "Echte ANOVA\n\n# Daten generieren mit 3 statt nur 2 Kategorien\nc &lt;- c(30, 19, 31, 23, 18, 25, 26, 24, 17, 20)\nblume2 &lt;- data.frame(cultivar = c(rep(\"a\", 10), rep(\"b\", 10), rep(\"c\", 10)), size = c(a,\n    b, c))\nblume2$cultivar &lt;- as.factor(blume2$cultivar)\n\nsummary(blume2)\n##  cultivar      size      \n##  a:10     Min.   : 7.00  \n##  b:10     1st Qu.:11.25  \n##  c:10     Median :15.50  \n##           Mean   :16.67  \n##           3rd Qu.:20.00  \n##           Max.   :31.00\nhead(blume2)\n##   cultivar size\n## 1        a   20\n## 2        a   19\n## 3        a   25\n## 4        a   10\n## 5        a    8\n## 6        a   15\n\npar(mfrow = c(1, 1))\nboxplot(size ~ cultivar, xlab = \"Sorte\", ylab = \"Blütengrösse [cm]\", data = blume2)\n\n\n\n\naov(size ~ cultivar, data = blume2)\n## Call:\n##    aov(formula = size ~ cultivar, data = blume2)\n## \n## Terms:\n##                 cultivar Residuals\n## Sum of Squares  736.0667  528.6000\n## Deg. of Freedom        2        27\n## \n## Residual standard error: 4.424678\n## Estimated effects may be unbalanced\nsummary(aov(size ~ cultivar, data = blume2))\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cultivar     2  736.1   368.0    18.8 7.68e-06 ***\n## Residuals   27  528.6    19.6                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary.lm(aov(size ~ cultivar, data = blume2))\n## \n## Call:\n## aov(formula = size ~ cultivar, data = blume2)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -7.300 -3.375 -0.300  2.700  9.700 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   15.300      1.399  10.935 2.02e-11 ***\n## cultivarb     -3.900      1.979  -1.971 0.059065 .  \n## cultivarc      8.000      1.979   4.043 0.000395 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.425 on 27 degrees of freedom\n## Multiple R-squared:  0.582,  Adjusted R-squared:  0.5511 \n## F-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06\n\naov.1 &lt;- aov(size ~ cultivar, data = blume2)\nsummary(aov.1)\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cultivar     2  736.1   368.0    18.8 7.68e-06 ***\n## Residuals   27  528.6    19.6                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary.lm(aov.1)\n## \n## Call:\n## aov(formula = size ~ cultivar, data = blume2)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -7.300 -3.375 -0.300  2.700  9.700 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   15.300      1.399  10.935 2.02e-11 ***\n## cultivarb     -3.900      1.979  -1.971 0.059065 .  \n## cultivarc      8.000      1.979   4.043 0.000395 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.425 on 27 degrees of freedom\n## Multiple R-squared:  0.582,  Adjusted R-squared:  0.5511 \n## F-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06\n\n# Berechnung Mittelwerte usw. zur Charakterisierung der Gruppen\naggregate(size ~ cultivar, blume2, function(x) c(Mean = mean(x), SD = sd(x), Min = min(x),\n    Max = max(x)))\n##   cultivar size.Mean   size.SD  size.Min  size.Max\n## 1        a 15.300000  5.207900  8.000000 25.000000\n## 2        b 11.400000  2.836273  7.000000 16.000000\n## 3        c 23.300000  4.854551 17.000000 31.000000\n\nlm.1 &lt;- lm(size ~ cultivar, data = blume2)\nsummary(lm.1)\n## \n## Call:\n## lm(formula = size ~ cultivar, data = blume2)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -7.300 -3.375 -0.300  2.700  9.700 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   15.300      1.399  10.935 2.02e-11 ***\n## cultivarb     -3.900      1.979  -1.971 0.059065 .  \n## cultivarc      8.000      1.979   4.043 0.000395 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.425 on 27 degrees of freedom\n## Multiple R-squared:  0.582,  Adjusted R-squared:  0.5511 \n## F-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#tukeys-posthoc-test",
    "href": "stat1-4/Statistik2_Demo.html#tukeys-posthoc-test",
    "title": "Stat2: Demo",
    "section": "Tukeys Posthoc-Test",
    "text": "Tukeys Posthoc-Test\n\n# Load library\nif (!require(agricolae)) {\n    install.packages(\"agricolae\")\n}\nlibrary(agricolae)\n\n# Posthoc-Test\nHSD.test(aov.1, \"cultivar\", group = FALSE, console = TRUE)\n## \n## Study: aov.1 ~ \"cultivar\"\n## \n## HSD Test for size \n## \n## Mean Square Error:  19.57778 \n## \n## cultivar,  means\n## \n##   size      std  r Min Max\n## a 15.3 5.207900 10   8  25\n## b 11.4 2.836273 10   7  16\n## c 23.3 4.854551 10  17  31\n## \n## Alpha: 0.05 ; DF Error: 27 \n## Critical Value of Studentized Range: 3.506426 \n## \n## Comparison between treatments means\n## \n##       difference pvalue signif.        LCL       UCL\n## a - b        3.9 0.1388          -1.006213  8.806213\n## a - c       -8.0 0.0011      ** -12.906213 -3.093787\n## b - c      -11.9 0.0000     *** -16.806213 -6.993787"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#beispiel-posthoc-labels-in-plot",
    "href": "stat1-4/Statistik2_Demo.html#beispiel-posthoc-labels-in-plot",
    "title": "Stat2: Demo",
    "section": "Beispiel Posthoc-Labels in Plot",
    "text": "Beispiel Posthoc-Labels in Plot\n\n# ANOVA Mit Iris-Datenset, das in R integriert ist\naov.2 &lt;- aov(Sepal.Width ~ Species, data = iris)\n\n# Posthoc-Test\nHSD.test(aov.2, \"Species\", console = TRUE)\n## \n## Study: aov.2 ~ \"Species\"\n## \n## HSD Test for Sepal.Width \n## \n## Mean Square Error:  0.1153878 \n## \n## Species,  means\n## \n##            Sepal.Width       std  r Min Max\n## setosa           3.428 0.3790644 50 2.3 4.4\n## versicolor       2.770 0.3137983 50 2.0 3.4\n## virginica        2.974 0.3224966 50 2.2 3.8\n## \n## Alpha: 0.05 ; DF Error: 147 \n## Critical Value of Studentized Range: 3.348424 \n## \n## Minimun Significant Difference: 0.1608553 \n## \n## Treatments with the same letter are not significantly different.\n## \n##            Sepal.Width groups\n## setosa           3.428      a\n## virginica        2.974      b\n## versicolor       2.770      c\n\n# Plot mit labels\nboxplot(Sepal.Width ~ Species, data = iris)\n\n\n\nboxplot(Sepal.Width ~ Species, ylim = c(2, 5), data = iris)\ntext(1, 4.8, \"a\")\ntext(2, 4.8, \"c\")\ntext(3, 4.8, \"b\")\n\n\n\n\nDerselbe Plot mit ggplot\n\n# Load library\nlibrary(ggplot2)\n\nggplot(iris, aes(Species, Sepal.Width)) + geom_boxplot(size = 1) + annotate(\"text\",\n    y = 5, x = 1:3, label = c(\"a\", \"c\", \"b\"))"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#klassische-tests-der-modellannahmen-nicht-empfohlen",
    "href": "stat1-4/Statistik2_Demo.html#klassische-tests-der-modellannahmen-nicht-empfohlen",
    "title": "Stat2: Demo",
    "section": "Klassische Tests der Modellannahmen (NICHT EMPFOHLEN!!!)",
    "text": "Klassische Tests der Modellannahmen (NICHT EMPFOHLEN!!!)\n\n# Shapiro-Wilk Test auf Normalverteilung Pro Kategorie!  (H0 =\n# Notmalverteilung)\nshapiro.test(blume2$size[blume2$cultivar == \"a\"])\n## \n##  Shapiro-Wilk normality test\n## \n## data:  blume2$size[blume2$cultivar == \"a\"]\n## W = 0.97304, p-value = 0.9175\nshapiro.test(blume2$size[blume2$cultivar == \"b\"])\n## \n##  Shapiro-Wilk normality test\n## \n## data:  blume2$size[blume2$cultivar == \"b\"]\n## W = 0.97341, p-value = 0.9206\nshapiro.test(blume2$size[blume2$cultivar == \"c\"])\n## \n##  Shapiro-Wilk normality test\n## \n## data:  blume2$size[blume2$cultivar == \"c\"]\n## W = 0.94188, p-value = 0.5742\n`?`(var.test)\n\n# F-Test zum Vergleich zweier Varianzen (H0= Gleiche Varianzen)\nvar.test(blume2$size[blume2$cultivar == \"a\"], blume2$size[blume2$cultivar == \"b\"])\n## \n##  F test to compare two variances\n## \n## data:  blume2$size[blume2$cultivar == \"a\"] and blume2$size[blume2$cultivar == \"b\"]\n## F = 3.3715, num df = 9, denom df = 9, p-value = 0.08467\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 95 percent confidence interval:\n##   0.8374446 13.5738284\n## sample estimates:\n## ratio of variances \n##           3.371547\n\n\n# Load library\nif (!require(car)) {\n    install.packages(\"car\")\n}\nlibrary(car)\n\n# Test auf Homogenität der Varianzen\nleveneTest(blume2$size[blume2$cultivar == \"a\"], blume2$size[blume2$cultivar == \"b\"],\n    center = mean)\n## Levene's Test for Homogeneity of Variance (center = mean)\n##       Df    F value    Pr(&gt;F)    \n## group  7 3.0148e+30 &lt; 2.2e-16 ***\n##        2                         \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#nicht-parametrische-alternativen-wenn-modellannahmen-der-anvoa-massiv-verletzt-sind",
    "href": "stat1-4/Statistik2_Demo.html#nicht-parametrische-alternativen-wenn-modellannahmen-der-anvoa-massiv-verletzt-sind",
    "title": "Stat2: Demo",
    "section": "Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind",
    "text": "Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind\n\n# Nicht-parametrische Alternative zu t-Test\nwilcox.test(blume2$size[blume2$cultivar == \"a\"], blume2$size[blume2$cultivar == \"b\"])\n## \n##  Wilcoxon rank sum test with continuity correction\n## \n## data:  blume2$size[blume2$cultivar == \"a\"] and blume2$size[blume2$cultivar == \"b\"]\n## W = 73, p-value = 0.08789\n## alternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#zum-vergleich-normale-anova-noch-mal",
    "href": "stat1-4/Statistik2_Demo.html#zum-vergleich-normale-anova-noch-mal",
    "title": "Stat2: Demo",
    "section": "Zum Vergleich normale ANOVA noch mal",
    "text": "Zum Vergleich normale ANOVA noch mal\n\nsummary(aov(size ~ cultivar, data = blume2))\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cultivar     2  736.1   368.0    18.8 7.68e-06 ***\n## Residuals   27  528.6    19.6                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#bei-starken-abweichungen-von-der-normalverteilung-aber-ähnlichen-varianzen",
    "href": "stat1-4/Statistik2_Demo.html#bei-starken-abweichungen-von-der-normalverteilung-aber-ähnlichen-varianzen",
    "title": "Stat2: Demo",
    "section": "Bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen",
    "text": "Bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#kruskal-wallis-test",
    "href": "stat1-4/Statistik2_Demo.html#kruskal-wallis-test",
    "title": "Stat2: Demo",
    "section": "Kruskal-Wallis-Test",
    "text": "Kruskal-Wallis-Test\n\nkruskal.test(size ~ cultivar, data = blume2)\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  size by cultivar\n## Kruskal-Wallis chi-squared = 16.686, df = 2, p-value = 0.0002381\n\n\n# Load library\nif (!require(FSA)) {\n    install.packages(\"FSA\")\n}\nlibrary(FSA)\n\n# korrigierte p-Werte nach Bejamini-Hochberg\ndunnTest(size ~ cultivar, method = \"bh\", data = blume2)\n##   Comparison         Z      P.unadj        P.adj\n## 1      a - b  1.526210 1.269575e-01 0.1269575490\n## 2      a - c -2.518247 1.179407e-02 0.0176911039\n## 3      b - c -4.044457 5.244459e-05 0.0001573338"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#bei-erheblicher-heteroskedastizität-aber-relative-normalsymmetrisch-verteilten-residuen",
    "href": "stat1-4/Statistik2_Demo.html#bei-erheblicher-heteroskedastizität-aber-relative-normalsymmetrisch-verteilten-residuen",
    "title": "Stat2: Demo",
    "section": "Bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen",
    "text": "Bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#welch-test",
    "href": "stat1-4/Statistik2_Demo.html#welch-test",
    "title": "Stat2: Demo",
    "section": "Welch-Test",
    "text": "Welch-Test\n\noneway.test(size ~ cultivar, var.equal = F, data = blume2)\n## \n##  One-way analysis of means (not assuming equal variances)\n## \n## data:  size and cultivar\n## F = 21.642, num df = 2.000, denom df = 16.564, p-value = 2.397e-05"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#faktorielle-anova",
    "href": "stat1-4/Statistik2_Demo.html#faktorielle-anova",
    "title": "Stat2: Demo",
    "section": "2-faktorielle ANOVA",
    "text": "2-faktorielle ANOVA\n\n# Daten generieren\nd &lt;- c(10, 12, 11, 13, 10, 25, 12, 30, 26, 13)\ne &lt;- c(15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nf &lt;- c(10, 12, 11, 13, 10, 9, 2, 4, 7, 13)\n\nblume3 &lt;- data.frame(cultivar = c(rep(\"a\", 20), rep(\"b\", 20), rep(\"c\", 20)), house = c(rep(c(rep(\"yes\",\n    10), rep(\"no\", 10)), 3)), size = c(a, b, c, d, e, f))\nblume3\n##    cultivar house size\n## 1         a   yes   20\n## 2         a   yes   19\n## 3         a   yes   25\n## 4         a   yes   10\n## 5         a   yes    8\n## 6         a   yes   15\n## 7         a   yes   13\n## 8         a   yes   18\n## 9         a   yes   11\n## 10        a   yes   14\n## 11        a    no   12\n## 12        a    no   15\n## 13        a    no   16\n## 14        a    no    7\n## 15        a    no    8\n## 16        a    no   10\n## 17        a    no   12\n## 18        a    no   11\n## 19        a    no   13\n## 20        a    no   10\n## 21        b   yes   30\n## 22        b   yes   19\n## 23        b   yes   31\n## 24        b   yes   23\n## 25        b   yes   18\n## 26        b   yes   25\n## 27        b   yes   26\n## 28        b   yes   24\n## 29        b   yes   17\n## 30        b   yes   20\n## 31        b    no   10\n## 32        b    no   12\n## 33        b    no   11\n## 34        b    no   13\n## 35        b    no   10\n## 36        b    no   25\n## 37        b    no   12\n## 38        b    no   30\n## 39        b    no   26\n## 40        b    no   13\n## 41        c   yes   15\n## 42        c   yes   13\n## 43        c   yes   18\n## 44        c   yes   11\n## 45        c   yes   14\n## 46        c   yes   25\n## 47        c   yes   39\n## 48        c   yes   38\n## 49        c   yes   28\n## 50        c   yes   24\n## 51        c    no   10\n## 52        c    no   12\n## 53        c    no   11\n## 54        c    no   13\n## 55        c    no   10\n## 56        c    no    9\n## 57        c    no    2\n## 58        c    no    4\n## 59        c    no    7\n## 60        c    no   13\n\n\nboxplot(size ~ cultivar + house, data = blume3)\n\n\n\n\nsummary(aov(size ~ cultivar + house, data = blume3))\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cultivar     2  417.1   208.6   5.005     0.01 *  \n## house        1  992.3   992.3  23.815 9.19e-06 ***\n## Residuals   56 2333.2    41.7                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(aov(size ~ cultivar + house + cultivar:house, data = blume3))\n##                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cultivar        2  417.1   208.6   5.364   0.0075 ** \n## house           1  992.3   992.3  25.520 5.33e-06 ***\n## cultivar:house  2  233.6   116.8   3.004   0.0579 .  \n## Residuals      54 2099.6    38.9                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Kurzschreibweise: '*' bedeutet, dass Interaktion zwischen cultivar und house\n# eingeschlossen wird\nsummary(aov(size ~ cultivar * house, data = blume3))\n##                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cultivar        2  417.1   208.6   5.364   0.0075 ** \n## house           1  992.3   992.3  25.520 5.33e-06 ***\n## cultivar:house  2  233.6   116.8   3.004   0.0579 .  \n## Residuals      54 2099.6    38.9                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov(size ~ cultivar + house, data = blume3))\n## \n## Call:\n## aov(formula = size ~ cultivar + house, data = blume3)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -9.733 -4.696 -1.050  2.717 19.133 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    9.283      1.667   5.570 7.52e-07 ***\n## cultivarb      6.400      2.041   3.135  0.00273 ** \n## cultivarc      2.450      2.041   1.200  0.23509    \n## houseyes       8.133      1.667   4.880 9.19e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.455 on 56 degrees of freedom\n## Multiple R-squared:  0.3766, Adjusted R-squared:  0.3432 \n## F-statistic: 11.28 on 3 and 56 DF,  p-value: 6.848e-06\n\ninteraction.plot(blume3$cultivar, blume3$house, blume3$size)\n\n\n\ninteraction.plot(blume3$house, blume3$cultivar, blume3$size)\n\n\n\n\n\n\nanova(lm(blume3$size ~ blume3$cultivar * blume3$house), lm(blume3$size ~ blume3$cultivar +\n    blume3$house))\n## Analysis of Variance Table\n## \n## Model 1: blume3$size ~ blume3$cultivar * blume3$house\n## Model 2: blume3$size ~ blume3$cultivar + blume3$house\n##   Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n## 1     54 2099.6                              \n## 2     56 2333.2 -2   -233.63 3.0044 0.05792 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(lm(blume3$size ~ blume3$house), lm(blume3$size ~ blume3$cultivar * blume3$house))\n## Analysis of Variance Table\n## \n## Model 1: blume3$size ~ blume3$house\n## Model 2: blume3$size ~ blume3$cultivar * blume3$house\n##   Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n## 1     58 2750.3                                \n## 2     54 2099.6  4    650.73 4.1841 0.005045 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Visualisierung 2-fach-Interaktion etwas elaborierter mit ggplot\n\nif (!require(sjPlot)) {\n    install.packages(\"sjPlot\")\n}\nlibrary(sjPlot)\nif (!require(ggplot2)) {\n    install.packages(\"ggplot2\")\n}\nlibrary(ggplot2)\ntheme_set(theme_classic())\n\naov &lt;- aov(size ~ cultivar * house, data = blume3)\nplot_model(aov, type = \"pred\", terms = c(\"cultivar\", \"house\"))\n\n\n\n\n\n# Geht auch für 3-fach-Interaktionen\n\n# Datensatz zum Einfluss von Management und Hirschbeweidung auf den\n# Pflanzenartenreichtum\n\nRiesch &lt;- read.delim(\"datasets/statistik/Riesch_et_al_ReMe_Extract.csv\", sep = \";\",\n    stringsAsFactors = TRUE)\nstr(Riesch)\n## 'data.frame':    60 obs. of  5 variables:\n##  $ Plot.ID         : Factor w/ 60 levels \"Eul_A1_MP_14\",..: 9 17 33 37 49 10 18 34 38 50 ...\n##  $ Year            : Factor w/ 2 levels \"Year 1\",\"Year 4\": 1 1 1 1 1 2 2 2 2 2 ...\n##  $ Treatment       : Factor w/ 3 levels \"burnt\",\"mown\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Plot.type       : Factor w/ 2 levels \"fenced\",\"open\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Species.richness: int  45 49 44 52 43 37 42 36 46 38 ...\n\naov.deer &lt;- aov(Species.richness ~ Year * Treatment * Plot.type, data = Riesch)\nplot_model(aov.deer, type = \"pred\", terms = c(\"Year\", \"Treatment\", \"Plot.type\"))"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#korrelationen",
    "href": "stat1-4/Statistik2_Demo.html#korrelationen",
    "title": "Stat2: Demo",
    "section": "Korrelationen",
    "text": "Korrelationen\n\n## Korrelationen und Regressionen\n\n# Datensatz zum Einfluss von Stickstoffdepositionen auf den\n# Pflanzenartenreichtum\ndf &lt;- read.delim(\"datasets/statistik/Nitrogen.csv\", sep = \";\")\nsummary(df)\n##   N.deposition   Species.richness\n##  Min.   : 2.00   Min.   :12.0    \n##  1st Qu.: 9.00   1st Qu.:17.5    \n##  Median :20.00   Median :21.0    \n##  Mean   :20.53   Mean   :20.2    \n##  3rd Qu.:30.50   3rd Qu.:23.0    \n##  Max.   :55.00   Max.   :28.0\n\n# Plotten der Beziehung\nplot(Species.richness ~ N.deposition, data = df)\n\n\n\n\n# Load library\nif (!require(car)) {\n    install.packages(\"car\")\n}\nlibrary(car)\n\n# Daten anschauen\nscatterplot(Species.richness ~ N.deposition, data = df)\n\n\n\n\n\n# Korrelationen\ncor.test(df$Species.richness, df$N.deposition, method = \"pearson\")\n## \n##  Pearson's product-moment correlation\n## \n## data:  df$Species.richness and df$N.deposition\n## t = -5.2941, df = 13, p-value = 0.0001453\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.9405572 -0.5450218\n## sample estimates:\n##        cor \n## -0.8265238\ncor.test(df$N.deposition, df$Species.richness, method = \"pearson\")\n## \n##  Pearson's product-moment correlation\n## \n## data:  df$N.deposition and df$Species.richness\n## t = -5.2941, df = 13, p-value = 0.0001453\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.9405572 -0.5450218\n## sample estimates:\n##        cor \n## -0.8265238\ncor.test(df$Species.richness, df$N.deposition, method = \"spearman\")\n## \n##  Spearman's rank correlation rho\n## \n## data:  df$Species.richness and df$N.deposition\n## S = 1015.5, p-value = 0.0002259\n## alternative hypothesis: true rho is not equal to 0\n## sample estimates:\n##        rho \n## -0.8133721\ncor.test(df$Species.richness, df$N.deposition, method = \"kendall\")\n## \n##  Kendall's rank correlation tau\n## \n## data:  df$Species.richness and df$N.deposition\n## z = -3.308, p-value = 0.0009398\n## alternative hypothesis: true tau is not equal to 0\n## sample estimates:\n##       tau \n## -0.657115\n\n# Jetzt als Regression\nlm &lt;- lm(Species.richness ~ N.deposition, data = df)\nanova(lm)  #ANOVA-Tabelle, 1. Möglichkeit\n## Analysis of Variance Table\n## \n## Response: Species.richness\n##              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \n## N.deposition  1 233.91 233.908  28.028 0.0001453 ***\n## Residuals    13 108.49   8.346                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary.aov(lm)  #ANOVA-Tabelle, 2. Möglichkeit\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## N.deposition  1  233.9  233.91   28.03 0.000145 ***\n## Residuals    13  108.5    8.35                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(lm)  #Regressionskoeffizienten\n## \n## Call:\n## lm(formula = Species.richness ~ N.deposition, data = df)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4.9184 -1.9992  0.4493  2.0015  4.6081 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  25.60502    1.26440  20.251 3.25e-11 ***\n## N.deposition -0.26323    0.04972  -5.294 0.000145 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.889 on 13 degrees of freedom\n## Multiple R-squared:  0.6831, Adjusted R-squared:  0.6588 \n## F-statistic: 28.03 on 1 and 13 DF,  p-value: 0.0001453\n\n# Signifikantes Ergebnis visualisieren\nplot(Species.richness ~ N.deposition, data = df)\nabline(lm)"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#beispiele-modelldiagnostik",
    "href": "stat1-4/Statistik2_Demo.html#beispiele-modelldiagnostik",
    "title": "Stat2: Demo",
    "section": "Beispiele Modelldiagnostik",
    "text": "Beispiele Modelldiagnostik\n\npar(mfrow = c(2, 2))  #4 Plots in einem Fenster\nplot(lm(b ~ a))\n\n\n\n\n\n# Load library\nif (!require(ggfortify)) {\n    install.packages(\"ggfortify\")\n}\nlibrary(ggfortify)\n\nautoplot(lm(b ~ a))\n\n\n\n\n# Modellstatistik nicht OK\ng &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nh &lt;- c(12, 15, 10, 7, 8, 10, 12, 11, 13, 10, 25, 12, 30, 26, 13)\npar(mfrow = c(1, 1))\n\nplot(h ~ g, xlim = c(0, 40), ylim = c(0, 30))\nabline(lm(h ~ g))\n\n\n\n\npar(mfrow = c(2, 2))\nplot(lm(h ~ g))\n\n\n\n\n\n# Modelldiagnostik mit ggplot\ndf &lt;- data.frame(g, h)\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + ggplot(df,\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + aes(x\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + =\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + g,\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + y\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + =\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + h))\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + +\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + #\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + scale_x_continuous(limits\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + =\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + c(0,25))\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + +\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + scale_y_continuous(limits\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + =\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + c(0,25))\nggplot(df, aes(x = g, y = h)) + # scale_x_continuous(limits = c(0,25)) +  scale_y_continuous(limits = c(0,25)) + +\ngeom_point() + geom_smooth(method = \"lm\", color = \"black\", size = 0.5, se = F) +\n    theme_classic()\n## Error in `ggplot_add()`:\n## ! Can't add `ggplot(df, aes(x = g, y = h))` to a &lt;ggplot&gt; object.\n\npar(mfrow = c(2, 2))\nautoplot(lm(h ~ g))"
  },
  {
    "objectID": "stat1-4/Statistik2_Uebung.html",
    "href": "stat1-4/Statistik2_Uebung.html",
    "title": "Stat2: Übung",
    "section": "",
    "text": "Abzugeben sind am Ende\n\nlauffähiges R-Skript\nbegründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation)\nausformulierter Methoden- und Ergebnisteil (für eine wiss.Arbeit).\n\n\nBitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu eurem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument (oder ein z.B. mit Quarto generiertes pdf- oder html-Dokument), in dem ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentiert.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen etc.\nExplorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten\nAuswahl und Begründung eines statistischen Verfahrens\nBestimmung des vollständigen/maximalen Models\nSelektion des/der besten Models/Modelle\nDurchführen der Modelldiagnostik für dieses\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\n\nFormuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen/Tabellen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (je einen ausformulierten Absatz von ca. 60-100 Worten bzw. 3-8 Sätzen). Alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\n\n\nÜbung 2.1: Regression\nRegressionsanalyse mit SAR.csv\nDer Datensatz beschreibt die Zunahme der Artenzahlen (richness) von Pflanzen in Trockenrasen der Schweiz in Abhängigkeit von der Probeflächengrösse (area, hier in m²). Diese Beziehung bezeichnet man als Artenzahl-Areal-Kurve (Species-area relationship = SAR).\n\nLadet den Datensatz in R und macht eine explorative Datenanalyse.\nWählt unter den schon gelernten Methoden der Regressionsanalyse ein adäquates Vorgehen zur Analyse dieser Daten und führt diese dann durch.\nPrüft anhand der Residuen, ob die Modellvoraussetzungen erfüllt waren\nFalls die Modelldiagnostik negativ ausfällt, überlegt, welche Datentransformation helfen könnte, und rechnet neue Modelle mit einer oder ggf. mehreren Datentransformationen, bis ihr eine statistisch zufriedenstellende Lösung gefunden habt.\nStellt die erhaltenen Ergebnisse angemessen dar (Text, Abbildung und/oder Tabelle).\nKennt ihr ggf. noch eine andere geeignete Herangehensweise?\n\n\n\nÜbung 2.2: Einfaktorielle ANOVA\nANOVA mit Datensatz_novanimal_Uebung_Statistik2.2.csv\nFührt mit dem Datensatz novanimal.csv eine einfaktorielle ANOVA durch. Gibt es Unterschiede zwischen der Anzahl verkaufter Gerichte “tot_sold” (Buffet, Fleisch oder Vegetarisch) pro Woche?\n\n\nÜbung 2.3N: Mehrfaktorielle ANOVA (NatWis)\nANOVA mit kormoran.csv\nDer Datensatz enthält 40 Beobachtungen zu Tauchzeiten zweier Kormoranunterarten (C = Phalocrocorax carbo carbo und S = Phalacrocorax carbo sinensis) aus vier Jahreszeiten (F = Frühling, S = Sommer, H = Herbst, W = Winter).\n\nLest den Datensatz nach R ein und führt eine adäquate Analyse durch, um beantworten zu können, wie Unterart und Jahreszeit die Tauchzeit beeinflussen.\nStellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle).\nGibt es eine Interaktion?\n\n\n\nÜbung 2.3S: Mehrfaktorielle ANOVA mit Interaktion (SozWis)\nMANOVA mit Datensatz_novanimal_Uebung_Statistik2.3.csv\nIn der Mensa gibt es zwei unterschiedliche Preisniveaus bzgl. den Gerichten: eine preisgünstigere Menülinie (“World” & “Favorite”) und eine teuere Menülinie (“Kitchen”). Gibt es Unterschiede zwischen dem Kauf von preisgünstigeren resp. teureren Menülinien betreffend Menüinhalt & Hochschulzugehörigkeit?"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#musterlösung-beispiel",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#musterlösung-beispiel",
    "title": "Stat2: Lösung Beispiel",
    "section": "Musterlösung Beispiel",
    "text": "Musterlösung Beispiel\n\nDownload dieses Lösungsscript via “&lt;/&gt;Code” (oben rechts)\nLösungstext als Download\n\n\nÜbungsaufgabe\n(hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\n\nLaden Sie den Datensatz decay.csv. Dieser enthält die Zahl radioaktiver Zerfälle pro Zeiteinheit (amount) für Zeitpunkte (time) nach dem Start des Experimentes.\nErmitteln Sie ein statistisches Modell, dass die Zerfallshäufigkeit in Abhängigkeit von der Zeit beschreibt.\nBitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument (oder ein z.B. mit Quarto generiertes pdf- oder html-Dokument), in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabhängige(n) Variablen\nExplorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten\nAuswahl und Begründung eines statistischen Verfahrens (es gibt hier mehrere statistisch korrekte Möglichkeiten!)\nErmittlung eines Modells\nDurchführen der Modelldiagnostik für das gewählte Modell\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\nFormulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit)."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#kommentierter-lösungsweg",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#kommentierter-lösungsweg",
    "title": "Stat2: Lösung Beispiel",
    "section": "Kommentierter Lösungsweg",
    "text": "Kommentierter Lösungsweg\n\nsummary(decay)\n##       time          amount       \n##  Min.   : 0.0   Min.   :  8.196  \n##  1st Qu.: 7.5   1st Qu.: 21.522  \n##  Median :15.0   Median : 35.015  \n##  Mean   :15.0   Mean   : 42.146  \n##  3rd Qu.:22.5   3rd Qu.: 57.460  \n##  Max.   :30.0   Max.   :125.000\nstr(decay)\n## 'data.frame':    31 obs. of  2 variables:\n##  $ time  : int  0 1 2 3 4 5 6 7 8 9 ...\n##  $ amount: num  125 100.2 70 83.5 100 ...\n\nMan erkennt, dass es 31 Beobachtungen für die Zeit als Integer von Zerfällen gibt, die als rationale Zahlen angegeben werden (dass die Zahl der Zerfälle nicht ganzzahlig ist, deutet darauf hin, dass sie möglicherweise nur in einem Teil des Zeitintervalls oder für einen Teil des betrachteten Raumes gemessen und dann hochgerechnet wurde.)\n\nExplorative Datenanalyse\n\nboxplot(decay$time)\n\n\n\nboxplot(decay$amount)\n\n\n\nplot(amount ~ time, data = decay)\n\n\n\n\nWährend der Boxplot für time wunderbar symmetrisch ohne Ausreisser ist, zeigt amount eine stark rechtsschiefe (linkssteile) Verteilung mit einem Ausreisser. Das deutet schon an, dass ein einfaches lineares Modell vermutlich die Modellannahmen verletzen wird. Auch der einfache Scatterplot zeigt, dass ein lineares Modell wohl nicht adäquat ist. Wir rechnen aber erst einmal weiter.\n\n\nEinfaches lineares Modell\n\nlm.1 &lt;- lm(amount ~ time, data = decay)\nsummary(lm.1)\n## \n## Call:\n## lm(formula = amount ~ time, data = decay)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -19.065 -10.029  -2.058   5.107  40.447 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  84.5534     5.0277   16.82  &lt; 2e-16 ***\n## time         -2.8272     0.2879   -9.82 9.94e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 14.34 on 29 degrees of freedom\n## Multiple R-squared:  0.7688, Adjusted R-squared:  0.7608 \n## F-statistic: 96.44 on 1 and 29 DF,  p-value: 9.939e-11\n\nDas sieht erst einmal nach einem Supermodell aus, höchstsignifikant und mit einem hohen R² von fast 77%. ABER: wir müssen uns noch die Modelldiagnostik ansehen…\n\n\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(lm.1)\n\n\n\n\nHier zeigen die wichtigen oberen Plots beide massive Abweichungen vom „Soll”. Der Plot oben links zeigt eine „Banane” und beim Q-Q-Plot oben rechts weichen die Punkte rechts der Mitte alle stark nach oben von der Solllinie ab. Wir haben unser Modell also offensichtlich falsch spezifiziert. Um eine Idee zu bekommen, was falsch ist, plotten wir noch, wie das Ergebnis dieses Modells aussähe:\n\n\nErgebnisplot\n\npar(mfrow = c(1, 1))\nplot(decay$time, decay$amount)\nabline(lm.1, col = \"red\")\n\n\n\n\nDie Punkte links liegen alle über der Regressionslinie, die in der Mitte darunter und die ganz rechts wieder systematisch darüber (darum im Diagnostikplot oben die „Banane”). Es liegt also offensichtlich keine lineare Beziehung vor, sondern eine curvilineare.\nUm diese korrekt zu analysieren, gibt es im Prinzip drei Möglichkeiten, wovon am zweiten Kurstag nur eine hatten, während die zweite und dritte in Statistik 3 und 4 folgten. Im Folgenden sind alle drei nacheinander dargestellt (in der Klausur würde es aber genügen, eine davon darzustellen, wenn die Aufgabenstellung wie oben lautet)."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#variante-1-log-transformation-der-abängigen-variablen",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#variante-1-log-transformation-der-abängigen-variablen",
    "title": "Stat2: Lösung Beispiel",
    "section": "Variante (1): log-Transformation der abängigen Variablen",
    "text": "Variante (1): log-Transformation der abängigen Variablen\nDass die Verteilung der abhängigen Variable nicht normal ist, haben wir ja schon bei der explorativen Datenanalyse am Anfang gesehen. Da sie stark linkssteil ist, zugleich aber keine Nullwerte enthält, bietet sich eine Logarithmustransformation an, hier z. B. mit dem natürlichen Logarithmus.\n\npar(mfrow = c(1, 2))\nboxplot(decay$amount)\nboxplot(log(decay$amount))\n\n\n\nhist(decay$amount)\nhist(log(decay$amount))\n\n\n\n\nDie log-transformierte Variante rechts sieht sowohl im Boxplot als auch im Histogramm viel symmetrischer/besser normalverteilt aus. Damit ergibt sich dann folgendes lineares Modell\n\nlm.2 &lt;- lm(log(amount) ~ time, data = decay)\nsummary(lm.2)\n## \n## Call:\n## lm(formula = log(amount) ~ time, data = decay)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.5935 -0.2043  0.0067  0.2198  0.6297 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  4.547386   0.100295   45.34  &lt; 2e-16 ***\n## time        -0.068528   0.005743  -11.93 1.04e-12 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.286 on 29 degrees of freedom\n## Multiple R-squared:  0.8308, Adjusted R-squared:  0.825 \n## F-statistic: 142.4 on 1 and 29 DF,  p-value: 1.038e-12\n\nJetzt ist der R²-Wert noch höher und der p-Wert noch niedriger als im ursprünglichen linearen Modell ohne Transformation. Das erlaubt aber keine Aussage, da wir Äpfel mit Birnen vergleichen, da die abhängige Variable einmal untransformiert und einmal log-transformiert ist. Entscheidend ist die Modelldiagnostik.\n\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(lm.2)\n\n\n\n\nDer Q-Q-Plot sieht jetzt exzellent aus, der Plot rechts oben hat kaum noch eine Banane, nur noch einen leichten Keil. Insgesamt deutlich besser und auf jeden Fall ein statistisch korrektes Modell.\nLösungen 2 und 3 greifen auf Methoden von Statistik 3 und 4 zurück, sie sind hier nur zum Vergleich angeführt"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#variante-2-quadratische-regression",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#variante-2-quadratische-regression",
    "title": "Stat2: Lösung Beispiel",
    "section": "Variante (2): Quadratische Regression",
    "text": "Variante (2): Quadratische Regression\n(kommt erst in Statistik 3) könnte für die Datenverteilung passen, entspricht aber nicht der physikalischen\n\nGesetzmässigkeit\n\nmodel.quad &lt;- lm(amount ~ time + I(time^2), data = decay)\nsummary(model.quad)\n## \n## Call:\n## lm(formula = amount ~ time + I(time^2), data = decay)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -22.302  -6.044  -1.603   4.224  20.581 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 106.38880    4.65627  22.849  &lt; 2e-16 ***\n## time         -7.34485    0.71844 -10.223 5.90e-11 ***\n## I(time^2)     0.15059    0.02314   6.507 4.73e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.205 on 28 degrees of freedom\n## Multiple R-squared:  0.908,  Adjusted R-squared:  0.9014 \n## F-statistic: 138.1 on 2 and 28 DF,  p-value: 3.122e-15\n\nHier können wir R² mit dem ursprünglichen Modell vergleichen (beide haben amount als abhängige Grösse) und es sieht viel besser aus. Sowohl der lineare als auch der quadratische Term sind hochsignifikant. Sicherheitshalber vergleichen wir die beiden Modelle aber noch mittels ANOVA.\n\n\nVergleich mit dem einfachen Modell mittels ANOVA (es ginge auch AICc)\n\nanova(lm.1, model.quad)\n## Analysis of Variance Table\n## \n## Model 1: amount ~ time\n## Model 2: amount ~ time + I(time^2)\n##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n## 1     29 5960.6                                  \n## 2     28 2372.6  1    3588.1 42.344 4.727e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIn der Tat ist das komplexere Modell (jenes mit dem quadratischen Term) höchstsignifikant besser. Jetzt brauchen wir noch die Modelldiagnostik.\n\n\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(model.quad)"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#variante-3-nicht-lineare-regression",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#variante-3-nicht-lineare-regression",
    "title": "Stat2: Lösung Beispiel",
    "section": "Variante (3): Nicht lineare Regression",
    "text": "Variante (3): Nicht lineare Regression\n(die beste, hatten wir aber am 2. Tag noch nicht; mit Startwerten muss man ggf. ausprobieren) mit Startwerten muss man ggf. ausprobieren\n\nmodel.nls &lt;- nls(amount ~ a * exp(-b * time), start = (list(a = 100, b = 1)), data = decay)\nsummary(model.nls)\n## \n## Formula: amount ~ a * exp(-b * time)\n## \n## Parameters:\n##    Estimate Std. Error t value Pr(&gt;|t|)    \n## a 1.081e+02  4.993e+00   21.66  &lt; 2e-16 ***\n## b 8.019e-02  5.833e-03   13.75 3.12e-14 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.243 on 29 degrees of freedom\n## \n## Number of iterations to convergence: 8 \n## Achieved convergence tolerance: 7.976e-06\n\n\nModelldiagnostik\n\nif (!require(nlstools)) {\n    install.packages(\"nlstools\")\n}\nlibrary(nlstools)\nresiduals.nls &lt;- nlsResiduals(model.nls)\nplot(residuals.nls)\n\n\n\n\nFür nls kann man nicht den normalen Plotbefehl für die Residualdiagnostik nehmen, sondern verwendet das Äquivalent aus nlstools. Die beiden entscheidenden Plots sind jetzt links oben und rechts unten. Der QQ-Plot hat im unteren Bereich einen kleinen Schönheitsfehler, aber ansonsten ist alles OK."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#ergebnisplots",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#ergebnisplots",
    "title": "Stat2: Lösung Beispiel",
    "section": "Ergebnisplots",
    "text": "Ergebnisplots\nDa alle drei Lösungen zumindest statistisch OK waren, sollen jetzt noch die zugehörigen Ergebnisplots erstellt werden.\n\npar(mfrow = c(1, 1))\nxv &lt;- seq(0, 30, 0.1)\n\n\nlineares Modell mit log-transformierter Abhängiger\n\n\nplot(decay$time, decay$amount)\nyv1 &lt;- exp(predict(lm.2, list(time = xv)))\nlines(xv, yv1, col = \"red\")\n\n\n\n\n\nquadratisches Modell\n\n\nplot(decay$time, decay$amount)\nyv2 &lt;- predict(model.quad, list(time = xv))\nlines(xv, yv2, col = \"blue\")\n\n\n\n\n\nnicht-lineares Modell\n\n\nplot(decay$time, decay$amount)\nyv3 &lt;- predict(model.nls, list(time = xv))\nlines(xv, yv3, col = \"green\")\n\n\n\n\nOptisch betrachtet, geben (2) und (3) den empirischen Zusammenhang etwas besser wieder als (1), da sie im linken Bereich die hohen Werte besser treffen. Man könnte sogar meinen, bei Betrachtung der Daten, dass die Werte ab time = 28 wieder leicht ansteigen, was die quadratische Funktion wiedergibt. Wer sich aber mit Physik etwas auskennt, weiss, dass Version (2) physikalisch nicht zutrifft, da die Zerfallsrate mit der Zeit immer weiter abfällt. Aufgrund der kurzen Messreihe wäre eine quadratische Funktion trotzdem eine statistisch korrekte Interpretation. Mit längeren Messreihen würde sich jedoch schnell zeigen, dass sie nicht zutrifft."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_1.html#daten-einlesen-und-exlporieren",
    "href": "stat1-4/Statistik2_Loesung_1.html#daten-einlesen-und-exlporieren",
    "title": "Stat2: Lösung 2.1",
    "section": "Daten einlesen und exlporieren",
    "text": "Daten einlesen und exlporieren\n\n\nSAR &lt;- read.delim(\"datasets/statistik/SAR.csv\", sep = \";\")  # Daten einlesen\nhead(SAR)  # Daten anschauen\n##    area richness\n## 1 1e-04        2\n## 2 1e-03        4\n## 3 1e-02        4\n## 4 1e-01       10\n## 5 1e+00       26\n## 6 1e+01       37\nstr(SAR)  # Datenformat überprüfen\n## 'data.frame':    156 obs. of  2 variables:\n##  $ area    : num  1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e-04 1e-03 1e-02 1e-01 ...\n##  $ richness: int  2 4 4 10 26 37 2 5 11 15 ...\n\n\n\nsummary(SAR)  # Überblick verschaffen\n##       area             richness    \n##  Min.   :  0.0001   Min.   : 1.00  \n##  1st Qu.:  0.0010   1st Qu.: 4.00  \n##  Median :  0.1000   Median : 9.00  \n##  Mean   :  9.4017   Mean   :16.37  \n##  3rd Qu.:  1.0000   3rd Qu.:24.00  \n##  Max.   :100.0000   Max.   :85.00\n\n\n\nboxplot(SAR$area)  # Boxplot der Flächengrösse\n\n\n\n\n-&gt; Erklärende Variable extrem rechtsschief.\n\n\nboxplot(SAR$richness)  # Boxplot der Artenzahl\n\n\n\n\n-&gt; Auch abhängige Variable extrem rechtsschief.\n\n\nplot(richness ~ area, data = SAR)  # Daten plotten\n\n\n\n\n-&gt; Zusammenhang sieht nicht linear aus.\nFazit Datenexploration: Sowohl die abhängige als auch die unabhängige Variable sind extrem rechtsschief verteilt und ihr Zusammenhang sieht nicht linear aus. Die Voraussetzungen für ein lineares Modell sehen also schlecht aus. Um diese Vermutung zu überprüfen, wird im Folgenden ein Lineares Modell mit anschliessender Modelldiagnostik gerechnet."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_1.html#einfaches-lineares-modell-mit-modelldiagnostik",
    "href": "stat1-4/Statistik2_Loesung_1.html#einfaches-lineares-modell-mit-modelldiagnostik",
    "title": "Stat2: Lösung 2.1",
    "section": "Einfaches lineares Modell mit Modelldiagnostik",
    "text": "Einfaches lineares Modell mit Modelldiagnostik\n\n\nlm.1 &lt;- lm(richness ~ area, data = SAR)  # lm erstellen\nsummary(lm.1)  # lm anschauen\n## \n## Call:\n## lm(formula = richness ~ area, data = SAR)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -29.567  -8.474  -3.503   6.112  35.317 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  11.4742     0.9582   11.97   &lt;2e-16 ***\n## area          0.5209     0.0342   15.23   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 11.27 on 154 degrees of freedom\n## Multiple R-squared:  0.601,  Adjusted R-squared:  0.5984 \n## F-statistic: 231.9 on 1 and 154 DF,  p-value: &lt; 2.2e-16\n\n-&gt; Zwar hochsignifikant, aber stimmen die Voraussetzungen?\n\n\n# Modell und Daten plotten\nplot(SAR$area, SAR$richness, xlab = \"Area [m²]\", ylab = \"Species richness\")  # Daten plotten\nabline(lm.1, col = \"red\")  # Modell plotten\n\n\n\n\n-&gt; Eine Gerade scheint ein schlechtes Modell zu sein für die Daten.\n\n\n# Modelldiagnostik\npar(mfrow = c(2, 2))  # 4-Plot-panel\nplot(lm.1)\n\n\n\n\n-&gt; Auch Modelldiagnostikplots sehen schlecht aus.\nFazit lm: Wie erwartet sind die Modellvoraussetzungen nicht gut erfüllt: Das geplottete Modell verläuft teils auffällig “neben den Daten”, im Residuals vs. Fittet Plot zeigen die Residuen Trichter- und Bananenform und der Q-Q-Plot zeigt starke Abweichung der Residuen von Normalverteilung. Da drängt sich Datentransformation auf. Als nächstes wird darum Log10-Transformation ausprobiert."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_1.html#lösung-a-log-transformation-der-abhängigen-variablen",
    "href": "stat1-4/Statistik2_Loesung_1.html#lösung-a-log-transformation-der-abhängigen-variablen",
    "title": "Stat2: Lösung 2.1",
    "section": "Lösung A: log-Transformation der abhängigen Variablen",
    "text": "Lösung A: log-Transformation der abhängigen Variablen\n\n\n# Daten vor und nach log10-Transformation vergleichen\npar(mfrow = c(2, 2))\nboxplot(SAR$richness)\nboxplot(log10(SAR$richness))\nhist(SAR$richness)\nhist(log10(SAR$richness))\n\n\n\n\n-&gt; Tansformation zeigt den gewünschten Effekt.\n\n\n# lm rechnen mit log10 transformierter abhängigen Variable\nSAR$log_richness &lt;- log10(SAR$richness)\nlm.2 &lt;- lm(log_richness ~ area, data = SAR)\nsummary(lm.2)\n## \n## Call:\n## lm(formula = log_richness ~ area, data = SAR)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.85613 -0.34114 -0.01204  0.36365  0.75729 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 0.856116   0.036657   23.36  &lt; 2e-16 ***\n## area        0.010259   0.001309    7.84 6.94e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4313 on 154 degrees of freedom\n## Multiple R-squared:  0.2853, Adjusted R-squared:  0.2806 \n## F-statistic: 61.47 on 1 and 154 DF,  p-value: 6.939e-13\n\n-&gt; Zwar hochsignifikant, aber stimmen die Voraussetzungen??\n\n\n# Modelldiagnostik\npar(mfrow = c(2, 2))\nplot(lm.2)\n\n\n\n\n-&gt; Modelldiagnostikplots sehen noch schlechter aus als mit untransformierten Daten!\nFazit Lösung A: log-Transformation der abhängigen Variablen hat das Modell nicht verbessert. Im Gegebteil… Als nächstes wird darum eine zusätzlichge Log10-Transformation der Abhängigen Variablen ausprobiert."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_1.html#lösung-b-log-transformation-beider-variablen",
    "href": "stat1-4/Statistik2_Loesung_1.html#lösung-b-log-transformation-beider-variablen",
    "title": "Stat2: Lösung 2.1",
    "section": "Lösung B: log-Transformation beider Variablen",
    "text": "Lösung B: log-Transformation beider Variablen\n\n\n# Daten vor und nach log10-Transformation vergleichen\npar(mfrow = c(2, 2))\nboxplot(SAR$area)\nboxplot(log10(SAR$area))\nhist(SAR$area)\nhist(log10(SAR$area))\n\n\n\n\n-&gt; Tansformation zeigt den gewünschten Effekt.\n\n\n# lm rechnen mit log10-Transformation beider VAriablen\nSAR$log_area &lt;- log10(SAR$area)\nlm.3 &lt;- lm(log_richness ~ log_area, data = SAR)\nsummary(lm.3)\n## \n## Call:\n## lm(formula = log_richness ~ log_area, data = SAR)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.50241 -0.09353  0.02130  0.09965  0.40068 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 1.265730   0.015607   81.10   &lt;2e-16 ***\n## log_area    0.254440   0.006926   36.73   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1633 on 154 degrees of freedom\n## Multiple R-squared:  0.8976, Adjusted R-squared:  0.8969 \n## F-statistic:  1349 on 1 and 154 DF,  p-value: &lt; 2.2e-16\n\n-&gt; Zwar hochsignifikant, aber stimmen die Voraussetzungen??\n\n\n# Modelldiagnostik\npar(mfrow = c(2, 2))\nplot(lm.3)\n\n\n\n\n-&gt; Das sieht jetzt vergleichsweise sehr gut aus! (Bis auf ein paar Aussreisser)\n\n\n# Modell und Daten plotten\nplot(SAR$log_area, SAR$log_richness, xlab = \"log10(Area [m²])\", ylab = \"log10(Species richness)\")  # Daten plotten\nabline(lm.3, col = \"red\")  # Modell plotten\n\n\n\n\n-&gt; Das Modell bildet die Daten gut ab.\nFazit Lösung B: Ein lineares Modell mit log-Transformation der unabhängigen und der abhängigen Variablen scheint die Daten am besten abzubilden. Abschliessend sollen nun die drei Modelle in einem Plot dargestellt werden."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_1.html#darstellung-der-drei-modelle",
    "href": "stat1-4/Statistik2_Loesung_1.html#darstellung-der-drei-modelle",
    "title": "Stat2: Lösung 2.1",
    "section": "Darstellung der drei Modelle",
    "text": "Darstellung der drei Modelle\n\n\n# Input-Vektor mit x-Werten für die Modelle erstellen, der die Bandbreite der\n# Daten abdeckt\nxv &lt;- seq(min(SAR$area), max(SAR$area), 0.1)\n\n\nplot(SAR$area, SAR$richness)  # Daten plotten\nabline(lm.1, col = \"red\")  # Modell 1 (untransformiert) zu Plot hinzufügen\n\n# Modell 2 (Anhängige Variable log10-transformiert) Modellvoraussagen berechnen\nlogyvlm2 &lt;- predict(lm.2, list(area = xv))\n# Modellvoraussagen rücktransformieren\nyvlm2 &lt;- 10^logyvlm2  # 10^ ist Umkekrfunktion von Log10\n# Zu Plot hinzufügen\nlines(xv, yvlm2, col = \"blue\")  # Modell 2 auf untransformierte Fläche plotten\n\n\n# Modell 2 (beide Variablen log10-transformiert) Modellvoraussagen berechnen\nlog10xv &lt;- log10(xv)  # Tansformierter Input-Vektor erstellen\nlogyvlm3 &lt;- predict(lm.3, list(log_area = log10xv))\n# Modellvoraussagen rücktransformieren\nyvlm3 &lt;- 10^logyvlm3  # 10^ ist Umkekrfunktion von Log10\nlines(xv, yvlm3, col = \"green\")  # Modell 2 auf untransformierte Fläche plotten\n\n\n\n\n--&gt; Auch hier lässt sich bestätigen dass Modell lm2 (blau) die Daten am schlechtesten und Modell lm3 (grün) die Daten am besten abbildet."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_2223s.html#musterlösung-übung-2.2",
    "href": "stat1-4/Statistik2_Loesung_2223s.html#musterlösung-übung-2.2",
    "title": "Stat2: Lösung 2.2 & 2.3S",
    "section": "Musterlösung Übung 2.2",
    "text": "Musterlösung Übung 2.2\n\nKommentierter Lösungsweg\n\n#lade die Daten\ndf &lt;- readr::read_csv2(\"datasets/statistik/Datensatz_novanimal_Uebung_Statistik2.1.csv\")\n\n# überprüft die Voraussetzungen für eine ANOVA\n# Schaut euch die Verteilungen der Mittelwerte an (plus Standardabweichungen)\n# Sind Mittelwerte nahe bei Null? \n# Gäbe uns einen weiteren Hinweis auf eine spezielle Binomail-Verteilung \naggregate(tot_sold ~ label_content, data = df, FUN = function(x) c(mn = mean(x), n = sd(x) ))\n\n  label_content tot_sold.mn tot_sold.n\n1       Fleisch  1135.58333  200.03384\n2  Hot and Cold   308.33333   23.53077\n3   Vegetarisch   739.25000  213.54204\n\n# Boxplot\nggplot(df, aes(x = label_content, y= tot_sold)) +\n  # Achtung: Reihenfolge spielt hier eine Rolle!\n  stat_boxplot(geom = \"errorbar\", width = 0.25) +\n  geom_boxplot(fill=\"white\", color = \"black\", size = 1, width = .5) +\n  labs(x = \"\\nMenu-Inhalt\", y = \"Anzahl verkaufte Gerichte pro Woche\\n\") +\n  # achtung erster Hinweis einer Varianzheterogenität, wegen den Hot&Cold Gerichten\n  mytheme\n\n\n\n#alternative mit base\nboxplot(df$tot_sold~df$label_content)\n\n\n\n# definiert das Modell (vgl. Skript Statistik 2)\nmodel &lt;- aov(tot_sold ~ label_content, data = df)\n\nsummary.lm(model)  \n\n\nCall:\naov(formula = tot_sold ~ label_content, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-290.250 -135.083    1.667  125.500  282.417 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                1135.58      48.92  23.211  &lt; 2e-16 ***\nlabel_contentHot and Cold  -827.25      69.19 -11.956 1.54e-13 ***\nlabel_contentVegetarisch   -396.33      69.19  -5.728 2.15e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 169.5 on 33 degrees of freedom\nMultiple R-squared:  0.8125,    Adjusted R-squared:  0.8012 \nF-statistic: 71.52 on 2 and 33 DF,  p-value: 1.007e-12\n\n# überprüft die Modelvoraussetzungen\npar(mfrow = c(2,2))\nplot(model)\n\n\n\n\n Fazit: Inspektion der Modellvoraussetzung zeigt klare Verletzungen des Residualplots (zeigt einen “Trichter”, siehe Skript Statistik 2), D.h. die Voraussetzung der Homoskedastizität sind verletzt. Mögliche nächste Schritte:\n\nMenüinhalt “Buffet” aus der Analyse ausschliessen, da sowieso kein richtiger Menüinhalt (ACHTUNG: Informationsverlust & inhaltiche Begründung für diesen Ausschluss)\nDatentransformation z.B. log-Transformation\nnicht-parametrischer Test (ACHTUNG: auch dieser setzt Voraussetzungen voraus)\nein glm Model (general linear model) mit einer poisson/quasipoisson link Funktion (vgl. Skript Statistik 4), weitere Infos dazu Link \n\n\n# überprüft die Voraussetzungen des Welch-Tests:\n# Gibt es eine hohe Varianzheterogenität und ist die relative Verteilung der \n# Residuen gegeben? (siehe Statistik 2)\n# Ja Varianzheterogenität ist gegeben, aber die Verteilung der Residuen folgt \n# einem \"Trichter\", also keiner \"normalen/symmetrischen\" Verteilung um 0\n# Daher ziehe ich eine Transformation der AV einem nicht-parametrischen Test vor\n# für weitere Infos: \n# https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/\n\n# achtung hier log10, bei Rücktransformation achten\nmodel_log &lt;- aov(log10(tot_sold) ~ label_content, data = df) \npar(mfrow = c(2,2))\nplot(model_log) # scheint ok zu sein\n\n\n\nsummary.lm(model_log) # Referenzkategorie ist Fleisch\n\n\nCall:\naov(formula = log10(tot_sold) ~ label_content, data = df)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.198920 -0.059343  0.003477  0.062579  0.150567 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                3.04908    0.02585 117.942  &lt; 2e-16 ***\nlabel_contentHot and Cold -0.56121    0.03656 -15.350  &lt; 2e-16 ***\nlabel_contentVegetarisch  -0.19792    0.03656  -5.413 5.45e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08956 on 33 degrees of freedom\nMultiple R-squared:  0.8802,    Adjusted R-squared:  0.8729 \nF-statistic: 121.2 on 2 and 33 DF,  p-value: 6.238e-16\n\nTukeyHSD(model_log) # (vgl. Statistik 2)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = log10(tot_sold) ~ label_content, data = df)\n\n$label_content\n                               diff        lwr        upr   p adj\nHot and Cold-Fleisch     -0.5612085 -0.6509215 -0.4714955 0.0e+00\nVegetarisch-Fleisch      -0.1979175 -0.2876305 -0.1082044 1.6e-05\nVegetarisch-Hot and Cold  0.3632910  0.2735780  0.4530041 0.0e+00\n\n# Achtung Beta-Werte resp. Koeffinzienten sind nicht direkt interpretierbar\n# sie müssten zuerst wieder zurück transformiert werden, hier ein Beispiel dafür:\n# für Fleisch\n10^model_log$coefficients[1]\n\n(Intercept) \n   1119.655 \n\n# für Hot & Cold,\n10^(model_log$coefficients[1] + model_log$coefficients[2])\n\n(Intercept) \n   307.5216 \n\n# ist equivalent zu\n10^(model_log$coefficients[1]) * 10^(model_log$coefficients[2])\n\n(Intercept) \n   307.5216 \n\n# für Vegi\n10^(model_log$coefficients[1] + model_log$coefficients[3])\n\n(Intercept) \n   709.8501 \n\n\n\nMethoden\nZiel war es, die Unterschiede in den wöchentlichen Verkaufszahlen pro Menüinhalt aufzuzeigen. Da die Responsevariable (Verkaufszahlen) “metrisch” und die Prädiktorvariable kategorial sind, wurde eine einfaktorielle ANOVA gerechnet. Die visuelle Inspektion des Modells zeigte insbesondere schwere Verletzungen der Homoskedastizität. Der Boxplot bestätigt dieser Befund. Weil die Voraussetzungen schwer verletzt sind, wurde eine log-Transformation der Responsevariable vorgenommen. Anschliessend wurde erneut eine ANOVA gerechnet und die Modelvoraussetzungen visuell inspiziert: Homoskedastizität und Normalverteilung der Residuen sind gegeben. Für mehr Informationen zu log-Transformationen und Darstellung der Ergebnisse findet ihr hier\n\n\nErgebnisse\nDie Menüinhalte (Fleisch, Vegetarisch und Buffet) unterscheiden sich in den wöchentlichen Verkaufszahlen signifikant (p &lt; .001). Die Abbildung 1 zeigt die wöchentlichen Verkaufszahlen pro Menüinhalt.\n\n\n\n\n\nAbbildung 23.1: Die wöchentlichen Verkaufzahlen unterscheiden sich je nach Menüinhalt stark. Das Modell wurde mit den log-tranformierten Daten gerechnet."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_2223s.html#musterlösung-übung-2.3s-sozwis",
    "href": "stat1-4/Statistik2_Loesung_2223s.html#musterlösung-übung-2.3s-sozwis",
    "title": "Stat2: Lösung 2.2 & 2.3S",
    "section": "Musterlösung Übung 2.3S (SozWis)",
    "text": "Musterlösung Übung 2.3S (SozWis)\n\nLese-Empfehlung Kapitel 7 von Manny Gimond\n\n\nKommentierter Lösungsweg\n\n\n  article_description tot_sold.mn tot_sold.n\n1           Fav_World    622.6667   178.7944\n2             Kitchen    128.5000    22.2085\n\n\n\n\n\n\n# definiert das Modell (Skript Statistik 2)\nmodel &lt;- aov(tot_sold ~ article_description * member, data = df)\n\nsummary.lm(model)\n\n\nCall:\naov(formula = tot_sold ~ article_description * member, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-91.00 -17.33   0.50  14.83  83.00 \n\nCoefficients:\n                                             Estimate Std. Error t value\n(Intercept)                                   452.333      9.734   46.47\narticle_descriptionKitchen                   -327.000     13.766  -23.75\nmemberStudierende                             340.667     13.766   24.75\narticle_descriptionKitchen:memberStudierende -334.333     19.469  -17.17\n                                             Pr(&gt;|t|)    \n(Intercept)                                    &lt;2e-16 ***\narticle_descriptionKitchen                     &lt;2e-16 ***\nmemberStudierende                              &lt;2e-16 ***\narticle_descriptionKitchen:memberStudierende   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 33.72 on 44 degrees of freedom\nMultiple R-squared:  0.9864,    Adjusted R-squared:  0.9855 \nF-statistic:  1063 on 3 and 44 DF,  p-value: &lt; 2.2e-16\n\n# überprüft die Modelvoraussetzungen (Statistik 2)\npar(mfrow = c(2,2)) # alternativ gäbe es die ggfortify::autoplot(model) funktion\nplot(model)\n\n\n\n\nFazit: Die Inspektion des Modells zeigt kleinere Verletzungen bei der Normalverteilung der Residuen (Q-Q Plot). Aufgrund keiner starken Verbesserung durch eine Transformation der Responsevariable, entscheide ich mich für eine ANOVA ohne log-transformierten Responsevariablen (AV).\n\n# sieht aus, als ob die Voraussetzungen für eine Anova nur geringfügig verletzt sind\n# mögliche alternativen: \n# 0. keine Tranformation der AV (machen wir hier)\n# 1. log-transformation um die grossen werte zu minimieren (nur möglich, wenn \n# keine 0 enthalten sind und die Mittelwerte weit von 0 entfernt sind (bei uns wäre dieser Fall erfüllt)\n# =&gt; bei Zähldaten ist dies leider nicht immer gegeben)\n# 2. nicht parametrische Test z.B. Welch-Test, wenn hohe Varianzheterogenität \n# zwischen den Residuen\n\n#0) keine Tranformation\n# post-hov Vergleiche\nTukeyHSD(model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = tot_sold ~ article_description * member, data = df)\n\n$article_description\n                       diff      lwr       upr p adj\nKitchen-Fav_World -494.1667 -513.785 -474.5484     0\n\n$member\n                           diff      lwr      upr p adj\nStudierende-Mitarbeitende 173.5 153.8817 193.1183     0\n\n$`article_description:member`\n                                                     diff        lwr        upr\nKitchen:Mitarbeitende-Fav_World:Mitarbeitende -327.000000 -363.75650 -290.24350\nFav_World:Studierende-Fav_World:Mitarbeitende  340.666667  303.91017  377.42317\nKitchen:Studierende-Fav_World:Mitarbeitende   -320.666667 -357.42317 -283.91017\nFav_World:Studierende-Kitchen:Mitarbeitende    667.666667  630.91017  704.42317\nKitchen:Studierende-Kitchen:Mitarbeitende        6.333333  -30.42317   43.08983\nKitchen:Studierende-Fav_World:Studierende     -661.333333 -698.08983 -624.57683\n                                                  p adj\nKitchen:Mitarbeitende-Fav_World:Mitarbeitende 0.0000000\nFav_World:Studierende-Fav_World:Mitarbeitende 0.0000000\nKitchen:Studierende-Fav_World:Mitarbeitende   0.0000000\nFav_World:Studierende-Kitchen:Mitarbeitende   0.0000000\nKitchen:Studierende-Kitchen:Mitarbeitende     0.9672944\nKitchen:Studierende-Fav_World:Studierende     0.0000000\n\n#1) Alterativ: log-transformation\nmodel_log &lt;- aov(log10(tot_sold) ~ article_description * member, data = df)\n\nsummary.lm(model_log) # interaktion ist nun nicht mehr signifikant: vgl. \n\n\nCall:\naov(formula = log10(tot_sold) ~ article_description * member, \n    data = df)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.191372 -0.025043  0.003191  0.037604  0.182842 \n\nCoefficients:\n                                             Estimate Std. Error t value\n(Intercept)                                   2.65417    0.01696 156.533\narticle_descriptionKitchen                   -0.56517    0.02398 -23.569\nmemberStudierende                             0.24438    0.02398  10.191\narticle_descriptionKitchen:memberStudierende -0.21726    0.03391  -6.407\n                                             Pr(&gt;|t|)    \n(Intercept)                                   &lt; 2e-16 ***\narticle_descriptionKitchen                    &lt; 2e-16 ***\nmemberStudierende                            3.71e-13 ***\narticle_descriptionKitchen:memberStudierende 8.51e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05874 on 44 degrees of freedom\nMultiple R-squared:  0.9745,    Adjusted R-squared:  0.9728 \nF-statistic: 561.4 on 3 and 44 DF,  p-value: &lt; 2.2e-16\n\n# nochmals euren Boxplot zu beginn, machen diese Koeffizienten sinn?\n\n# überprüft die Modelvoraussetzungen (vgl. Skript Statistik 2)\n# bringt aber keine wesentliche Verbesserung, daher bleibe ich bei den \n# untranfromierten Daten\npar(mfrow = c(2,2))\nplot(model_log)\n\n\n\n# post-hov Vergleiche\nTukeyHSD(model_log) # gibt sehr ähnliche Resultate im Vergleich zum nicht-transformierten Model\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = log10(tot_sold) ~ article_description * member, data = df)\n\n$article_description\n                        diff        lwr        upr p adj\nKitchen-Fav_World -0.6738029 -0.7079755 -0.6396302     0\n\n$member\n                               diff       lwr       upr p adj\nStudierende-Mitarbeitende 0.1357518 0.1015791 0.1699244     0\n\n$`article_description:member`\n                                                     diff         lwr\nKitchen:Mitarbeitende-Fav_World:Mitarbeitende -0.56517128 -0.62919652\nFav_World:Studierende-Fav_World:Mitarbeitende  0.24438333  0.18035809\nKitchen:Studierende-Fav_World:Mitarbeitende   -0.53805110 -0.60207634\nFav_World:Studierende-Kitchen:Mitarbeitende    0.80955461  0.74552937\nKitchen:Studierende-Kitchen:Mitarbeitende      0.02712017 -0.03690507\nKitchen:Studierende-Fav_World:Studierende     -0.78243444 -0.84645968\n                                                      upr     p adj\nKitchen:Mitarbeitende-Fav_World:Mitarbeitende -0.50114604 0.0000000\nFav_World:Studierende-Fav_World:Mitarbeitende  0.30840857 0.0000000\nKitchen:Studierende-Fav_World:Mitarbeitende   -0.47402586 0.0000000\nFav_World:Studierende-Kitchen:Mitarbeitende    0.87357985 0.0000000\nKitchen:Studierende-Kitchen:Mitarbeitende      0.09114541 0.6726112\nKitchen:Studierende-Fav_World:Studierende     -0.71840920 0.0000000\n\n\n\nMethode\nZiel war es die Unterschiede zwischen den preisgünstigeren und teureren Menülinien und der Hochschulzugehörigkeit herauszufinden: Hierfür wurde eine ANOVA mit Interaktion gerechnet, da wir eine (quasi)-metrische Responsevariable und zwei Prädiktorvariablen (Menülinie und Hochschulzugehörigkeit) haben.\nDie Voraussetzungen für eine ANOVA waren im ersten Model nicht stark verletzt, lediglich die Normalverteilung der Residuen: Deshalb habe wurde auf eine log-Transformation der Responsevariable verzichtet. Anschliessend wurden noch post-hoc Einzelvergleiche nach Tukey durchgeführt.\nKleiner Exkurs: Verkaufsdaten sind Zähldaten und per se binomial-Verteilt, da es keine negativen Werte geben kann. Ich versuche immer folgende Fragen zu beantworten:\n\nWie weit ist der Mittelwert von “Null entfernt”? -&gt; Wenn ja uns keine Voraussetzungen zur Normalverteilung gibt, kann auch eine Normalverteilung angenommen werden\nBeinhalten die Daten viele “Null’s”? -&gt; Wenn ja muss eine spezielle binomial Verteilung angenommen werden, z.B. negative binomiale Transformation mit GLM (see Skript XY)\n\n\n\nErgebnisse\nDie wöchentlichen Verkaufszahlen der Menülinien unterscheiden sich nach Hochschulzugehörigkeit signifikant (p &lt; .001). Inhaltich bedeutet dies, dass Studierende signifikant häufiger die preisgünstigere Menülinie “Favorite & World” als Mitarbeitende kaufen. Entgegen der Annahme gibt es aber keine signifikanten Unterschiede zwischen Studierende und Mitarbeitende bei dem Kauf der teureren Menülinie “Kitchen”. Über die möglichen Gründe können nur spekuliert werden, hierfür bedarf es weiteren Analysen z.B. mit dem Prädiktor “Menüinhalt”.\n\n\n\n\n\nAbbildung 23.2: Box-Whisker-Plots der wöchentlichen Verkaufszahlen pro Menü-Inhalte. Kleinbuchstaben bezeichnen homogene Gruppen auf p &lt; .05 nach Tukeys post-hoc-Test."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_23n.html#kommentierter-lösungsweg",
    "href": "stat1-4/Statistik2_Loesung_23n.html#kommentierter-lösungsweg",
    "title": "Stat2: Lösung 2.3N",
    "section": "Kommentierter Lösungsweg",
    "text": "Kommentierter Lösungsweg\n\n# Working directory muss angepasst werden\nkormoran &lt;- read.delim(\"datasets/statistik/kormoran.csv\", sep = \";\", stringsAsFactors = T)  # \n\n# Ueberpruefen, ob Einlesen richtig funktioniert hat und welche Datenstruktur\n# vorliegt\nstr(kormoran)\n## 'data.frame':    40 obs. of  4 variables:\n##  $ Obs       : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ Tauchzeit : num  9.5 11.9 13.4 13.8 15.3 15.5 15.6 16.7 16.8 18.7 ...\n##  $ Unterart  : Factor w/ 2 levels \"C\",\"S\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Jahreszeit: Factor w/ 4 levels \"F\",\"H\",\"S\",\"W\": 1 1 1 1 1 3 3 3 3 3 ...\nsummary(kormoran)\n##       Obs          Tauchzeit     Unterart Jahreszeit\n##  Min.   : 1.00   Min.   : 9.50   C:20     F:10      \n##  1st Qu.:10.75   1st Qu.:13.38   S:20     H:10      \n##  Median :20.50   Median :16.75            S:10      \n##  Mean   :20.50   Mean   :17.40            W:10      \n##  3rd Qu.:30.25   3rd Qu.:20.77                      \n##  Max.   :40.00   Max.   :30.40\n\nMan erkennt, dass es sich um einen Dataframe mit einer metrischen (Tauchzeit) und zwei kategorialen (Unterart, Jahreszeit) Variablen handelt. Die adäquate Analyse (1 metrische Abhängige vs. 2 kategoriale Unabhängige) ist damit eine zweifaktorielle ANOVA Die Sortierung der Jahreszeiten (default: alphabetisch) ist inhaltlich aber nicht sinnvoll und sollte angepasst werden.\n\n# Umsortieren der Faktoren, damit sie in den Boxplots eine sinnvolle Reihung\n# haben\nkormoran$Jahreszeit &lt;- ordered(kormoran$Jahreszeit, levels = c(\"F\", \"S\", \"H\", \"W\"))\nkormoran$Jahreszeit\n##  [1] F F F F F S S S S S H H H H H W W W W W F F F F F S S S S S H H H H H W W W\n## [39] W W\n## Levels: F &lt; S &lt; H &lt; W\n\n# Explorative Datenanalyse (zeigt uns die Gesamtverteilung)\nboxplot(kormoran$Tauchzeit)\n\n\n\n\nDas ist noch OK für parametrische Verfahren (Box ziemlich symmetrisch um Median, Whisker etwas asymmetrisch aber nicht kritisch). Wegen der leichten Asymmetrie (Linksschiefe) könnte man eine log-Transformation ausprobieren.\n\nboxplot(log10(kormoran$Tauchzeit))\n\n\n\n\nDer Gesamtboxplot für log10 sieht perfekt symmetrisch aus, das spräche also für eine log10-Transformation. De facto kommt es aber nicht auf den Gesamtboxplot an, sondern auf die einzelnen.\n\n# Explorative Datenanalyse (Check auf Normalverteilung der Residuen und\n# Varianzhomogenitaet)\nboxplot(Tauchzeit ~ Jahreszeit * Unterart, data = kormoran)\n\n\n\nboxplot(log10(Tauchzeit) ~ Jahreszeit * Unterart, data = kormoran)\n\n\n\n\nHier sieht mal die Verteilung für die untransformierten Daten, mal für die transformierten besser aus. Da die Transformation keine klare Verbesserung bringt, bleiben wir im Folgenden bei den untransformierten Daten, da diese leichter (direkter) interpretiert werden können\n\n# Vollständiges Modell mit Interaktion\naov.1 &lt;- aov(Tauchzeit ~ Unterart * Jahreszeit, data = kormoran)\naov.1\n## Call:\n##    aov(formula = Tauchzeit ~ Unterart * Jahreszeit, data = kormoran)\n## \n## Terms:\n##                 Unterart Jahreszeit Unterart:Jahreszeit Residuals\n## Sum of Squares   106.929    756.170              11.009    84.992\n## Deg. of Freedom        1          3                   3        32\n## \n## Residual standard error: 1.629724\n## Estimated effects may be unbalanced\nsummary(aov.1)\n##                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## Unterart             1  106.9  106.93  40.259 4.01e-07 ***\n## Jahreszeit           3  756.2  252.06  94.901 5.19e-16 ***\n## Unterart:Jahreszeit  3   11.0    3.67   1.382    0.266    \n## Residuals           32   85.0    2.66                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# p-Wert der Interaktion ist 0.266\n\nDas volle (maximale) Modell zeigt, dass es keine signifikante Interaktion zwischen Jahreszeit und Unterart gibt. Wir können das Modell also vereinfachen, indem wir die Interaktion herausnehmen (+ statt * in der Modellspezifikation)\n\n# Modellvereinfachung\naov.2 &lt;- aov(Tauchzeit ~ Unterart + Jahreszeit, data = kormoran)\naov.2\n## Call:\n##    aov(formula = Tauchzeit ~ Unterart + Jahreszeit, data = kormoran)\n## \n## Terms:\n##                 Unterart Jahreszeit Residuals\n## Sum of Squares   106.929    756.170    96.001\n## Deg. of Freedom        1          3        35\n## \n## Residual standard error: 1.656166\n## Estimated effects may be unbalanced\nsummary(aov.2)\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## Unterart     1  106.9  106.93   38.98 3.69e-07 ***\n## Jahreszeit   3  756.2  252.06   91.89  &lt; 2e-16 ***\n## Residuals   35   96.0    2.74                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIm so vereinfachten Modell sind alle verbleibenden Terme signifikant, wir sind also beim „minimal adäquaten Modell“ angelangt\n\n# Anderer Weg, um zu pruefen, ob man das komplexere Modell mit Interaktion\n# behalten soll\nanova(aov.1, aov.2)\n## Analysis of Variance Table\n## \n## Model 1: Tauchzeit ~ Unterart * Jahreszeit\n## Model 2: Tauchzeit ~ Unterart + Jahreszeit\n##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n## 1     32 84.992                           \n## 2     35 96.001 -3   -11.009 1.3817 0.2661\n# In diesem Fall bekommen wir den gleichen p-Wert wie oben (0.266)\n\n# Modelldiagnostik\npar(mfrow = c(2, 2))  #alle vier Abbildungen in einem 2 x 2 Raster\nplot(aov.2)\n\n\n\n\n\ninfluence.measures(aov.2)  # \n# kann man sich zusätzlich zum 'plot' ansehen, um herauszufinden, ob es evtl.\n# sehr einflussreiche Werte mit Cook's D von 1 oder grösser gibt\n\nLinks oben ist alles bestens, d. h. keine Hinweise auf Varianzheterogenität („Keil“) oder Nichtlinearität („Banane“) Rechts oben ganz gut, allerdings weichen Punkte 1 und 20 deutlich von der optimalen Gerade ab -&gt; aus diesem Grund können wir es doch noch mal mit der log10-Transformation versuchen (s.u.) Rechts unten: kein Punkt hat einen problematischen Einfluss (die roten Linien für Cook’s D &gt; 0.5 und &gt; 1 sind noch nicht einmal im Bildausschnitt.\n\n# Alternative mit log10\naov.3 &lt;- aov(log10(Tauchzeit) ~ Unterart + Jahreszeit, data = kormoran)\naov.3\n## Call:\n##    aov(formula = log10(Tauchzeit) ~ Unterart + Jahreszeit, data = kormoran)\n## \n## Terms:\n##                  Unterart Jahreszeit Residuals\n## Sum of Squares  0.0627004  0.4958434 0.0562031\n## Deg. of Freedom         1          3        35\n## \n## Residual standard error: 0.04007247\n## Estimated effects may be unbalanced\nsummary(aov.3)\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## Unterart     1 0.0627 0.06270   39.05 3.64e-07 ***\n## Jahreszeit   3 0.4958 0.16528  102.93  &lt; 2e-16 ***\n## Residuals   35 0.0562 0.00161                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nplot(aov.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRechts oben: Punkt 20 jetzt auf der Linie, aber Punkt 1 weicht umso deutlicher ab -&gt; keine Verbesserung -&gt; wir bleiben bei den untransformierten Daten. Da wir keine Interaktion zwischen Unterart und Jahreszeit festgestellt haben, brauchen wir auch keinen Interaktionsplot (unnötig kompliziert), statt dessen können wir die Ergebnisse am besten mit zwei getrennten Plots für die beiden Faktoren darstellen. Bitte die Achsenbeschriftungen und den Tukey post-hoc-Test nicht vergessen.\n\npar(mfrow = c(1, 1))  #Zurückschalten auf Einzelplots\nif (!require(multcomp)) {\n    install.packages(\"multcomp\")\n}\nlibrary(multcomp)\n\nboxplot(Tauchzeit ~ Unterart, data = kormoran)\n\n\n\n\nletters &lt;- cld(glht(aov.2, linfct = mcp(Jahreszeit = \"Tukey\")))\nboxplot(Tauchzeit ~ Jahreszeit, data = kormoran)\nmtext(letters$mcletters$Letters, at = 1:4)\n\n\n\n\nJetzt brauchen wir noch die Mittelwerte bzw. Effektgroessen\nFür den Ergebnistext brauchen wir auch noch Angaben zu den Effektgrössen. Hier sind zwei Möglichkeiten, um an sie zu gelangen.\n\naggregate(Tauchzeit ~ Jahreszeit, FUN = mean, data = kormoran)\n##   Jahreszeit Tauchzeit\n## 1          F     11.86\n## 2          S     15.09\n## 3          H     19.23\n## 4          W     23.42\naggregate(Tauchzeit ~ Unterart, FUN = mean, data = kormoran)\n##   Unterart Tauchzeit\n## 1        C    19.035\n## 2        S    15.765\n\nsummary(lm(Tauchzeit ~ Jahreszeit, data = kormoran))\n## \n## Call:\n## lm(formula = Tauchzeit ~ Jahreszeit, data = kormoran)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.820 -1.617 -0.145  1.587  6.980 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   17.4000     0.3754  46.351  &lt; 2e-16 ***\n## Jahreszeit.L   8.6804     0.7508  11.562 1.12e-13 ***\n## Jahreszeit.Q   0.4800     0.7508   0.639    0.527    \n## Jahreszeit.C  -0.1923     0.7508  -0.256    0.799    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.374 on 36 degrees of freedom\n## Multiple R-squared:  0.7884, Adjusted R-squared:  0.7708 \n## F-statistic: 44.72 on 3 and 36 DF,  p-value: 3.156e-12\nsummary(lm(Tauchzeit ~ Unterart, data = kormoran))\n## \n## Call:\n## lm(formula = Tauchzeit ~ Unterart, data = kormoran)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -9.535 -3.585 -0.335  3.760 11.365 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   19.035      1.059  17.976   &lt;2e-16 ***\n## UnterartS     -3.270      1.498  -2.184   0.0352 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.736 on 38 degrees of freedom\n## Multiple R-squared:  0.1115, Adjusted R-squared:  0.08811 \n## F-statistic: 4.768 on 1 and 38 DF,  p-value: 0.03523"
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#ancova",
    "href": "stat1-4/Statistik3_Demo.html#ancova",
    "title": "Stat3: Demo",
    "section": "ANCOVA",
    "text": "ANCOVA\nExperiment zur Fruchtproduktion (“Fruit”) von Ipomopsis sp. (“Fruit”) in Abhängigkeit von der Beweidung (“Grazing” mit 2 Levels: “Grazed”, “Ungrazed”) und korrigiert für die Pflanzengrösse vor der Beweidung (hier ausgedrückt als Durchmesser an der Spitze des Wurzelstock: “Root”)\n\n# Daten einlesen und anschauen\n\ncompensation &lt;- read.delim(\"datasets/statistik/ipomopsis.csv\", sep = \",\", stringsAsFactors = T)\nhead(compensation)\n##   X  Root Fruit  Grazing\n## 1 1 6.225 59.77 Ungrazed\n## 2 2 6.487 60.98 Ungrazed\n## 3 3 4.919 14.73 Ungrazed\n## 4 4 5.130 19.28 Ungrazed\n## 5 5 5.417 34.25 Ungrazed\n## 6 6 5.359 35.53 Ungrazed\nsummary(compensation)\n##        X              Root            Fruit            Grazing  \n##  Min.   : 1.00   Min.   : 4.426   Min.   : 14.73   Grazed  :20  \n##  1st Qu.:10.75   1st Qu.: 6.083   1st Qu.: 41.15   Ungrazed:20  \n##  Median :20.50   Median : 7.123   Median : 60.88                \n##  Mean   :20.50   Mean   : 7.181   Mean   : 59.41                \n##  3rd Qu.:30.25   3rd Qu.: 8.510   3rd Qu.: 76.19                \n##  Max.   :40.00   Max.   :10.253   Max.   :116.05\n\n# Pflanzengrösse ('Root') vs. Fruchtproduktion ('Fruit')\nplot(Fruit ~ Root, data = compensation)\n\n\n\n\n-&gt; Je grösser die Pflanze, desto grösser ihre Fruchtproduktion.\n\n# Beweidung ('Grazing') vs. Fruchtroduktion ('Fruit)\nboxplot(Fruit ~ Grazing, data = compensation)\n\n\n\n\n-&gt; In der beweideten Gruppe scheint die Fruchtproduktion grösser. Liegt dies an der Beweidung oder an unterschiedlichen Pflanzengrössen zwischen den Gruppen?\n\n# Plotten der vollständigen Daten/Information\nlibrary(tidyverse)\nggplot(compensation, aes(Root, Fruit, color = Grazing)) + geom_point() + theme_classic()\n\n\n\n\n-&gt; Die grössere Fruchtproduktion innerhalb der beweideten Gruppe scheint also ein Resultat von unterschiedlichen Pflanzengrössen zwischen den Gruppen zu sein und nicht an der Beweidung zu liegen.\n\n# Lineare Modelle definieren und anschauen\n\naoc.1 &lt;- lm(Fruit ~ Root * Grazing, data = compensation)  # Volles Modell mit Interaktion\nsummary.aov(aoc.1)\n##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## Root          1  16795   16795 359.968  &lt; 2e-16 ***\n## Grazing       1   5264    5264 112.832 1.21e-12 ***\n## Root:Grazing  1      5       5   0.103     0.75    \n## Residuals    36   1680      47                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naoc.2 &lt;- lm(Fruit ~ Grazing + Root, data = compensation)  # Finales Modell ohne die (nicht signifikante Interaktion)\nsummary.aov(aoc.2)  # ANOVA-Tabelle\n##             Df Sum Sq Mean Sq F value  Pr(&gt;F)    \n## Grazing      1   2910    2910   63.93 1.4e-09 ***\n## Root         1  19149   19149  420.62 &lt; 2e-16 ***\n## Residuals   37   1684      46                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(aoc.2)  # Parameter-Tabelle\n## \n## Call:\n## lm(formula = Fruit ~ Grazing + Root, data = compensation)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -17.1920  -2.8224   0.3223   3.9144  17.3290 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)     -127.829      9.664  -13.23 1.35e-15 ***\n## GrazingUngrazed   36.103      3.357   10.75 6.11e-13 ***\n## Root              23.560      1.149   20.51  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.747 on 37 degrees of freedom\n## Multiple R-squared:  0.9291, Adjusted R-squared:  0.9252 \n## F-statistic: 242.3 on 2 and 37 DF,  p-value: &lt; 2.2e-16\n\n# Residualplots anschauen\npar(mfrow = c(2, 2))\nplot(aoc.2)\n\n\n\n\n-&gt; Das ANCOVA-Modell widerspiegelt die Zusammenhänge wie sie aufgrund der grafisch dargestellten Daten zu vermuten sind gut. Die Residual-Plots zeigen 3 Ausreisser (Beobachtungen 27, 34 und 37), welche “aus der Reihe tanzen”."
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#polynomische-regression",
    "href": "stat1-4/Statistik3_Demo.html#polynomische-regression",
    "title": "Stat3: Demo",
    "section": "Polynomische Regression",
    "text": "Polynomische Regression\n\n# Daten generieren und Modelle rechnen\npred &lt;- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24)  # 'pred' sei unsere unabhängige Variable\nresp &lt;- c(12, 15, 10, 7, 2, 10, 12, 11, 13, 10, 9, 2, 4, 7, 13)  # 'resp' sei unsere abhängige Variable\n\nplot(pred, resp)  # So sehen die Daten aus\n\n\n\n\n# Modelle definieren\nlm.1 &lt;- lm(resp ~ pred)  # Einfaches lineares Modell\nlm.quad &lt;- lm(resp ~ pred + I(pred^2))  # lineares Modell mit quadratischem Term\n\nsummary(lm.1)  # Modell anschauen\n## \n## Call:\n## lm(formula = resp ~ pred)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.0549 -1.7015  0.5654  2.0617  5.6406 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  12.2879     2.4472   5.021 0.000234 ***\n## pred         -0.1541     0.1092  -1.412 0.181538    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.863 on 13 degrees of freedom\n## Multiple R-squared:  0.1329, Adjusted R-squared:  0.06622 \n## F-statistic: 1.993 on 1 and 13 DF,  p-value: 0.1815\n\n-&gt; kein signifikanter Zusammenhang und entsprechend kleines Bestimmtheitsmass (adj. R2 = 0.07)\n\nsummary(lm.quad)  # Modell anschauen\n## \n## Call:\n## lm(formula = resp ~ pred + I(pred^2))\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4.3866 -1.1018 -0.2027  1.3831  4.4211 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept) -2.239308   3.811746  -0.587  0.56777   \n## pred         1.330933   0.360105   3.696  0.00306 **\n## I(pred^2)   -0.031587   0.007504  -4.209  0.00121 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.555 on 12 degrees of freedom\n## Multiple R-squared:  0.6499, Adjusted R-squared:  0.5915 \n## F-statistic: 11.14 on 2 and 12 DF,  p-value: 0.001842\n\n-&gt; signifikanter Zusammenhang und viel besseres Bestimmtheitsmass (adj. R2 = 0.60)\n\n# Modelle plotten\n\npar(mfrow = c(1, 2))\n\n# 1. lineares Modell\nplot(resp ~ pred, main = \"Lineares Modell\")\nabline(lm.1, col = \"blue\")\n\n# 2. quadratisches Modell\nplot(resp ~ pred, main = \"Quadratisches  Modell\")\nxv &lt;- seq(0, 40, 0.1)  # Input für Modellvoraussage via predict ()\nyv2 &lt;- predict(lm.quad, list(pred = xv))\nlines(xv, yv2, col = \"red\")\n\n\n\n\n\n# Residualplots\npar(mfrow = c(2, 2))\nplot(lm.1, main = \"Lineares Modell\")\n\n\n\nplot(lm.quad, main = \"Quadratisches  Modell\")\n\n\n\n\n\nSimulation Overfitting\n\n# Beispieldaten mit 6 Datenpunkten\ntest &lt;- data.frame(x = c(1, 2, 3, 4, 5, 6), y = c(34, 21, 70, 47, 23, 45))\n\npar(mfrow = c(1, 1))\nplot(y ~ x, data = test)\n\n\n\n\n\n# Zunehmend komplizierte Modelle (je komplizierter desto overfitteter)\n# definieren\nlm.0 &lt;- lm(y ~ 1, data = test)\nlm.1 &lt;- lm(y ~ x, data = test)\nlm.2 &lt;- lm(y ~ x + I(x^2), data = test)\nlm.3 &lt;- lm(y ~ x + I(x^2) + I(x^3), data = test)\nlm.4 &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data = test)\nlm.5 &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5), data = test)\n\n# Summaries rechnen\nsmy.0 &lt;- summary(lm.0)\nsmy.1 &lt;- summary(lm.1)\nsmy.2 &lt;- summary(lm.2)\nsmy.3 &lt;- summary(lm.3)\nsmy.4 &lt;- summary(lm.4)\nsmy.5 &lt;- summary(lm.5)\n\n# R2 vergleichen\n\nsmy.0$r.squared\n## [1] 0\nsmy.1$r.squared\n## [1] 0.01242685\nsmy.2$r.squared\n## [1] 0.1105981\nsmy.3$r.squared\n## [1] 0.1697982\nsmy.4$r.squared\n## [1] 0.874639\nsmy.5$r.squared\n## [1] 1\nsmy.5$adj.r.squared\n## [1] NaN\n\n-&gt; R2 wird immer grösser, d.h. die Modelle werden immer besser. ;-)\n\n# Modelle plotten\nxv &lt;- seq(from = 0, to = 10, by = 0.1)\nplot(y ~ x, cex = 2, col = \"black\", lwd = 3, data = test)\nyv &lt;- predict(lm.1, list(x = xv))\nlines(xv, yv, col = \"red\", lwd = 3)\ntext(x = c(1, 70), \"lm.1\", col = \"red\")\nyv &lt;- predict(lm.2, list(x = xv))\nlines(xv, yv, col = \"blue\", lwd = 3)\ntext(x = c(1, 65), \"lm.2\", col = \"blue\")\nyv &lt;- predict(lm.3, list(x = xv))\nlines(xv, yv, col = \"green\", lwd = 3)\ntext(x = c(1, 60), \"lm.3\", col = \"green\")\nyv &lt;- predict(lm.4, list(x = xv))\nlines(xv, yv, col = \"orange\", lwd = 3)\ntext(x = c(1, 55), \"lm.4\", col = \"orange\")\nyv &lt;- predict(lm.5, list(x = xv))\nlines(xv, yv, col = \"violet\", lwd = 3)\ntext(x = c(1, 50), \"lm.5\", col = \"violet\")\n\n\n\n\n-&gt; Auch der optische Fit wird immer besser. Wir bestreiben jedoch Overfitting und Overfittig ist nicht gut: Denn, macht es Sinn, 6 Datenpunkte mit einem Modell mit 6 Parametern zu fitten??"
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#multiple-lineare-regression-basierend-auf-logan-beispiel-9a",
    "href": "stat1-4/Statistik3_Demo.html#multiple-lineare-regression-basierend-auf-logan-beispiel-9a",
    "title": "Stat3: Demo",
    "section": "Multiple lineare Regression (basierend auf Logan, Beispiel 9A)",
    "text": "Multiple lineare Regression (basierend auf Logan, Beispiel 9A)\n\n# Daten laden und anschauen\nloyn &lt;- read.delim(\"datasets/statistik/loyn.csv\", sep = \",\")\nsummary(loyn)\n##        X             ABUND            AREA            YR.ISOL    \n##  Min.   : 1.00   Min.   : 1.50   Min.   :   0.10   Min.   :1890  \n##  1st Qu.:14.75   1st Qu.:12.40   1st Qu.:   2.00   1st Qu.:1928  \n##  Median :28.50   Median :21.05   Median :   7.50   Median :1962  \n##  Mean   :28.50   Mean   :19.51   Mean   :  69.27   Mean   :1950  \n##  3rd Qu.:42.25   3rd Qu.:28.30   3rd Qu.:  29.75   3rd Qu.:1966  \n##  Max.   :56.00   Max.   :39.60   Max.   :1771.00   Max.   :1976  \n##       DIST            LDIST            GRAZE            ALT       \n##  Min.   :  26.0   Min.   :  26.0   Min.   :1.000   Min.   : 60.0  \n##  1st Qu.:  93.0   1st Qu.: 158.2   1st Qu.:2.000   1st Qu.:120.0  \n##  Median : 234.0   Median : 338.5   Median :3.000   Median :140.0  \n##  Mean   : 240.4   Mean   : 733.3   Mean   :2.982   Mean   :146.2  \n##  3rd Qu.: 333.2   3rd Qu.: 913.8   3rd Qu.:4.000   3rd Qu.:182.5  \n##  Max.   :1427.0   Max.   :4426.0   Max.   :5.000   Max.   :260.0\n\n\nKorrelation zwischen den Prädiktoren\n\n# Wir setzen die Schwelle bei |0.7|\n\ncor &lt;- cor(loyn[, 3:8])  # Korrelationen rechnen details siehe: '?cor'\n\n# Korrelationen Visualisieren (google: 'correlation plot r'...)\nif (!require(corrplot)) {\n    install.packages(\"corrplot\")\n}\nlibrary(corrplot)\n\ncorrplot.mixed(cor, lower = \"ellipse\", upper = \"number\", order = \"AOE\")\n\n\n\n\n-&gt; Keine Korrelation ist &gt;|0.7| . Aber es gilt zu beachten , dass GRAZE ziemlich stark |&gt;0.6| mit YR.ISOL korreliert ist\n\n# Volles Modell definieren\n\nnames(loyn)\n## [1] \"X\"       \"ABUND\"   \"AREA\"    \"YR.ISOL\" \"DIST\"    \"LDIST\"   \"GRAZE\"  \n## [8] \"ALT\"\nlm.1 &lt;- lm(ABUND ~ YR.ISOL + AREA + DIST + LDIST + GRAZE + ALT, data = loyn)\nif (!require(car)) {\n    install.packages(\"car\")\n}\nlibrary(car)\n\npar(mfrow = c(2, 2))\nplot(lm.1)\n\n\n\n\n-&gt; Plot sieht zwar ok aus, aber mit 6 Prädiktoren ist das Modell wohl “overfitted”\n\nvif(lm.1)\n##  YR.ISOL     AREA     DIST    LDIST    GRAZE      ALT \n## 1.841657 1.337627 1.227387 1.255028 2.307661 1.574537\n\n\n\nModellvereinfachung\nSchrittweise die am wenigsten signifkanten Terme entfernen:\n\nlm.1 &lt;- lm(ABUND ~ YR.ISOL + AREA + DIST + LDIST + GRAZE + ALT, data = loyn)\nsummary(lm.1)\n## \n## Call:\n## lm(formula = ABUND ~ YR.ISOL + AREA + DIST + LDIST + GRAZE + \n##     ALT, data = loyn)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -17.6638  -4.6409  -0.0883   4.2858  20.1042 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept) -1.097e+02  1.133e+02  -0.968  0.33791   \n## YR.ISOL      6.693e-02  5.684e-02   1.177  0.24472   \n## AREA         8.866e-04  4.657e-03   0.190  0.84980   \n## DIST         3.811e-03  5.418e-03   0.703  0.48514   \n## LDIST        1.418e-03  1.310e-03   1.082  0.28451   \n## GRAZE       -3.447e+00  1.107e+00  -3.114  0.00308 **\n## ALT          4.772e-02  3.089e-02   1.545  0.12878   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.947 on 49 degrees of freedom\n## Multiple R-squared:  0.5118, Adjusted R-squared:  0.452 \n## F-statistic: 8.561 on 6 and 49 DF,  p-value: 2.24e-06\n\nlm.2 &lt;- update(lm.1, ~. - AREA)  # Prädiktor mit grösstem p-Wert entfernen\nanova(lm.1, lm.2)  # Modelle vergleichen (falls signifikant, so müssten man den Prädiktor wieder ins Modell nehmen)\n## Analysis of Variance Table\n## \n## Model 1: ABUND ~ YR.ISOL + AREA + DIST + LDIST + GRAZE + ALT\n## Model 2: ABUND ~ YR.ISOL + DIST + LDIST + GRAZE + ALT\n##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n## 1     49 3094.2                           \n## 2     50 3096.5 -1   -2.2886 0.0362 0.8498\nsummary(lm.2)  # Neues einfacheres Modell anschauen und Prädiktor mit grösstem p-Wert ausfindig machen\n## \n## Call:\n## lm(formula = ABUND ~ YR.ISOL + DIST + LDIST + GRAZE + ALT, data = loyn)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -17.7240  -4.7245   0.0206   4.2698  20.0630 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept) -1.044e+02  1.089e+02  -0.959  0.34202   \n## YR.ISOL      6.418e-02  5.445e-02   1.179  0.24409   \n## DIST         3.884e-03  5.352e-03   0.726  0.47145   \n## LDIST        1.440e-03  1.292e-03   1.115  0.27036   \n## GRAZE       -3.500e+00  1.060e+00  -3.303  0.00177 **\n## ALT          4.964e-02  2.891e-02   1.717  0.09212 . \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.87 on 50 degrees of freedom\n## Multiple R-squared:  0.5114, Adjusted R-squared:  0.4626 \n## F-statistic: 10.47 on 5 and 50 DF,  p-value: 6.532e-07\n\n# Oben beschriebene Schritte wiederholen bis nur noch signifikante Prädiktoren\n# im Modell\nlm.3 &lt;- update(lm.2, ~. - DIST)\nanova(lm.2, lm.3)\n## Analysis of Variance Table\n## \n## Model 1: ABUND ~ YR.ISOL + DIST + LDIST + GRAZE + ALT\n## Model 2: ABUND ~ YR.ISOL + LDIST + GRAZE + ALT\n##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n## 1     50 3096.5                           \n## 2     51 3129.1 -1   -32.609 0.5265 0.4714\nsummary(lm.3)\n## \n## Call:\n## lm(formula = ABUND ~ YR.ISOL + LDIST + GRAZE + ALT, data = loyn)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -18.4659  -4.8236   0.1506   4.9245  19.8891 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -99.587487 108.158382  -0.921 0.361513    \n## YR.ISOL       0.062627   0.054157   1.156 0.252910    \n## LDIST         0.001677   0.001245   1.347 0.184026    \n## GRAZE        -3.699613   1.018706  -3.632 0.000653 ***\n## ALT           0.046485   0.028446   1.634 0.108386    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.833 on 51 degrees of freedom\n## Multiple R-squared:  0.5063, Adjusted R-squared:  0.4676 \n## F-statistic: 13.07 on 4 and 51 DF,  p-value: 2.123e-07\n\nlm.4 &lt;- update(lm.3, ~. - YR.ISOL)\nanova(lm.3, lm.4)\n## Analysis of Variance Table\n## \n## Model 1: ABUND ~ YR.ISOL + LDIST + GRAZE + ALT\n## Model 2: ABUND ~ LDIST + GRAZE + ALT\n##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n## 1     51 3129.1                           \n## 2     52 3211.2 -1   -82.047 1.3372 0.2529\nsummary(lm.4)\n## \n## Call:\n## lm(formula = ABUND ~ LDIST + GRAZE + ALT, data = loyn)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -18.155  -4.148  -0.503   4.649  18.588 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 25.289313   6.080034   4.159  0.00012 ***\n## LDIST        0.001455   0.001234   1.179  0.24362    \n## GRAZE       -4.430947   0.801206  -5.530 1.05e-06 ***\n## ALT          0.043565   0.028425   1.533  0.13144    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.858 on 52 degrees of freedom\n## Multiple R-squared:  0.4933, Adjusted R-squared:  0.4641 \n## F-statistic: 16.88 on 3 and 52 DF,  p-value: 8.777e-08\n\nlm.5 &lt;- update(lm.4, ~. - LDIST)\nanova(lm.4, lm.5)\n## Analysis of Variance Table\n## \n## Model 1: ABUND ~ LDIST + GRAZE + ALT\n## Model 2: ABUND ~ GRAZE + ALT\n##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n## 1     52 3211.2                           \n## 2     53 3297.1 -1   -85.892 1.3909 0.2436\nsummary(lm.5)\n## \n## Call:\n## lm(formula = ABUND ~ GRAZE + ALT, data = loyn)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -19.1677  -4.8261   0.0266   4.6944  19.1054 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 28.55582    5.43245   5.257 2.67e-06 ***\n## GRAZE       -4.59679    0.79167  -5.806 3.67e-07 ***\n## ALT          0.03191    0.02675   1.193    0.238    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.887 on 53 degrees of freedom\n## Multiple R-squared:  0.4798, Adjusted R-squared:  0.4602 \n## F-statistic: 24.44 on 2 and 53 DF,  p-value: 3.011e-08\n\nlm.6 &lt;- update(lm.5, ~. - ALT)\nanova(lm.5, lm.6)\n## Analysis of Variance Table\n## \n## Model 1: ABUND ~ GRAZE + ALT\n## Model 2: ABUND ~ GRAZE\n##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n## 1     53 3297.1                           \n## 2     54 3385.6 -1   -88.519 1.4229 0.2382\nsummary(lm.6)\n## \n## Call:\n## lm(formula = ABUND ~ GRAZE, data = loyn)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -19.1066  -5.4097   0.0934   4.4856  18.2747 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  34.3692     2.4095  14.264  &lt; 2e-16 ***\n## GRAZE        -4.9813     0.7259  -6.862  6.9e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.918 on 54 degrees of freedom\n## Multiple R-squared:  0.4658, Adjusted R-squared:  0.4559 \n## F-statistic: 47.09 on 1 and 54 DF,  p-value: 6.897e-09\n\npar(mfrow = c(2, 2))\nplot(lm.6)\n\n\n\n\n-&gt; das minimal adäquate Modell enthält nur noch einen Prädiktor (GRAZE) und dessen Residualplots sehen ok aus.\n\n\nHierarchical partitioning\nWir können auch schauen wie bedeutsam die einzelnen Variablen sind:\n\n\nif (!require(hier.part)) {\n    install.packages(\"hier.part\")\n}\nlibrary(hier.part)\n## Error in library(hier.part): there is no package called 'hier.part'\nloyn.preds &lt;- with(loyn, data.frame(YR.ISOL, AREA, DIST, LDIST, GRAZE, ALT))\n\npar(mfrow = c(1, 1))\nhier.part(loyn$ABUND, loyn.preds, gof = \"Rsqu\")\n## Error in hier.part(loyn$ABUND, loyn.preds, gof = \"Rsqu\"): could not find function \"hier.part\"\n\n-&gt; auch hier sticht GRAZE heraus. (und an zweiter Stelle YR.ISOL, der mit GRAZE am stärksten korreliert ist)\n\n\nPartial regressions\n\n\navPlots(lm.1, ask = F)"
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#multimodel-inference",
    "href": "stat1-4/Statistik3_Demo.html#multimodel-inference",
    "title": "Stat3: Demo",
    "section": "Multimodel inference",
    "text": "Multimodel inference\n\nif (!require(MuMIn)) {\n    install.packages(\"MuMIn\")\n}\nlibrary(MuMIn)\n\nglobal.model &lt;- lm(ABUND ~ YR.ISOL + AREA + DIST + LDIST + GRAZE + ALT, data = loyn)\n\noptions(na.action = \"na.fail\")\n\nallmodels &lt;- dredge(global.model)\nallmodels\n## Global model call: lm(formula = ABUND ~ YR.ISOL + AREA + DIST + LDIST + GRAZE + \n##     ALT, data = loyn)\n## ---\n## Model selection table \n##        (Int)     ALT        ARE      DIS    GRA       LDI  YR.ISO df   logLik\n## 9    34.3700                             -4.981                    3 -194.315\n## 10   28.5600 0.03191                     -4.597                    4 -193.573\n## 41  -62.7500                             -4.440           0.04898  4 -193.886\n## 26   25.2900 0.04356                     -4.431 0.0014550          5 -192.834\n## 25   33.7400                             -4.967 0.0007979          4 -194.071\n## 13   33.2300                    0.003224 -4.858                    4 -194.102\n## 11   33.9000          1.969e-03          -4.871                    4 -194.201\n## 14   25.6300 0.03834            0.004970 -4.330                    5 -193.081\n## 42  -73.5800 0.03285                     -4.017           0.05143  5 -193.087\n## 58  -99.5900 0.04649                     -3.700 0.0016770 0.06263  6 -192.109\n## 57  -74.9700                             -4.359 0.0009527 0.05477  5 -193.538\n## 12   28.6100 0.03094  5.055e-04          -4.580                    5 -193.566\n## 43  -85.8200          3.243e-03          -4.133           0.06023  5 -193.596\n## 45  -68.9900                    0.003542 -4.277           0.05150  5 -193.626\n## 46  -85.3900 0.03990            0.005386 -3.679           0.05577  6 -192.503\n## 30   23.6500 0.04645            0.003635 -4.262 0.0012290          6 -192.583\n## 27   33.3100          1.892e-03          -4.861 0.0007836          5 -193.965\n## 29   33.0400                    0.002360 -4.880 0.0006227          5 -193.968\n## 15   32.8100          1.886e-03 0.003153 -4.755                    5 -193.997\n## 28   25.2400 0.04416 -2.645e-04          -4.438 0.0014660          6 -192.832\n## 44  -85.1900 0.02954  1.785e-03          -3.891           0.05737  6 -193.006\n## 16   25.6300 0.03828  2.748e-05 0.004967 -4.329                    6 -193.081\n## 62 -104.4000 0.04964            0.003884 -3.500 0.0014400 0.06418  7 -191.816\n## 59  -98.3600          3.274e-03          -4.049 0.0009602 0.06617  6 -193.239\n## 47  -91.6700          3.200e-03 0.003490 -3.977           0.06256  6 -193.341\n## 60 -106.3000 0.04414  1.117e-03          -3.627 0.0016430 0.06612  7 -192.076\n## 61  -77.0200                    0.002499 -4.260 0.0007691 0.05543  6 -193.420\n## 48  -93.8000 0.03722  1.342e-03 0.005240 -3.593           0.06013  7 -192.456\n## 31   32.6300          1.848e-03 0.002304 -4.779 0.0006130          6 -193.867\n## 32   23.5300 0.04762 -5.038e-04 0.003682 -4.274 0.0012470          7 -192.576\n## 63 -100.1000          3.239e-03 0.002430 -3.956 0.0007817 0.06669  7 -193.127\n## 64 -109.7000 0.04772  8.866e-04 0.003811 -3.447 0.0014180 0.06693  8 -191.795\n## 38 -325.3000 0.07807            0.011030                  0.16960  5 -198.542\n## 50 -355.4000 0.08742                            0.0027220 0.18470  5 -198.549\n## 54 -336.0000 0.08950            0.008464        0.0020850 0.17380  6 -197.343\n## 52 -363.9000 0.07191  5.500e-03                 0.0024540 0.19020  6 -197.858\n## 40 -336.7000 0.06363  5.431e-03 0.009912                  0.17650  6 -197.870\n## 56 -344.8000 0.07638  4.590e-03 0.007723        0.0019170 0.17930  7 -196.852\n## 34 -348.5000 0.07006                                      0.18350  4 -200.670\n## 36 -360.2000 0.05243  7.028e-03                           0.19060  5 -199.584\n## 35 -393.4000          1.036e-02                           0.21140  4 -201.103\n## 39 -380.7000          9.678e-03 0.007598                  0.20400  5 -200.121\n## 51 -402.6000          1.019e-02                 0.0014210 0.21560  5 -200.496\n## 55 -389.0000          9.682e-03 0.006312        0.0009292 0.20800  6 -199.883\n## 37 -377.6000                    0.008890                  0.20260  4 -202.444\n## 33 -392.3000                                              0.21120  3 -203.690\n## 49 -402.3000                                    0.0015230 0.21580  4 -203.054\n## 53 -385.8000                    0.007611        0.0009246 0.20660  5 -202.227\n## 6     1.1570 0.10280            0.013820                           4 -204.646\n## 22   -1.1380 0.11310            0.011680        0.0017790          5 -203.948\n## 8     2.2060 0.09511  3.112e-03 0.013240                           5 -204.467\n## 18    1.1530 0.11220                            0.0026530          4 -205.786\n## 2     5.5980 0.09515                                               3 -207.358\n## 24   -0.2323 0.10680  2.339e-03 0.011360        0.0016890          6 -203.846\n## 20    2.4160 0.10280  3.509e-03                 0.0024810          5 -205.568\n## 4     6.9990 0.08318  5.050e-03                                    4 -206.917\n## 7    16.3800          9.404e-03 0.010330                           4 -208.625\n## 3    18.8000          1.033e-02                                    3 -209.974\n## 5    16.7300                    0.011570                           3 -210.265\n## 1    19.5100                                                       2 -211.871\n## 19   18.1300          1.022e-02                 0.0009186          4 -209.789\n## 23   16.3100          9.404e-03 0.010120        0.0001591          5 -208.620\n## 21   16.6700                    0.011360        0.0001598          4 -210.260\n## 17   18.7700                                    0.0010210          3 -211.658\n##     AICc delta weight\n## 9  395.1  0.00  0.145\n## 10 395.9  0.84  0.095\n## 41 396.6  1.46  0.070\n## 26 396.9  1.78  0.059\n## 25 396.9  1.84  0.058\n## 13 397.0  1.90  0.056\n## 11 397.2  2.10  0.051\n## 14 397.4  2.27  0.046\n## 42 397.4  2.28  0.046\n## 58 397.9  2.84  0.035\n## 57 398.3  3.19  0.029\n## 12 398.3  3.24  0.029\n## 43 398.4  3.30  0.028\n## 45 398.5  3.36  0.027\n## 46 398.7  3.63  0.024\n## 30 398.9  3.79  0.022\n## 27 399.1  4.04  0.019\n## 29 399.1  4.04  0.019\n## 15 399.2  4.10  0.019\n## 28 399.4  4.29  0.017\n## 44 399.7  4.63  0.014\n## 16 399.9  4.79  0.013\n## 62 400.0  4.87  0.013\n## 59 400.2  5.10  0.011\n## 47 400.4  5.31  0.010\n## 60 400.5  5.40  0.010\n## 61 400.6  5.46  0.009\n## 48 401.2  6.15  0.007\n## 31 401.4  6.36  0.006\n## 32 401.5  6.39  0.006\n## 63 402.6  7.50  0.003\n## 64 402.7  7.56  0.003\n## 38 408.3 13.19  0.000\n## 50 408.3 13.21  0.000\n## 54 408.4 13.31  0.000\n## 52 409.4 14.34  0.000\n## 40 409.5 14.36  0.000\n## 56 410.0 14.95  0.000\n## 34 410.1 15.03  0.000\n## 36 410.4 15.28  0.000\n## 35 411.0 15.90  0.000\n## 39 411.4 16.35  0.000\n## 51 412.2 17.10  0.000\n## 55 413.5 18.39  0.000\n## 37 413.7 18.58  0.000\n## 33 413.8 18.75  0.000\n## 49 414.9 19.80  0.000\n## 53 415.7 20.56  0.000\n## 6  418.1 22.99  0.000\n## 22 419.1 24.01  0.000\n## 8  420.1 25.04  0.000\n## 18 420.4 25.27  0.000\n## 2  421.2 26.09  0.000\n## 24 421.4 26.32  0.000\n## 20 422.3 27.25  0.000\n## 4  422.6 27.53  0.000\n## 7  426.0 30.94  0.000\n## 3  426.4 31.32  0.000\n## 5  427.0 31.90  0.000\n## 1  428.0 32.88  0.000\n## 19 428.4 33.27  0.000\n## 23 428.4 33.35  0.000\n## 21 429.3 34.21  0.000\n## 17 429.8 34.69  0.000\n## Models ranked by AICc(x)\n\n# Variable importance\nsw(allmodels)\n##                      GRAZE ALT  YR.ISOL LDIST DIST AREA\n## Sum of weights:      1.00  0.44 0.34    0.32  0.28 0.25\n## N containing models:   32    32   32      32    32   32\n\n-&gt; Auch mit dieser Sichtweise ist GRAZE der wichtigste Prädiktor\n\navgmodel &lt;- model.avg(allmodels, subset = TRUE)\nsummary(avgmodel)\n## \n## Call:\n## model.avg(object = allmodels, subset = TRUE)\n## \n## Component model call: \n## lm(formula = ABUND ~ &lt;64 unique rhs&gt;, data = loyn)\n## \n## Component models: \n##        df  logLik   AICc delta weight\n## 4       3 -194.31 395.09  0.00   0.14\n## 14      4 -193.57 395.93  0.84   0.10\n## 46      4 -193.89 396.56  1.46   0.07\n## 145     5 -192.83 396.87  1.78   0.06\n## 45      4 -194.07 396.93  1.84   0.06\n## 34      4 -194.10 396.99  1.90   0.06\n## 24      4 -194.20 397.19  2.10   0.05\n## 134     5 -193.08 397.36  2.27   0.05\n## 146     5 -193.09 397.37  2.28   0.05\n## 1456    6 -192.11 397.93  2.84   0.03\n## 456     5 -193.54 398.28  3.19   0.03\n## 124     5 -193.57 398.33  3.24   0.03\n## 246     5 -193.60 398.39  3.30   0.03\n## 346     5 -193.63 398.45  3.36   0.03\n## 1346    6 -192.50 398.72  3.63   0.02\n## 1345    6 -192.58 398.88  3.79   0.02\n## 245     5 -193.97 399.13  4.04   0.02\n## 345     5 -193.97 399.14  4.04   0.02\n## 234     5 -194.00 399.19  4.10   0.02\n## 1245    6 -192.83 399.38  4.29   0.02\n## 1246    6 -193.01 399.73  4.63   0.01\n## 1234    6 -193.08 399.88  4.79   0.01\n## 13456   7 -191.82 399.96  4.87   0.01\n## 2456    6 -193.24 400.19  5.10   0.01\n## 2346    6 -193.34 400.40  5.31   0.01\n## 12456   7 -192.08 400.49  5.40   0.01\n## 3456    6 -193.42 400.55  5.46   0.01\n## 12346   7 -192.46 401.25  6.15   0.01\n## 2345    6 -193.87 401.45  6.36   0.01\n## 12345   7 -192.58 401.49  6.39   0.01\n## 23456   7 -193.13 402.59  7.50   0.00\n## 123456  8 -191.79 402.65  7.56   0.00\n## 136     5 -198.54 408.28 13.19   0.00\n## 156     5 -198.55 408.30 13.21   0.00\n## 1356    6 -197.34 408.40 13.31   0.00\n## 1256    6 -197.86 409.43 14.34   0.00\n## 1236    6 -197.87 409.45 14.36   0.00\n## 12356   7 -196.85 410.04 14.95   0.00\n## 16      4 -200.67 410.13 15.03   0.00\n## 126     5 -199.58 410.37 15.28   0.00\n## 26      4 -201.10 410.99 15.90   0.00\n## 236     5 -200.12 411.44 16.35   0.00\n## 256     5 -200.50 412.19 17.10   0.00\n## 2356    6 -199.88 413.48 18.39   0.00\n## 36      4 -202.44 413.67 18.58   0.00\n## 6       3 -203.69 413.84 18.75   0.00\n## 56      4 -203.05 414.89 19.80   0.00\n## 356     5 -202.23 415.65 20.56   0.00\n## 13      4 -204.65 418.08 22.99   0.00\n## 135     5 -203.95 419.10 24.01   0.00\n## 123     5 -204.47 420.13 25.04   0.00\n## 15      4 -205.79 420.36 25.27   0.00\n## 1       3 -207.36 421.18 26.09   0.00\n## 1235    6 -203.85 421.41 26.32   0.00\n## 125     5 -205.57 422.34 27.25   0.00\n## 12      4 -206.92 422.62 27.53   0.00\n## 23      4 -208.63 426.04 30.94   0.00\n## 2       3 -209.97 426.41 31.32   0.00\n## 3       3 -210.27 426.99 31.90   0.00\n## (Null)  2 -211.87 427.97 32.88   0.00\n## 25      4 -209.79 428.36 33.27   0.00\n## 235     5 -208.62 428.44 33.35   0.00\n## 35      4 -210.26 429.30 34.21   0.00\n## 5       3 -211.66 429.78 34.69   0.00\n## \n## Term codes: \n##     ALT    AREA    DIST   GRAZE   LDIST YR.ISOL \n##       1       2       3       4       5       6 \n## \n## Model-averaged coefficients:  \n## (full average) \n##               Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept) -7.5601112 84.0460963  85.2129010   0.089    0.929    \n## GRAZE       -4.4923671  0.9653718   0.9836120   4.567  4.9e-06 ***\n## ALT          0.0168833  0.0269646   0.0272737   0.619    0.536    \n## YR.ISOL      0.0192028  0.0420833   0.0426707   0.450    0.653    \n## LDIST        0.0003700  0.0009029   0.0009158   0.404    0.686    \n## DIST         0.0010880  0.0033129   0.0033689   0.323    0.747    \n## AREA         0.0004120  0.0023720   0.0024205   0.170    0.865    \n##  \n## (conditional average) \n##              Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept) -7.560111  84.046096   85.212901   0.089    0.929    \n## GRAZE       -4.497801   0.953221    0.971711   4.629  3.7e-06 ***\n## ALT          0.038392   0.028768    0.029424   1.305    0.192    \n## YR.ISOL      0.056407   0.055710    0.057008   0.989    0.322    \n## LDIST        0.001152   0.001280    0.001308   0.881    0.378    \n## DIST         0.003833   0.005305    0.005428   0.706    0.480    \n## AREA         0.001673   0.004554    0.004656   0.359    0.719    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "stat1-4/Statistik3_Uebung.html",
    "href": "stat1-4/Statistik3_Uebung.html",
    "title": "Stat3: Übung",
    "section": "",
    "text": "Datensatz Ukraine_bearbeitet.csv\n\n\nÜbung 3: Multiple Regression\n\nBereiten Sie den Datensatz Ukraine_bearbeitet.csv für das Einlesen in R vor und lesen Sie ihn dann ein. Dieser enthält Pflanzenartenzahlen (Species.richness) von 199 10 m² grossen Plots (Vegetationsaufnahmen) von Steppenrasen in der Ukraine sowie zahlreiche Umweltvariablen, deren Bedeutung und Einheiten im Kopf der ExcelTabelle angegeben sind.\nErmitteln Sie ein minimal adäquates Modell, das den Artenreichtum in den Plots durch die Umweltvariablen erklärt.\nBitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen: welches sind die abhängige(n) und welches die unabängige(n) Variablen, sind alle Variablen für die Analyse geeignet?\nExplorative Datenanalyse, um zu sehen, ob die abhängige Variable in der vorliegenden Form für die Analyse geeignet ist\nDefinition eines globalen Modelles und dessen Reduktion zu einem minimal adäquaten Modell\nDurchführen der Modelldiagnostik für dieses\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\nFormulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit)."
  },
  {
    "objectID": "stat1-4/Statistik3_Loesung.html#lösung-übung-3.1",
    "href": "stat1-4/Statistik3_Loesung.html#lösung-übung-3.1",
    "title": "Stat3: Lösung",
    "section": "Lösung Übung 3.1",
    "text": "Lösung Übung 3.1\nSchon vor dem Einlesen kürzt man am besten bereits in Excel die Variablennamen so ab, dass sie noch eindeutig, aber nicht unnötig lang sind, etwa indem man die Einheiten wegstreicht\n\n# Aus der Excel-Tabelle wurde das relevante Arbeitsblatt als csv gespeichert\nukraine &lt;- read.delim(\"datasets/statistik/Ukraine_bearbeitet.csv\", sep = \",\")\n\n\nukraine\n\n\nstr(ukraine)\n## 'data.frame':    199 obs. of  24 variables:\n##  $ X                : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ PlotID           : chr  \"UA01NW\" \"UA01SE\" \"UA02NW\" \"UA02SE\" ...\n##  $ Species_richness : int  44 53 48 50 53 40 46 56 30 35 ...\n##  $ Altitude         : int  179 178 188 183 162 165 153 158 192 197 ...\n##  $ Inclination      : int  24 17 27 33 7 33 30 32 25 18 ...\n##  $ Heat_index       : num  -0.42 -0.3 -0.51 -0.65 -0.09 -0.42 0 -0.59 0.46 0.32 ...\n##  $ Microrelief      : num  5 2.5 2 2 3 4 16 15 5 3 ...\n##  $ Grazing_intensity: int  0 0 0 0 0 0 1 1 0 0 ...\n##  $ Litter           : int  12 10 0 4 15 30 5 6 10 20 ...\n##  $ Stones_and_rocks : num  0 0 0 0 0 0 40 10 0 0 ...\n##  $ Gravel           : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Fine_soil        : num  2 5 0 7 0 0 2 5 5 2 ...\n##  $ Sand             : int  45 30 20 20 55 30 10 30 10 5 ...\n##  $ Silt             : int  40 35 60 60 10 35 60 35 60 90 ...\n##  $ Clay             : int  15 35 20 20 35 35 30 35 30 5 ...\n##  $ pH               : num  7.32 6.91 6.72 6.44 6.1 6.23 6.79 6.43 7.19 7 ...\n##  $ Conductivity     : int  90 115 126 90 73 76 163 119 151 69 ...\n##  $ CaCO3            : num  0.0754 0.1271 0.0723 0.0771 0.0829 ...\n##  $ N_total          : num  0.14 0.17 0.24 0.26 0.29 0.2 0.34 0.29 0.18 0.2 ...\n##  $ C_org            : num  1.54 1.97 2.99 3.22 3.77 2.5 4.59 3.67 2.16 2.38 ...\n##  $ CN_ratio         : num  10.8 11.5 12.5 12.6 12.8 ...\n##  $ Temperature      : int  79 79 80 80 82 82 82 82 79 83 ...\n##  $ Temperature_range: int  330 330 329 329 328 328 328 328 326 327 ...\n##  $ Precipitation    : int  608 608 603 603 594 594 594 594 600 586 ...\nsummary(ukraine)\n##        X            PlotID          Species_richness    Altitude    \n##  Min.   :  1.0   Length:199         Min.   :14.00    Min.   : 73.0  \n##  1st Qu.: 50.5   Class :character   1st Qu.:34.00    1st Qu.:140.0  \n##  Median :100.0   Mode  :character   Median :40.00    Median :166.0  \n##  Mean   :100.0                      Mean   :40.23    Mean   :161.7  \n##  3rd Qu.:149.5                      3rd Qu.:47.50    3rd Qu.:188.0  \n##  Max.   :199.0                      Max.   :67.00    Max.   :251.0  \n##                                                                     \n##   Inclination      Heat_index        Microrelief      Grazing_intensity\n##  Min.   : 1.00   Min.   :-0.94000   Min.   :  0.000   Min.   :0.0000   \n##  1st Qu.:12.00   1st Qu.:-0.15500   1st Qu.:  2.500   1st Qu.:0.0000   \n##  Median :19.00   Median : 0.01000   Median :  5.000   Median :1.0000   \n##  Mean   :19.28   Mean   : 0.01603   Mean   :  7.126   Mean   :0.9296   \n##  3rd Qu.:25.00   3rd Qu.: 0.21500   3rd Qu.:  7.000   3rd Qu.:2.0000   \n##  Max.   :48.00   Max.   : 0.85000   Max.   :100.000   Max.   :3.0000   \n##                                                                        \n##      Litter      Stones_and_rocks     Gravel         Fine_soil    \n##  Min.   : 0.00   Min.   : 0.000   Min.   : 0.000   Min.   : 0.00  \n##  1st Qu.: 3.50   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 2.00  \n##  Median : 7.00   Median : 0.500   Median : 0.000   Median : 5.00  \n##  Mean   :12.16   Mean   : 3.994   Mean   : 2.984   Mean   : 7.02  \n##  3rd Qu.:13.50   3rd Qu.: 4.000   3rd Qu.: 3.000   3rd Qu.:10.00  \n##  Max.   :90.00   Max.   :68.000   Max.   :40.000   Max.   :38.00  \n##                                                                   \n##       Sand            Silt            Clay             pH       \n##  Min.   : 5.00   Min.   : 5.00   Min.   : 5.00   Min.   :4.890  \n##  1st Qu.:20.00   1st Qu.:20.00   1st Qu.:20.00   1st Qu.:7.240  \n##  Median :30.00   Median :35.00   Median :20.00   Median :7.420  \n##  Mean   :35.81   Mean   :40.43   Mean   :23.74   Mean   :7.286  \n##  3rd Qu.:55.00   3rd Qu.:60.00   3rd Qu.:35.00   3rd Qu.:7.545  \n##  Max.   :80.00   Max.   :90.00   Max.   :55.00   Max.   :7.790  \n##  NA's   :1       NA's   :1       NA's   :1                      \n##   Conductivity       CaCO3            N_total           C_org       \n##  Min.   : 40.0   Min.   : 0.0042   Min.   :0.0700   Min.   : 1.040  \n##  1st Qu.:148.5   1st Qu.: 0.4306   1st Qu.:0.2000   1st Qu.: 2.850  \n##  Median :171.0   Median : 4.6578   Median :0.2700   Median : 3.560  \n##  Mean   :162.3   Mean   : 7.4757   Mean   :0.2788   Mean   : 3.689  \n##  3rd Qu.:189.5   3rd Qu.:13.0002   3rd Qu.:0.3300   3rd Qu.: 4.400  \n##  Max.   :232.0   Max.   :35.2992   Max.   :0.9500   Max.   :11.300  \n##                                                                     \n##     CN_ratio      Temperature    Temperature_range Precipitation  \n##  Min.   : 6.04   Min.   :78.00   Min.   :326.0     Min.   :577.0  \n##  1st Qu.:12.24   1st Qu.:82.00   1st Qu.:328.0     1st Qu.:583.0  \n##  Median :12.95   Median :84.00   Median :329.0     Median :592.0  \n##  Mean   :13.48   Mean   :84.82   Mean   :328.6     Mean   :596.4  \n##  3rd Qu.:14.02   3rd Qu.:88.00   3rd Qu.:330.0     3rd Qu.:602.5  \n##  Max.   :27.42   Max.   :92.00   Max.   :331.0     Max.   :630.0  \n## \n\nMan erkennt, dass alle Spalten bis auf die erste mit der Plot ID numerisch (num oder int) und dass die abhängige Variable in Spalte 2 sowie die Prediktorvariablen in den Spalten 3 bis 23 stehen.\n\n# Explorative Datenanalyse der abhängigen Variablen\nboxplot(ukraine$Species_richness)\n\n\n\n\nDer Boxplot sieht sehr gut symmetrisch aus. Insofern gibt es keinen Anlass über eine Transformation nachzudenken. (Da es sich bei Artenzahlen um Zähldaten handelt, müsste man theoretisch ein glm mit Poisson-Verteilung rechnen; bei einem Mittelwert, der hinreichend von Null verschieden ist (hier: ca. 40), ist eine Poisson-Verteilung aber praktisch nicht von einer Normalverteilung zu unterscheiden und wir können uns den Aufwand auch sparen).\n\ncor &lt;- cor(ukraine[, 3:23])\ncor\ncor[abs(cor) &lt; 0.7] &lt;- 0\ncor\n\nDie Korrelationsanalyse dient dazu, zu entscheiden, ob die Prädiktorvariablen hinreichend voneinander unabhängig sind, um alle in das globale Modell hinein zu nehmen. Bei Pearson’s Korrelationskoeffizienten r, die betragsmässig grösser als 0.7 sind, würde es problematisch. Alternativ hätten wir auch den VIF (Variance Inflation Factor) als Kriterium für den möglichen Ausschluss von Variablen aus dem globalen Modell nehmen können. Diese initiale Korrelationsanalyse zeigt uns aber, dass unsere Daten noch ein anderes Problem haben: für die drei Korngrössenfraktionen des Bodens (Sand, Silt, Clay) stehen lauter NA’s. Um herauszufinden, was das Problem ist, geben wir ein:\n\nsummary(ukraine$Sand)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    5.00   20.00   30.00   35.81   55.00   80.00       1\nukraine[!complete.cases(ukraine), ]  # Zeigt zeilen mit NAs ein\n##     X PlotID Species_richness Altitude Inclination Heat_index Microrelief\n## 85 85 UAR061               23      159          48        0.1         100\n##    Grazing_intensity Litter Stones_and_rocks Gravel Fine_soil Sand Silt Clay\n## 85                 0      1               68      0         1   NA   NA   NA\n##      pH Conductivity   CaCO3 N_total C_org CN_ratio Temperature\n## 85 7.53          203 10.9638    0.95  11.3    11.86          82\n##    Temperature_range Precipitation\n## 85               327           599\n\nDa gibt es offensichtlich je ein NA in jeder dieser Zeilen. Jetzt können wir entscheiden, entweder auf die drei Variablen oder auf die eine Beobachtung zu verzichten. Da wir eh schon eher mehr unabhängige Variablen haben als wir händeln können, entscheide ich pragmatisch für ersteres. Wir rechnen die Korrelation also noch einmal ohne diese drei Spalten (es sind die Nummern 12:14, wie wir aus der anfänglichen Variablenbetrachtung oben wissen).\n\ncor &lt;- cor(ukraine[, c(3:11, 15:23)])\ncor[abs(cor) &lt; 0.7] &lt;- 0\ncor\n\nWenn man auf cor nun doppel-clickt und es in einem separaten Fenster öffnet, sieht man, wo es problematische Korrelationen zwischen Variablenpaaren gibt. Es sind dies Altitude vs. Temperature und N.total vs. C.org. Wir müssen aus jedem dieser Paare jetzt eine Variable rauswerfen, am besten jene, die weniger gut interpretierbar ist. Ich entscheide mich dafür Temperature statt Altitude (weil das der direktere ökologische Wirkfaktor ist) und C.org statt N.total zu behalten (weil es in der Literatur mehr Daten zum Humusgehalt als zum N-Gehalt gibt, damit eine bessere Vergleichbarkeit erzielt wird). Die Aussagen, die wir für die beibehaltene Variable erzielen, stehen aber +/- auch für die entfernte. Das Problem ist aber, dass wir immer noch 16 Variablen haben, was einen sehr leistungsfähigen Rechner oder sehr lange Rechenzeit erfordern würde. Wir sollten also unter 15 Variablen kommen. Wir könnten uns jetzt überlegen, welche uns ökologisch am wichtigsten sind, oder ein noch strengeres Kriterium bei r verwenden, etwa 0.6\n\ncor &lt;- cor(ukraine[, c(3:11, 15:23)])\ncor[abs(cor) &lt; 0.6] &lt;- 0\ncor\n##                   Species_richness   Altitude Inclination Heat_index\n## Species_richness                 1  0.0000000           0          0\n## Altitude                         0  1.0000000           0          0\n## Inclination                      0  0.0000000           1          0\n## Heat_index                       0  0.0000000           0          1\n## Microrelief                      0  0.0000000           0          0\n## Grazing_intensity                0  0.0000000           0          0\n## Litter                           0  0.0000000           0          0\n## Stones_and_rocks                 0  0.0000000           0          0\n## Gravel                           0  0.0000000           0          0\n## Clay                            NA         NA          NA         NA\n## pH                               0  0.0000000           0          0\n## Conductivity                     0  0.0000000           0          0\n## CaCO3                            0  0.0000000           0          0\n## N_total                          0  0.0000000           0          0\n## C_org                            0  0.0000000           0          0\n## CN_ratio                         0  0.0000000           0          0\n## Temperature                      0 -0.8309559           0          0\n## Temperature_range                0 -0.6794514           0          0\n##                   Microrelief Grazing_intensity Litter Stones_and_rocks Gravel\n## Species_richness            0                 0      0                0      0\n## Altitude                    0                 0      0                0      0\n## Inclination                 0                 0      0                0      0\n## Heat_index                  0                 0      0                0      0\n## Microrelief                 1                 0      0                0      0\n## Grazing_intensity           0                 1      0                0      0\n## Litter                      0                 0      1                0      0\n## Stones_and_rocks            0                 0      0                1      0\n## Gravel                      0                 0      0                0      1\n## Clay                       NA                NA     NA               NA     NA\n## pH                          0                 0      0                0      0\n## Conductivity                0                 0      0                0      0\n## CaCO3                       0                 0      0                0      0\n## N_total                     0                 0      0                0      0\n## C_org                       0                 0      0                0      0\n## CN_ratio                    0                 0      0                0      0\n## Temperature                 0                 0      0                0      0\n## Temperature_range           0                 0      0                0      0\n##                   Clay       pH Conductivity CaCO3   N_total     C_org CN_ratio\n## Species_richness    NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n## Altitude            NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n## Inclination         NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n## Heat_index          NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n## Microrelief         NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n## Grazing_intensity   NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n## Litter              NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n## Stones_and_rocks    NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n## Gravel              NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n## Clay                 1       NA           NA    NA        NA        NA       NA\n## pH                  NA 1.000000     0.674678     0 0.0000000 0.0000000        0\n## Conductivity        NA 0.674678     1.000000     0 0.0000000 0.0000000        0\n## CaCO3               NA 0.000000     0.000000     1 0.0000000 0.0000000        0\n## N_total             NA 0.000000     0.000000     0 1.0000000 0.9551133        0\n## C_org               NA 0.000000     0.000000     0 0.9551133 1.0000000        0\n## CN_ratio            NA 0.000000     0.000000     0 0.0000000 0.0000000        1\n## Temperature         NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n## Temperature_range   NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n##                   Temperature Temperature_range\n## Species_richness    0.0000000         0.0000000\n## Altitude           -0.8309559        -0.6794514\n## Inclination         0.0000000         0.0000000\n## Heat_index          0.0000000         0.0000000\n## Microrelief         0.0000000         0.0000000\n## Grazing_intensity   0.0000000         0.0000000\n## Litter              0.0000000         0.0000000\n## Stones_and_rocks    0.0000000         0.0000000\n## Gravel              0.0000000         0.0000000\n## Clay                       NA                NA\n## pH                  0.0000000         0.0000000\n## Conductivity        0.0000000         0.0000000\n## CaCO3               0.0000000         0.0000000\n## N_total             0.0000000         0.0000000\n## C_org               0.0000000         0.0000000\n## CN_ratio            0.0000000         0.0000000\n## Temperature         1.0000000         0.6900784\n## Temperature_range   0.6900784         1.0000000\n\nEntsprechend „werfen“ wir auch noch die folgenden Variablen „raus“: Temperature.range (positiv mit Temperature), Precipitation (negativ mit Temperature) sowie Conductivity (positiv mit pH).\nNun können wir das globale Modell definieren, indem wir alle verbleibenden Variablen aufnehmen, das sind 13. (Wenn das nicht eh schon so viele wären, dass es uns an die Grenze der Rechenleistung bringt, hätten wir auch noch darüber nachdenken können, einzelne quadratische Terme oder Interaktionsterme zu berücksichtigen).\n\nglobal.model &lt;- lm(Species_richness ~ Inclination + Heat_index + Microrelief + Grazing_intensity +\n    Litter + Stones_and_rocks + Gravel + Fine_soil + pH + CaCO3 + C_org + CN_ratio +\n    Temperature, data = ukraine)\n\nNun gibt es im Prinzip zwei Möglichkeiten, vom globalen (vollen) Modell zu einem minimal adäquaten Modell zu kommen. (1) Der Ansatz der „frequentist statistic“, in dem man aus dem vollen Modell so lange schrittweise Variablen entfernt, bis nur noch signifikante Variablen verbleiben. (2) Den informationstheoretischen Ansatz, bei dem alle denkbaren Modelle berechnet und verglichen werden (also alle möglichen Kombinationen von 13,12,…, 1, 0 Parametern). Diese Lösung stelle ich im Folgenden vor:\n\n# Multimodel inference\nif (!require(MuMIn)) {\n    install.packages(\"MuMIn\")\n}\nlibrary(MuMIn)\n\noptions(na.action = \"na.fail\")\nallmodels &lt;- dredge(global.model)\n\n\nallmodels\n\nJetzt bekommen wir die besten der insgesamt 8192 möglichen Modelle gelistet mit ihren Parameterschätzungen und ihrem AICc.\nDas beste Modell umfasst 5 Parameter (CaCO3, CN.ratio, Grazing.intensity. Heat.index, Litter). Allerdings ist das nächstbeste Modell (mit 6 Parametern) nur wenig schlechter (delta AICc = 0.71), was sich in fast gleichen (und zudem sehr niedrigen) Akaike weights bemerkbar macht. Nach dem Verständnis des Information theoretician approach, sollte man in einer solchen Situation nicht das eine „beste“ Modell benennen, sondern eine Aussage über die Gruppe der insgesamt brauchbaren Modelle treffen. Hierzu kann man (a) Importance der Parameter über alle Modelle hinweg berechnen (= Summe der Akaike weights aller Modelle, die den betreffenden Parameter enthalten) und/oder (b) ein nach Akaike weights gemitteltes Modell berechnen.\n\n# Importance values der Variablen\nsw(allmodels)\n##                      Heat_index Litter CaCO3 CN_ratio Grazing_intensity\n## Sum of weights:      1.00       0.92   0.82  0.73     0.68             \n## N containing models: 4096       4096   4096  4096     4096             \n##                      Stones_and_rocks Temperature Microrelief Gravel Fine_soil\n## Sum of weights:      0.43             0.39        0.33        0.31   0.31     \n## N containing models: 4096             4096        4096        4096   4096     \n##                      C_org pH   Inclination\n## Sum of weights:      0.30  0.26 0.26       \n## N containing models: 4096  4096 4096\n\nDemnach ist Heat.index die wichtigste Variable (in 100% aller relevanten Modelle), während ferner Litter, CaCO3, CN_ratio und Grazing_intensity in mehr als 50% der relevanten Modelle enthalten sind.\n\n# Modelaveraging (Achtung: dauert mit 13 Variablen einige Minuten)\nsummary(model.avg(allmodels, rank = \"AICc\"), subset = TRUE)\n\nAus dem gemittelten Modell können wir die Richtung der Beziehung (positiv oder negativ) und ggf. die Effektgrössen (wie verändert sich die Artenzahl, wenn die Prädiktorvariable um eine Einheit zunimmt?) ermitteln.\n\n# Modelldiagnostik nicht vergessen\npar(mfrow = c(2, 2))\nplot(global.model)\n\n\n\nplot(lm(Species_richness ~ Heat_index + Litter + CaCO3 + CN_ratio + Grazing_intensity,\n    data = ukraine))\n\n\n\n\nsummary(global.model)\n## \n## Call:\n## lm(formula = Species_richness ~ Inclination + Heat_index + Microrelief + \n##     Grazing_intensity + Litter + Stones_and_rocks + Gravel + \n##     Fine_soil + pH + CaCO3 + C_org + CN_ratio + Temperature, \n##     data = ukraine)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -25.1317  -5.8226   0.5007   5.9982  21.4941 \n## \n## Coefficients:\n##                    Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)        29.35672   17.93590   1.637  0.10338    \n## Inclination         0.01179    0.08581   0.137  0.89084    \n## Heat_index        -12.17483    2.41802  -5.035 1.13e-06 ***\n## Microrelief         0.07488    0.07312   1.024  0.30716    \n## Grazing_intensity   1.23000    0.67730   1.816  0.07098 .  \n## Litter             -0.12338    0.04309  -2.864  0.00467 ** \n## Stones_and_rocks   -0.14803    0.08840  -1.675  0.09570 .  \n## Gravel             -0.03114    0.12924  -0.241  0.80988    \n## Fine_soil          -0.08720    0.10181  -0.856  0.39286    \n## pH                 -0.21774    1.70826  -0.127  0.89871    \n## CaCO3               0.22638    0.10597   2.136  0.03397 *  \n## C_org               0.31994    0.55929   0.572  0.56798    \n## CN_ratio           -0.75167    0.33393  -2.251  0.02556 *  \n## Temperature         0.24527    0.21510   1.140  0.25566    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.149 on 185 degrees of freedom\n## Multiple R-squared:  0.2349, Adjusted R-squared:  0.1812 \n## F-statistic:  4.37 on 13 and 185 DF,  p-value: 2.027e-06\n\nWie immer kommt am Ende die Modelldiagnostik. Wir können uns entweder das globale Modell oder das Modell mit den 5 Variablen mit importance &gt; 50% anschauen. Das Bild sieht fast identisch aus und zeigt keinerlei problematische Abweichungen, d. h. links oben weder ein Keil, noch eine Banane, rechts oben eine nahezu perfekte Gerade."
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#von-lms-zu-glms",
    "href": "stat1-4/Statistik4_Demo.html#von-lms-zu-glms",
    "title": "Stat4: Demo",
    "section": "von LMs zu GLMs",
    "text": "von LMs zu GLMs\n\n# Daten erstellen und anschauen\ntemp &lt;- c(10, 12, 16, 20, 24, 25, 30, 33, 37)\nbesucher &lt;- c(40, 12, 50, 500, 400, 900, 1500, 900, 2000)\nstrand &lt;- data.frame(Temperatur = temp, Besucher = besucher)\n\nplot(besucher ~ temp, data = strand)\n\n\n\n\n\n# Modell definieren und anschauen\nlm.strand &lt;- lm(Besucher ~ Temperatur, data = strand)\nsummary(lm.strand)\n## \n## Call:\n## lm(formula = Besucher ~ Temperatur, data = strand)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -476.41 -176.89   55.59  218.82  353.11 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -855.01     290.54  -2.943 0.021625 *  \n## Temperatur     67.62      11.80   5.732 0.000712 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 311.7 on 7 degrees of freedom\n## Multiple R-squared:  0.8244, Adjusted R-squared:  0.7993 \n## F-statistic: 32.86 on 1 and 7 DF,  p-value: 0.0007115\n\npar(mfrow = c(2, 2))\nplot(lm.strand)\n\n\n\n\n\npar(mfrow = c(1, 1))\nxv &lt;- seq(0, 40, by = 0.1)\nyv &lt;- predict(lm.strand, list(Temperatur = xv))\nplot(strand$Temperatur, strand$Besucher, xlim = c(0, 40))\nlines(xv, yv, lwd = 3, col = \"blue\")\n\n\n\n\n\n# GLMs definieren und anschauen\nglm.gaussian &lt;- glm(Besucher ~ Temperatur, family = gaussian, data = strand)\nglm.poisson &lt;- glm(Besucher ~ Temperatur, family = poisson, data = strand)\n\nsummary(glm.gaussian)\n## \n## Call:\n## glm(formula = Besucher ~ Temperatur, family = gaussian, data = strand)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -476.41  -176.89    55.59   218.82   353.11  \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -855.01     290.54  -2.943 0.021625 *  \n## Temperatur     67.62      11.80   5.732 0.000712 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 97138.03)\n## \n##     Null deviance: 3871444  on 8  degrees of freedom\n## Residual deviance:  679966  on 7  degrees of freedom\n## AIC: 132.63\n## \n## Number of Fisher Scoring iterations: 2\nsummary(glm.poisson)\n## \n## Call:\n## glm(formula = Besucher ~ Temperatur, family = poisson, data = strand)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -13.577  -12.787   -4.491    9.515   15.488  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) 3.500301   0.056920   61.49   &lt;2e-16 ***\n## Temperatur  0.112817   0.001821   61.97   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 6011.8  on 8  degrees of freedom\n## Residual deviance: 1113.7  on 7  degrees of freedom\n## AIC: 1185.1\n## \n## Number of Fisher Scoring iterations: 5\n\nRücktranformation der Werte auf die orginale Skale (Hier Exponentialfunktion da family=possion als Link-Funktion den natürlichen Logarithmus (log) verwendet) Besucher = exp(3.50 + 0.11 Temperatur/°C)\n\n\nexp(3.500301)  # Anzahl besucher bei 0°C\n## [1] 33.12542\nexp(glm.poisson$coefficients[1])  # Werte aus Modell\n## (Intercept) \n##    33.12542\nexp(3.500301 + 30 * 0.112817)  # Anzahl besucher bei 30°C\n## [1] 977.3169\nexp(glm.poisson$coeff[1] * glm.poisson$coeff[2])  #coefficients kann mit coeff abgekürzt werden\n## (Intercept) \n##    1.484225\n\n# Test Overdispersion\nif (!require(AER)) {\n    install.packages(\"AER\")\n}\nlibrary(AER)\ndispersiontest(glm.poisson)\n## \n##  Overdispersion test\n## \n## data:  glm.poisson\n## z = 3.8576, p-value = 5.726e-05\n## alternative hypothesis: true dispersion is greater than 1\n## sample estimates:\n## dispersion \n##   116.5467\n\nglm.quasi &lt;- glm(Besucher ~ Temperatur, family = quasipoisson, data = strand)\nsummary(glm.quasi)\n## \n## Call:\n## glm(formula = Besucher ~ Temperatur, family = quasipoisson, data = strand)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -13.577  -12.787   -4.491    9.515   15.488  \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)  3.50030    0.69639   5.026  0.00152 **\n## Temperatur   0.11282    0.02227   5.065  0.00146 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for quasipoisson family taken to be 149.6826)\n## \n##     Null deviance: 6011.8  on 8  degrees of freedom\n## Residual deviance: 1113.7  on 7  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 5\n\n\npar(mfrow = c(2, 2))\nplot(glm.gaussian, main = \"glm.gaussian\")\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(glm.poisson, main = \"glm.poisson\")\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(glm.quasi, main = \"glm.quasi\")\n\n\n\n\n\npar(mfrow = c(1, 1))\nplot(strand$Temperatur, strand$Besucher, xlim = c(0, 40))\nxv &lt;- seq(0, 40, by = 0.1)\n\nyv &lt;- predict(lm.strand, list(Temperatur = xv))\nlines(xv, yv, lwd = 3, col = \"blue\")\n\nyv2 &lt;- predict(glm.poisson, list(Temperatur = xv))\nlines(xv, exp(yv2), lwd = 3, col = \"red\")\n\nyv3 &lt;- predict(glm.quasi, list(Temperatur = xv))\nlines(xv, exp(yv3), lwd = 3, col = \"green\")"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#logistische-regression",
    "href": "stat1-4/Statistik4_Demo.html#logistische-regression",
    "title": "Stat4: Demo",
    "section": "Logistische Regression",
    "text": "Logistische Regression\n\nbathing &lt;- data.frame(temperature = c(1, 2, 5, 9, 14, 14, 15, 19, 22, 24, 25, 26,\n    27, 28, 29), bathing = c(0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1))\nplot(bathing ~ temperature, data = bathing)\n\n\n\n\nglm.1 &lt;- glm(bathing ~ temperature, family = \"binomial\", data = bathing)\nsummary(glm.1)\n## \n## Call:\n## glm(formula = bathing ~ temperature, family = \"binomial\", data = bathing)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.7408  -0.4723  -0.1057   0.5123   1.8615  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)  \n## (Intercept)  -5.4652     2.8501  -1.918   0.0552 .\n## temperature   0.2805     0.1350   2.077   0.0378 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 20.728  on 14  degrees of freedom\n## Residual deviance: 10.829  on 13  degrees of freedom\n## AIC: 14.829\n## \n## Number of Fisher Scoring iterations: 6\n\n# Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq(glm.1$deviance, glm.1$df.resid)\n## [1] 0.6251679\n\n# Modellgüte (pseudo-R²)\n1 - (glm.1$dev/glm.1$null)\n## [1] 0.4775749\n\n# Steilheit der Beziehung (relative Änderung der odds bei x + 1 vs. x)\nexp(glm.1$coefficients[2])\n## temperature \n##    1.323807\n\n# LD50 (also hier: Temperatur, bei der 50% der Touristen baden)\n-glm.1$coefficients[1]/glm.1$coefficients[2]\n## (Intercept) \n##    19.48311\n\n# Vorhersagen\npredicted &lt;- predict(glm.1, type = \"response\")\n\n# Konfusionsmatrix\nkm &lt;- table(bathing$bathing, predicted &gt; 0.5)\nkm\n##    \n##     FALSE TRUE\n##   0     7    1\n##   1     1    6\n\n# Missklassifizierungsrate\n1 - sum(diag(km)/sum(km))\n## [1] 0.1333333\n\n# Plotting\nxs &lt;- seq(0, 30, l = 1000)\nmodel.predict &lt;- predict(glm.1, type = \"response\", se = T, newdata = data.frame(temperature = xs))\n\nplot(bathing ~ temperature, xlab = \"Temperature (°C)\", ylab = \"% Bathing\", pch = 16,\n    col = \"red\", data = bathing)\npoints(model.predict$fit ~ xs, type = \"l\")\nlines(model.predict$fit + model.predict$se.fit ~ xs, type = \"l\", lty = 2)\nlines(model.predict$fit - model.predict$se.fit ~ xs, type = \"l\", lty = 2)"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#nicht-lineare-regression",
    "href": "stat1-4/Statistik4_Demo.html#nicht-lineare-regression",
    "title": "Stat4: Demo",
    "section": "Nicht-lineare Regression",
    "text": "Nicht-lineare Regression\n\nif (!require(AICcmodavg)) {\n    install.packages(\"AICcmodavg\")\n}\nif (!require(nlstools)) {\n    install.packages(\"nlstools\")\n}\nlibrary(AICcmodavg)\nlibrary(nlstools)\n\nloyn &lt;- read.delim(\"datasets/statistik/loyn.csv\", sep = \",\")  # Verzeichnis muss dort gesetzt sein wo Daten sind\n\n# Selbstdefinierte Funktion, hier Potenzfunktion\npower.model &lt;- nls(ABUND ~ c * AREA^z, start = (list(c = 1, z = 0)), data = loyn)\nsummary(power.model)\n## \n## Formula: ABUND ~ c * AREA^z\n## \n## Parameters:\n##   Estimate Std. Error t value Pr(&gt;|t|)    \n## c 13.39418    1.30721  10.246 2.87e-14 ***\n## z  0.16010    0.02438   6.566 2.09e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.995 on 54 degrees of freedom\n## \n## Number of iterations to convergence: 12 \n## Achieved convergence tolerance: 7.122e-06\nAICc(power.model)\n## [1] 396.1723\n\n# Modeldiagnostik (in nlstools)\nplot(nlsResiduals(power.model))\n\n\n\n\n# Vordefinierte 'Selbststartfunktionen'#\n`?`(selfStart)\nlogistic.model &lt;- nls(ABUND ~ SSlogis(AREA, Asym, xmid, scal), data = loyn)\nsummary(logistic.model)\n## \n## Formula: ABUND ~ SSlogis(AREA, Asym, xmid, scal)\n## \n## Parameters:\n##      Estimate Std. Error t value Pr(&gt;|t|)    \n## Asym   31.306      2.207  14.182  &lt; 2e-16 ***\n## xmid    6.501      2.278   2.854  0.00614 ** \n## scal    9.880      3.152   3.135  0.00280 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.274 on 53 degrees of freedom\n## \n## Number of iterations to convergence: 8 \n## Achieved convergence tolerance: 4.371e-06\nAICc(logistic.model)\n## [1] 386.8643\n\n# Modeldiagnostik (in nlstools)\nplot(nlsResiduals(logistic.model))\n\n\n\n\n# Visualisierung\nplot(ABUND ~ AREA, data = loyn)\npar(mfrow = c(1, 1))\nxv &lt;- seq(0, 2000, 0.01)\n\n# 1. Potenzfunktion\nyv1 &lt;- predict(power.model, list(AREA = xv))\nlines(xv, yv1, col = \"green\")\n\n# 2. Logistische Funktion\nyv2 &lt;- predict(logistic.model, list(AREA = xv))\nlines(xv, yv2, col = \"blue\")\n\n\n\n\n# Visualisierung II\nplot(ABUND ~ log10(AREA), data = loyn)\npar(mfrow = c(1, 1))\n\n# 1. Potenzfunktion\nyv1 &lt;- predict(power.model, list(AREA = xv))\nlines(log10(xv), yv1, col = \"green\")\n\n# 2. Logistische Funktion\nyv2 &lt;- predict(logistic.model, list(AREA = xv))\nlines(log10(xv), yv2, col = \"blue\")\n\n\n\n\n# Model seletkion zwischen den nicht-lineraen Modelen\ncand.models &lt;- list()\ncand.models[[1]] &lt;- power.model\ncand.models[[2]] &lt;- logistic.model\n\nModnames &lt;- c(\"Power\", \"Logistic\")\n\naictab(cand.set = cand.models, modnames = Modnames)\n## \n## Model selection based on AICc:\n## \n##          K   AICc Delta_AICc AICcWt Cum.Wt      LL\n## Logistic 4 386.86       0.00   0.99   0.99 -189.04\n## Power    3 396.17       9.31   0.01   1.00 -194.86"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#smoother",
    "href": "stat1-4/Statistik4_Demo.html#smoother",
    "title": "Stat4: Demo",
    "section": "Smoother",
    "text": "Smoother\n\nloyn$log_AREA &lt;- log10(loyn$AREA)\nplot(ABUND ~ log_AREA, data = loyn)\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 0.25), lwd = 2, col = \"red\")\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 0.5), lwd = 2, col = \"blue\")\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 1), lwd = 2, col = \"green\")"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#gams",
    "href": "stat1-4/Statistik4_Demo.html#gams",
    "title": "Stat4: Demo",
    "section": "GAMs",
    "text": "GAMs\n\nif (!require(mgcv)) {\n    install.packages(\"mgcv\")\n}\nlibrary(mgcv)\n\ngam.1 &lt;- gam(ABUND ~ s(log_AREA), data = loyn)\ngam.1\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## ABUND ~ s(log_AREA)\n## \n## Estimated degrees of freedom:\n## 2.88  total = 3.88 \n## \n## GCV score: 52.145\nsummary(gam.1)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## ABUND ~ s(log_AREA)\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  19.5143     0.9309   20.96   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##               edf Ref.df     F p-value    \n## s(log_AREA) 2.884  3.628 21.14  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.579   Deviance explained = 60.1%\n## GCV = 52.145  Scale est. = 48.529    n = 56\n\nplot(loyn$log_AREA, loyn$ABUND, pch = 16)\nxv &lt;- seq(-1, 4, by = 0.1)\nyv &lt;- predict(gam.1, list(log_AREA = xv))\nlines(xv, yv, lwd = 2, col = \"red\")\n\n\n\n\nAICc(gam.1)\n## [1] 383.2109\nsummary(gam.1)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## ABUND ~ s(log_AREA)\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  19.5143     0.9309   20.96   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##               edf Ref.df     F p-value    \n## s(log_AREA) 2.884  3.628 21.14  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.579   Deviance explained = 60.1%\n## GCV = 52.145  Scale est. = 48.529    n = 56"
  },
  {
    "objectID": "stat1-4/Statistik4_Uebung.html#aufgabe-4.1-nicht-lineare-regression",
    "href": "stat1-4/Statistik4_Uebung.html#aufgabe-4.1-nicht-lineare-regression",
    "title": "Stat4: Übung",
    "section": "Aufgabe 4.1: Nicht-lineare Regression",
    "text": "Aufgabe 4.1: Nicht-lineare Regression\nDatensatz Curonian_Spit.csv\nDieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von geschachtelten Plots (Vegetationsaufnahmen) der Pflanzengesellschaft LolioCynosuretum im Nationalpark Kurische Nehrung (Russland) auf Flächengrössen (Area) von 0.0001 bis 900 m².\nErmittelt den funktionellen Zusammenhang (das beste Modell), der die Zunahme der Artenzahl mit der Flächengrösse am besten beschreibt.Berücksichtigt dabei mindestens die Potenzfunktion (power function, die logarithmische Funktion (logarithmic function,und eine Funktion mit Sättigung (saturation, asymptote) eurer Wahl."
  },
  {
    "objectID": "stat1-4/Statistik4_Uebung.html#aufgabe-4.2n-logistische-regression-natwis",
    "href": "stat1-4/Statistik4_Uebung.html#aufgabe-4.2n-logistische-regression-natwis",
    "title": "Stat4: Übung",
    "section": "Aufgabe 4.2N: Logistische Regression (NatWis)",
    "text": "Aufgabe 4.2N: Logistische Regression (NatWis)\nDatensatz polis.csv\nDer Datensatz polis.csv beschreibt für 19 Inseln im Golf von Kalifornien, ob Eidechsen der Gattung Uta vorkommen (presence/absence: PA) in Abhängigkeit von der Form der Inseln (Verhältnis Umfang zu Fläche: RATIO).\nBitte prüft mit einer logistischen Regression, ob und ggf. wie die Inselform die Präsenz der Eidechsen beinflusst"
  },
  {
    "objectID": "stat1-4/Statistik4_Uebung.html#aufgabe-4.2s-multiple-logistische-regression-sozwis",
    "href": "stat1-4/Statistik4_Uebung.html#aufgabe-4.2s-multiple-logistische-regression-sozwis",
    "title": "Stat4: Übung",
    "section": "Aufgabe 4.2S: Multiple logistische Regression (SozWis)",
    "text": "Aufgabe 4.2S: Multiple logistische Regression (SozWis)\nDatensatz Datensatz_novanimal_Uebung_Statistik4.2.csv\nFührt mit dem Datensatz der Gästebefragung eine logistische Regression durch. Kann der Mensabesuch durch die sozioökonomischen Variablen (Alter, Geschlecht, Hochschulzugehörigkeit), wahrgenommener Fleischkonsum und Umwelteinstellung vorhergesagt werden?"
  },
  {
    "objectID": "stat1-4/Statistik4_Loesung_1.html#musterlösung-übung-4.1-nicht-lineare-regression",
    "href": "stat1-4/Statistik4_Loesung_1.html#musterlösung-übung-4.1-nicht-lineare-regression",
    "title": "Stat4: Lösung 4.1",
    "section": "Musterlösung Übung 4.1: Nicht-lineare Regression",
    "text": "Musterlösung Übung 4.1: Nicht-lineare Regression\nÜbungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\n\nLaden Sie den Datensatz Curonia_spit.xlsx. Dieser enthält gemittelte\nPflanzenartenzahlen (Species.richness) von geschachtelten Plots (Vegetationsaufnahmen) der Pflanzengesellschaft Lolio-Cynosuretum im Nationalpark Kurische Nehrung (Russland) auf Flächengrössen (Area) von 0.0001 bis 900 m².\nErmitteln Sie den funktionellen Zusammenhang, der die Zunahme der Artenzahl mit der Flächengrösse am besten beschreibt. Berücksichtigen Sie dabei mindestens die Potenzfunktion (power function), die logarithmische Funktion (logarithmic function) und eine Funktion mit Sättigung (saturation, asymptote) Ihrer Wahl\nBitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen\nExplorative Datenanalyse, um zu sehen, ob eine nicht-lineare Regression überhaupt nötig ist und ob evtl. Dateneingabefehler vorliegen vorgenommen werden sollten\nDefinition von mindestens drei nicht-linearen Regressionsmodellen\nSelektion des/der besten Models/Modelle\nDurchführen der Modelldiagnostik für die Modelle in der engeren Auswahl, um zu entscheiden, ob das gewählte Vorgehen korrekt war oder ggf. angepasst werden muss\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\n\nFormulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).\n\n\nMusterlösung Übung 4.1 - Nicht-lineare Regression\nAus der Excel-Tabelle wurde das relevante Arbeitsblatt als csv gespeichert\n\ncuronian &lt;- read.delim(\"datasets/statistik/Curonian_spit.csv\", sep = \",\")\nstr(curonian)\n## 'data.frame':    16 obs. of  3 variables:\n##  $ X               : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ Area            : num  0.0001 0.0025 0.01 0.0625 0.25 1 4 9 16 25 ...\n##  $ Species.richness: num  2.1 9.1 14.3 23.1 30.1 37.4 48.5 54.5 58 59.9 ...\nsummary(curonian)\n##        X              Area          Species.richness\n##  Min.   : 1.00   Min.   :  0.0001   Min.   : 2.10   \n##  1st Qu.: 4.75   1st Qu.:  0.2031   1st Qu.:28.35   \n##  Median : 8.50   Median : 12.5000   Median :56.25   \n##  Mean   : 8.50   Mean   :147.1453   Mean   :50.09   \n##  3rd Qu.:12.25   3rd Qu.:131.2500   3rd Qu.:69.95   \n##  Max.   :16.00   Max.   :900.0000   Max.   :92.40\n\n# Explorative Datenanalyse\nplot(Species.richness ~ Area, data = curonian)\n\n\n\n\nEs liegt in der Tat ein nicht-linearer Zusammenhang vor, der sich gut mit nls analysieren lässt. Die Daten beinhalten keine erkennbaren Fehler, da der Artenreichtum der geschachtelten Plots mit der Fläche ansteigt.\n\n# Potenzfunktion selbst definiert\nif (!require(nlstools)) {\n    install.packages(\"nlstools\")\n}\nlibrary(nlstools)\n# power.model &lt;- nls(Species.richness~c*Area^z, data = curonian)\n# summary(power.model)\n\nFalls die Funktion so keine Ergebnisse liefert, oder das Ergebnis unsinnig aussieht, wenn man es später plottet, müsste man hier geeignete Startwerte angeben, die man aus der Betrachtung der Daten oder aus Erfahrungen mit der Funktion für ähnliche Datensets gewinnt,etwa so:\n\npower.model &lt;- nls(Species.richness ~ c * Area^z, start = (list(c = 1, z = 0.2)),\n    data = curonian)\nsummary(power.model)\n## \n## Formula: Species.richness ~ c * Area^z\n## \n## Parameters:\n##    Estimate Std. Error t value Pr(&gt;|t|)    \n## c 36.168960   1.408966   25.67 3.56e-13 ***\n## z  0.138941   0.007472   18.60 2.88e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.142 on 14 degrees of freedom\n## \n## Number of iterations to convergence: 9 \n## Achieved convergence tolerance: 8.143e-06\n\nDas Ergebnis ist identisch\n\n# logarithmische Funktion selbst definiert\nlogarithmic.model &lt;- nls(Species.richness ~ b0 + b1 * log10(Area), data = curonian)\nsummary(logarithmic.model)\n## \n## Formula: Species.richness ~ b0 + b1 * log10(Area)\n## \n## Parameters:\n##    Estimate Std. Error t value Pr(&gt;|t|)    \n## b0   43.333      1.358   31.91 1.78e-14 ***\n## b1   13.281      0.654   20.31 8.75e-12 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.265 on 14 degrees of freedom\n## \n## Number of iterations to convergence: 1 \n## Achieved convergence tolerance: 5.568e-09\n\nZu den verschiedenen Funktionen mit Sättigungswert (Asymptote) gehören Michaelis-Menten, das aymptotische Modell durch den Ursprung und die logistische Funktion. Die meisten gibt es in R als selbststartende Funktionen, was meist besser funktioniert als wenn man sich selbst Gedanken über Startwerte usw. machen muss. Man kann sie aber auch selbst definieren\nIm Folgenden habe ich ein paar unterschiedliche Sättigungsfunktionen mit verschiedenen Einstellungen durchprobiert, um zu zeigen, was alles passieren kann…\n\nmicmen.1 &lt;- nls(Species.richness ~ SSmicmen(Area, Vm, K), data = curonian)\nsummary(micmen.1)\n## \n## Formula: Species.richness ~ SSmicmen(Area, Vm, K)\n## \n## Parameters:\n##    Estimate Std. Error t value Pr(&gt;|t|)    \n## Vm  72.0108     4.2708  16.861 1.07e-10 ***\n## K    0.8477     0.4371   1.939   0.0729 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 11.96 on 14 degrees of freedom\n## \n## Number of iterations to convergence: 0 \n## Achieved convergence tolerance: 3.383e-06\n\n# Dasselbe selbst definiert (mit default-Startwerten)\nmicmen.2 &lt;- nls(Species.richness ~ Vm * Area/(K + Area), data = curonian)\nsummary(micmen.2)\n## \n## Formula: Species.richness ~ Vm * Area/(K + Area)\n## \n## Parameters:\n##    Estimate Std. Error t value Pr(&gt;|t|)    \n## Vm  46.7020     9.6748   4.827 0.000268 ***\n## K   -2.1532     0.5852  -3.679 0.002477 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 35.14 on 14 degrees of freedom\n## \n## Number of iterations to convergence: 23 \n## Achieved convergence tolerance: 9.097e-06\n\nHier ist das Ergebnis deutlich verschieden, ein Phänomen, das einem bei nicht-linearen Regressionen anders als bei linearen Regressionen immer wieder begegnen kann, da der Iterationsalgorithmus in lokalen Optima hängen bleiben kann. Oftmals dürfte die eingebaute Selbststartfunktion bessere Ergebnisse liefern, aber das werden wir unten sehen.\n\n# Dasselbe selbst definiert (mit sinnvollen Startwerten, basierend auf dem\n# Plot)\nmicmen.3 &lt;- nls(Species.richness ~ Vm * Area/(K + Area), start = list(Vm = 100, K = 1),\n    data = curonian)\nsummary(micmen.3)\n## \n## Formula: Species.richness ~ Vm * Area/(K + Area)\n## \n## Parameters:\n##    Estimate Std. Error t value Pr(&gt;|t|)    \n## Vm  72.0111     4.2708  16.861 1.07e-10 ***\n## K    0.8477     0.4371   1.939   0.0729 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 11.96 on 14 degrees of freedom\n## \n## Number of iterations to convergence: 22 \n## Achieved convergence tolerance: 7.003e-06\n\nWenn man sinnvollere Startwerte als die default-Werte (1 für alle Parameter) eingibt, hier etwas einen mutmasslichen Asymptoten-Wert (aus der Grafik) von Vm = ca. 100, dann bekommt man das gleiche Ergebnis wie bei der Selbsstartfunktion\n\n# Eine asymptotische Funktion durch den Ursprung (mit implementierter\n# Selbststartfunktion)\nasym.model &lt;- nls(Species.richness ~ SSasympOrig(Area, Asym, lrc), data = curonian)\nsummary(asym.model)\n## \n## Formula: Species.richness ~ SSasympOrig(Area, Asym, lrc)\n## \n## Parameters:\n##      Estimate Std. Error t value Pr(&gt;|t|)    \n## Asym  68.5066     4.4278  15.472 3.38e-10 ***\n## lrc    0.1184     0.4864   0.244    0.811    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 13.88 on 14 degrees of freedom\n## \n## Number of iterations to convergence: 0 \n## Achieved convergence tolerance: 2.867e-06\n\n\nlogistic.model &lt;- nls(Species.richness ~ SSlogis(Area, asym, xmid, scal), data = curonian)\nsummary(logistic.model)\n\nError in nls(y ~ 1/(1 + exp((xmid - x)/scal)), data = xy, start = list(xmid= aux[1L], : Iterationenzahl überschritt Maximum 50\nDas ist etwas, was einem bei nls immer wieder passieren kann. Die Iteration ist nach der eingestellten max. Iterationszahl noch nicht zu einem Ergebnis konvergiert. Um ein Ergebnis für diese Funktion zu bekommen, müsste man mit den Einstellungen von nls „herumspielen“, etwas bei den Startwerten oder den max. Um das effizient zu machen, braucht man aber etwas Erfahrung Interationszahlen (man kann z. B. manuell die Maximalzahl der Iterationen erhöhen, indem man in den Funktionsaufruf etwa maxiter =100 als zusätzliches Argument reinschreibtn).\nLogistische Regression mit Startwerten\n\nlogistic.model.2 &lt;- nls(Species.richness ~ asym/(1 + exp((xmid - Area)/scal)), control = nls.control(maxiter = 100),\n    start = (list(xmid = 1, scal = 0.2, asym = 100)), data = curonian)\nsummary(logistic.model.2)\n## \n## Formula: Species.richness ~ asym/(1 + exp((xmid - Area)/scal))\n## \n## Parameters:\n##      Estimate Std. Error t value Pr(&gt;|t|)    \n## xmid    3.970      1.608   2.469   0.0282 *  \n## scal    4.112      1.676   2.453   0.0290 *  \n## asym   73.634      4.507  16.339 4.79e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 12.11 on 13 degrees of freedom\n## \n## Number of iterations to convergence: 59 \n## Achieved convergence tolerance: 9.676e-06\n\n\n# Vergleich der Modellgüte mittels AICc\nlibrary(AICcmodavg)\ncand.models &lt;- list()\ncand.models[[1]] &lt;- power.model\ncand.models[[2]] &lt;- logarithmic.model\ncand.models[[3]] &lt;- micmen.1\ncand.models[[4]] &lt;- micmen.2\ncand.models[[5]] &lt;- asym.model\ncand.models[[6]] &lt;- logistic.model.2\n\nModnames &lt;- c(\"Power\", \"Logarithmic\", \"Michaelis-Menten (SS)\", \"Michaelis-Menten\",\n    \"Asymptotic through origin\", \"Logistische Regression\")\naictab(cand.set = cand.models, modnames = Modnames)\n## \n## Model selection based on AICc:\n## \n##                           K   AICc Delta_AICc AICcWt Cum.Wt     LL\n## Power                     3  96.75       0.00   0.98   0.98 -44.38\n## Logarithmic               3 104.43       7.68   0.02   1.00 -48.21\n## Michaelis-Menten (SS)     3 130.67      33.92   0.00   1.00 -61.34\n## Logistische Regression    4 133.53      36.78   0.00   1.00 -60.95\n## Asymptotic through origin 3 135.44      38.69   0.00   1.00 -63.72\n## Michaelis-Menten          3 165.17      68.42   0.00   1.00 -78.58\n\nDiese Ergebnistabelle vergleicht die Modellgüte zwischen den fünf Modellen, die wir in unsere Auswahl reingesteckt haben. Alle haben drei geschätzte Parameter (K), also zwei Funktionsparameter und die Varianz. Das beste Modell (niedrigster AICc bzw. Delta = 0) hat das Potenzmodell (power). Das zweitbeste Modell (logarithmic) hat bereits einen Delta-AICc von mehr als 4, ist daher statistisch nicht relevant. Das zeigt sich auch am Akaike weight, das für das zweite Modell nur noch 2 % ist. Die verschiedenen Modelle mit oberem Grenzwert (3-5) sind komplett ungeeignet.\n\n# Modelldiagnostik für das beste Modell\nlibrary(nlstools)\nplot(nlsResiduals(power.model))\n\n\n\n\nLinks oben sieht man zwar ein Muster (liegt daran, dass in diesem Fall die Plots geschachtelt, und nicht unabhängig waren), aber jedenfalls keinen problematischen Fall wie einen Bogen oder einen Keil. Der QQ-Plot rechts unten ist völlig OK. Somit haben wir auch keine problematische Abweichung von der Normalverteilung der Residuen. Da es sich bei den einzelnen Punkten allerdings bereits um arithmetische Mittelwerte aus je 8 Beobachtungen handelt, hätte man sich auch einfach auf das Central Limit Theorem beziehen können, das sagt, dass Mittelwerte automatisch einer Normalverteilung folgen.\n\n# Ergebnisplot\nplot(Species.richness ~ Area, pch = 16, xlab = \"Fläche [m²]\", ylab = \"Artenreichtum\",\n    data = curonian)\nxv &lt;- seq(0, 1000, by = 0.1)\nyv &lt;- predict(power.model, list(Area = xv))\nlines(xv, yv, lwd = 2, col = \"red\")\nyv2 &lt;- predict(micmen.1, list(Area = xv))\nlines(xv, yv2, lwd = 2, col = \"blue\")\n\n\n\n\nDas ist der Ergebnisplot für das beste Modell. Wichtig ist, dass man die Achsen korrekt beschriftet und nicht einfach die mehr oder weniger kryptischen Spaltennamen aus R nimmt.\nIm Weiteren habe ich noch eine Sättigungsfunktion (Michaelis-Menten mit Selbststarter) zum Vergleich hinzugeplottet\nMan erkennt, dass die Sättigungsfunktion offensichtlich den tatsächlichen Kurvenverlauf sehr schlecht widergibt. Im mittleren Kurvenbereich sind die Schätzwerte zu hoch, für grosse Flächen dann aber systematisch viel zu niedrig. Man kann die Darstellung im doppeltlogarithmischen Raum wiederholen, um die Kurvenanpassung im linken Bereich besser differenzieren zu können:\n\n# Ergebnisplot Double-log\nplot(log10(Species.richness) ~ log10(Area), pch = 16, xlab = \"log A\", ylab = \"log (S)\",\n    data = curonian)\n\nxv &lt;- seq(0, 1000, by = 1e-04)\n\nyv &lt;- predict(power.model, list(Area = xv))\nlines(log10(xv), log10(yv), lwd = 2, col = \"red\")\n\nyv2 &lt;- predict(micmen.1, list(Area = xv))\nlines(log10(xv), log10(yv2), lwd = 2, col = \"blue\")\n\n\n\n\nAuch hier sieht man, dass die rote Kurve zwar nicht perfekt, aber doch viel besser als die blaue Kurve ist."
  },
  {
    "objectID": "stat1-4/Statistik4_Loesung_2n.html#musterlösung-aufgabe-4.2n-multiple-logistische-regression",
    "href": "stat1-4/Statistik4_Loesung_2n.html#musterlösung-aufgabe-4.2n-multiple-logistische-regression",
    "title": "Stat4: Lösung 4.2N",
    "section": "Musterlösung Aufgabe 4.2N: Multiple logistische Regression",
    "text": "Musterlösung Aufgabe 4.2N: Multiple logistische Regression\n\npolis &lt;- read.csv(\"datasets/statistik/polis.csv\")\npolis\n##     X     ISLAND RATIO PA\n## 1   1       Bota 15.41  1\n## 2   2     Cabeza  5.63  1\n## 3   3    Cerraja 25.92  1\n## 4   4 Coronadito 15.17  0\n## 5   5     Flecha 13.04  1\n## 6   6   Gemelose 18.85  0\n## 7   7   Gemelosw 30.95  0\n## 8   8   Jorabado 22.87  0\n## 9   9     Mitlan 12.01  0\n## 10 10       Pata 11.60  1\n## 11 11      Piojo  6.09  1\n## 12 12      Smith  2.28  1\n## 13 13    Ventana  4.05  1\n## 14 14    Bahiaan 59.94  0\n## 15 15    Bahiaas 63.16  0\n## 16 16     Blanca 22.76  0\n## 17 17   Pescador 23.54  0\n## 18 18   Angeldlg  0.21  1\n## 19 19      Mejia  2.55  1\n\nstr(polis)\n## 'data.frame':    19 obs. of  4 variables:\n##  $ X     : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ ISLAND: chr  \"Bota\" \"Cabeza\" \"Cerraja\" \"Coronadito\" ...\n##  $ RATIO : num  15.41 5.63 25.92 15.17 13.04 ...\n##  $ PA    : int  1 1 1 0 1 0 0 0 0 1 ...\nsummary(polis)\n##        X           ISLAND              RATIO             PA        \n##  Min.   : 1.0   Length:19          Min.   : 0.21   Min.   :0.0000  \n##  1st Qu.: 5.5   Class :character   1st Qu.: 5.86   1st Qu.:0.0000  \n##  Median :10.0   Mode  :character   Median :15.17   Median :1.0000  \n##  Mean   :10.0                      Mean   :18.74   Mean   :0.5263  \n##  3rd Qu.:14.5                      3rd Qu.:23.20   3rd Qu.:1.0000  \n##  Max.   :19.0                      Max.   :63.16   Max.   :1.0000\n\nMan erkennt, dass polis 19 Beobachtungen von drei Parametern enthält, wobei ISLAND ein Faktor mit den Inselnamen ist, während RATIO metrisch ist und PA nur 0 oder 1 enthält. Prädiktorvariable ist RATIO, abhängige Variable PA, mithin ist das korrekte statistische Verfahren eine logistische Regression (GLM).\n\n# Explorative Datenanalyse\nboxplot(polis$RATIO)\n\n\n\n\nDer Boxplot zeigt zwei starke Ausreisser, ist also etwas rechtsschief. Da es sich aber um die unabhängige Variable handelt, muss uns das nicht weiter stören.\n\n# Definition des logistischen Modells\nglm.1 &lt;- glm(PA ~ RATIO, family = \"binomial\", data = polis)\nsummary(glm.1)\n## \n## Call:\n## glm(formula = PA ~ RATIO, family = \"binomial\", data = polis)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.6067  -0.6382   0.2368   0.4332   2.0986  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)  \n## (Intercept)   3.6061     1.6953   2.127   0.0334 *\n## RATIO        -0.2196     0.1005  -2.184   0.0289 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 26.287  on 18  degrees of freedom\n## Residual deviance: 14.221  on 17  degrees of freedom\n## AIC: 18.221\n## \n## Number of Fisher Scoring iterations: 6\n\nModell ist signifikant (p-Wert in Zeile RATIO ist &lt; 0.05). Jetzt müssen wir noch prüfen, ob es auch valide ist:\n\n# Modelldiagnostik für das gewählte Modell (wenn nicht signifikant, dann OK)\n1 - pchisq(glm.1$deviance, glm.1$df.resid)\n## [1] 0.6514215\n\n# Visuelle Inspektion der Linearität\nlibrary(car)\ncrPlots(glm.1, ask = F)\n\n\n\n\nBeide Aspekte sind OK, d.h. der Test war nicht signifikant und die pinkfarbene Linie liegt fast auf der theoretischen Linie (blau gestrichelt).\nJetzt brauchen wir noch die Modellgüte (Pseudo-R2):\n\n# Modellgüte (pseudo-R²)\n1 - (glm.1$dev/glm.1$null)\n## [1] 0.4590197\n\nUm zu unser Modell zu interpretieren müssen wir noch in Betracht ziehen, dass wir nicht die Vorkommenswahrscheinlichkeit selbst, sondern logit (Vorkommenswahrscheinlichkeit) modelliert haben. Unser Ergebnis (die beiden Parameterschätzungen von oben, also 3.6061 und -0.2196) muss also zunächst in etwas Interpretierbares übersetzt werden:\n\n# Steilheit der Beziehung in Modellen mit nur einem Parameter\nexp(glm.1$coef[2])\n##     RATIO \n## 0.8028734\n\n&lt; 1, d. h. Vorkommenswahrscheinlichkeit sinkt mit zunehmender Isolation.\n\n# LD50 für 1-Parameter-Modelle (hier also x-Werte, bei der 50% der Inseln\n# besiedelt sind)\n-glm.1$coef[1]/glm.1$coef[2]\n## (Intercept) \n##     16.4242\n\nAm besten stellen wir auch unsere Funktionsgleichung dar. Dazu müssen wir das „Rohergebnis“ (mit P = Vorkommenswahrscheinlichkeit)\nln (P/ (1- P)) = 3.606 – 0.220 RATIO\nso umformen, dass wir links nur P stehen haben:\nP = exp (3.606 – 0.220 RATIO) / (1 + exp (3.606 – 0.220 RATIO))\nDas ist also unsere vorhergesagte Regressionsfunktion, die wir in einem letzten Schritt auch noch visualisieren können (und sollten):\n\n# Ergebnisplots\npar(mfrow = c(1, 1))\n\nxs &lt;- seq(0, 70, l = 1000)\nglm.predict &lt;- predict(glm.1, type = \"response\", se = T, newdata = data.frame(RATIO = xs))\n\nplot(PA ~ RATIO, data = polis, xlab = \"Umfang-Flächen-Verhältnis\", ylab = \"Vorkommenswahrscheinlichkeit\",\n    pch = 16, col = \"red\")\npoints(glm.predict$fit ~ xs, type = \"l\")\nlines(glm.predict$fit + glm.predict$se.fit ~ xs, type = \"l\", lty = 2)\nlines(glm.predict$fit - glm.predict$se.fit ~ xs, type = \"l\", lty = 2)"
  },
  {
    "objectID": "stat1-4/Statistik4_Loesung_2s.html#musterlösung-übung-4.2s-multiple-logistische-regression-sozwis",
    "href": "stat1-4/Statistik4_Loesung_2s.html#musterlösung-übung-4.2s-multiple-logistische-regression-sozwis",
    "title": "Stat4: Lösung 4.2S",
    "section": "Musterlösung Übung 4.2S: Multiple logistische Regression (SozWis)",
    "text": "Musterlösung Übung 4.2S: Multiple logistische Regression (SozWis)\n\nLese-Empfehlung Kapitel 6 von Manny Gimond\nLese-Empfehlung Kapitel 4 von Gareth (2016)\nLese-Empfehlung Vorlesungsfolien von Oscar Torres-Reyna Princeton University\n\n\nKommentierter Lösungsweg\n\ndf &lt;- read_csv2(\"datasets/statistik/Datensatz_novanimal_Uebung_Statistik4.2.csv\")\n\n# sieht euch die Verteilung zwischen Mensagänger und Selbstverpfleger an sind\n# nicht gleichmässig verteilt, bei der Vorhersage müssen wir das\n# berücksichtigen\ntable(df$mensa)\n## \n##   0   1 \n## 282 786\ndf |&gt;\n    count(mensa)  # alternativ\n## # A tibble: 2 × 2\n##   mensa     n\n##   &lt;dbl&gt; &lt;int&gt;\n## 1     0   282\n## 2     1   786\n\n# definiert das logistische Modell und wendet es auf den Datensatz an\n\nmod0 &lt;- glm(mensa ~ gender + member + age_groups + meat + umwelteinstellung, data = df,\n    binomial(\"logit\"))\nsummary.lm(mod0)  # Umwelteinstellung scheint keinen Einfluss auf die \n## \n## Call:\n## glm(formula = mensa ~ gender + member + age_groups + meat + umwelteinstellung, \n##     family = binomial(\"logit\"), data = df)\n## \n## Weighted Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.6740 -0.8078  0.3712  0.5867  1.2379 \n## \n## Coefficients:\n##                              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                  -0.18889    0.40225  -0.470 0.638750    \n## genderMann                    0.71017    0.16018   4.434 1.02e-05 ***\n## memberStudent/in             -0.63072    0.29442  -2.142 0.032404 *  \n## age_groups26- bis 34-jaehrig  1.09429    0.19574   5.591 2.88e-08 ***\n## age_groups35- bis 49-jaehrig  1.75379    0.45968   3.815 0.000144 ***\n## age_groups50- bis 64-jaehrig  2.43530    0.78923   3.086 0.002083 ** \n## meat                          0.19945    0.05055   3.945 8.49e-05 ***\n## umwelteinstellung             0.19334    0.18688   1.035 0.301107    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.009 on 1060 degrees of freedom\n## Multiple R-squared:  0.004042,   Adjusted R-squared:  -0.002536 \n## F-statistic: 0.6145 on 7 and 1060 DF,  p-value: 0.7443\n# Verpflegung zu haben, gegeben die Daten\n\n# neues Modell ohne Umwelteinstellung\nmod1 &lt;- update(mod0, ~. - umwelteinstellung)\nsummary.lm(mod1)\n## \n## Call:\n## glm(formula = mensa ~ gender + member + age_groups + meat, family = binomial(\"logit\"), \n##     data = df)\n## \n## Weighted Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.0117 -0.8060  0.3584  0.6100  1.2407 \n## \n## Coefficients:\n##                              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                   0.03212    0.34053   0.094 0.924860    \n## genderMann                    0.69697    0.15951   4.369 1.37e-05 ***\n## memberStudent/in             -0.64418    0.29426  -2.189 0.028806 *  \n## age_groups26- bis 34-jaehrig  1.11651    0.19458   5.738 1.25e-08 ***\n## age_groups35- bis 49-jaehrig  1.77409    0.45947   3.861 0.000120 ***\n## age_groups50- bis 64-jaehrig  2.44683    0.78953   3.099 0.001992 ** \n## meat                          0.18070    0.04709   3.837 0.000132 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.01 on 1061 degrees of freedom\n## Multiple R-squared:  0.003998,   Adjusted R-squared:  -0.001635 \n## F-statistic: 0.7098 on 6 and 1061 DF,  p-value: 0.6418\n\n# Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq(mod1$deviance, mod1$df.resid)  # Ok\n## [1] 0.4509591\n\n# Modellgüte (pseudo-R²)\n1 - (mod1$dev/mod1$null)  # eher kleines pseudo-R2, deckt sich mit dem R-Squared aus dem obigen output summary.lm()\n## [1] 0.1354244\n\n# Konfusionsmatrix vom Datensatz Model Vorhersage hier ein anderes Beispiel:\npredicted &lt;- predict(mod1, df, type = \"response\")\n\n# erzeugt eine Tabelle mit den beobachteten Mensagänger/Selbstverpfleger und\n# den Vorhersagen des Modells\nkm &lt;- table(predicted &gt; 0.5, df$mensa)\n# alles was höher/grosser ist als 50% ist kommt in die Kategorie Mensagänger\n\n# anpassung der namen\ndimnames(km) &lt;- list(c(\"Modell Selbstverpfleger\", \"Modell Mensagänger\"), c(\"Daten Selbstverpfleger\",\n    \"Daten Mensagänger\"))\nkm\n##                         Daten Selbstverpfleger Daten Mensagänger\n## Modell Selbstverpfleger                     87                59\n## Modell Mensagänger                         195               727\n\n############# reminder:\n############# https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\n\n# TP = true positive: you predicted positive and it’s true; hier Vorhersage\n# Mensagänger stimmt also (727)\n\n# TN = true negative: you predicted negative and it’s true, hier Vorhersage der\n# Selbstverpfleger stimmt (87)\n\n# FP = false positive (fehler 1. art, auch Spezifizität genannt) you predicted\n# and it’s false. hier Modell sagt Mensagänger vorher (obwohl in Realität\n# Selbstverpfleger) (195)\n\n# FN = false negative (fehler 2. art, auch Sensitivität genannt), you predicted\n# negative and it’s false. hier Modell sagt Selbtverpfleger vorher (obwohl in\n# Realität Mensagänger) (59)\n\n\n# es scheint, dass das Modell häufig einen alpha Fehler macht, d.h.  das Modell\n# weist keine hohe Spezifizität auf: konkret werden viele Mensagänger als\n# Selbstverpfleger vorhergesagt resp. klassifiziert. Dafür gibt es mehere\n# Gründe:\n\n# 1) die Kriteriumsvariable ist sehr ungleich verteilt, d.h. es gibt weniger\n# Selbstverpfleger als Mensgänger im Datensatz\n\n# 2) nicht adäquates Modell z.B. link mit probit zeigt besserer fit\n\n# 3) Overfitting: wurde hier nicht berücksichtigt, in einem Paper/Arbeit müsste\n# noch einen Validierungstest gemacht werden z.B. test-train Cross-Validation\n# oder k fold Cross-Validation\n\n# kalkuliert die Missklassifizierungsrate\nmf &lt;- 1 - sum(diag(km)/sum(km))  # ist mit knapp 23 %  eher hoch\nmf\n## [1] 0.2378277\n\n# kleiner exkurs: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2636062/ col\n# wise proportion, da diese die 'Realität' darstellt\nkm_prop &lt;- prop.table(km, 2)\n\n# specificity = a / (a+c) =&gt; ability of a test to correctly\nspec = km_prop[1]/(km_prop[1] + km_prop[2])\nspec\n## [1] 0.3085106\n\n# sensitivity = d / (b+d) =&gt; Sensitivity is the ability of a\nsens = km_prop[4]/(km_prop[3] + km_prop[4])\nsens\n## [1] 0.9249364\n\n\n\nMethode\nIn der Aufgabe war es das Ziel zu schauen, ob wir einen potenziellen Besuch eines Mensagasts vorhersagen können und zwar in Abhängigkeit von den sozioökonomischen Variablen, wahrgenommene Fleischkonsum und der Umwelteinstellung. Die Kriteriumsvariable “Mensa” weist eine binäre Verteilung auf: Deshalb rechnen wir eine multiple logistische Regression mit den Prädiktoren “Alter”, “Geschlecht”, “Hochschulzugehörigkeit”, “Fleischkonsum” und “Umwelteinstellung. Mehr Informationen zu den logistischen Regressionen findet ihr im Buch von Crawley (2015) oder auch im Buch von Gareth (2016).\n\n\nErgebnisse\n\n\n# | echo: false | fig.cap: Konfusionsmatrix\nknitr::kable(km)\n\n\n\n\n\nDaten Selbstverpfleger\nDaten Mensagänger\n\n\n\n\nModell Selbstverpfleger\n87\n59\n\n\nModell Mensagänger\n195\n727\n\n\n\n\n\nDer Output des logistischen Models mit der Linkfunktin “logit” sagt und, dass das Modell nicht gut zu den Daten passt, d.h. mit dem Modell (gegeben die Daten) können wir nur schlecht vorhersagen, ob eine Person zukünftig sich in der Mensa verpflegt oder ihr Mittagessen selber mitnimmt. Hinweise dafür geben das kleine pseudo-R2 (14%) als auch die hohe Missklassifizierungsrate (24%): bei genauerer Betrachtung fällt auf, dass das Modell häufig einen alpha-Fehler begeht, d.h. unser Modell sagt zu viele Mensagänger vorher, obwohl diese in Realität Selbstverpfleger sind. Es gibt verschiedene Gründe für diesen schlechten Modelfit:\n\ndie Kriteriumsvariable ist sehr ungleich verteilt, d.h. es gibt weniger Selbstverpfleger als Mensgänger im Datensatz (26% vs. 74%)\ndie Prädiktorvariablen sind alle entweder kategorial oder ordinal: dies kann dazu führen, dass das Model keinen guten fit zu den Daten erzielt\n\nFazit: Es sollte nach einem weiteren adäquateren Modell gesucht werden: insbesondere ein Modell, welches einen mit ordinalen Prädiktorvariablen umgehen kann:\n\neine bessere Link-Funktion für das GLM suchen z.B. probit\npolynomiale Kontraste\nSmooth Splines hier\nmultinomiale Regression z.M. nnet::mulitom() hier"
  },
  {
    "objectID": "Stat5-8.html",
    "href": "Stat5-8.html",
    "title": "Statistik 5 - 8",
    "section": "",
    "text": "Statistik 5\nIn Statistik 5 lernen die Studierenden Lösungen kennen, welche die diversen Limitierungen von linearen Modellen überwinden. Während generalized linear models (GLMs) aus Statistik 4 bekannt sind, geht es jetzt um linear mixed effect models (LMMs und generalized linear mixed effect models (GLMMs). Dabei bezeichnet generalized die explizite Modellierung anderer Fehler- und Varianzstrukturen und mixed die Berücksichtigung von Abhängigkeiten bzw. Schachtelungen unter den Beobachtungen. Einfachere Fälle von LMMs, wie split-plot und repeated-measures ANOVAs, lassen sich noch mit dem aov-Befehl in Base R bewältigen, für komplexere Versuchsdesigns/Analysen gibt es spezielle R packages. Abschliessend gibt es eine kurze Einführung in GLMMs, die eine Analyse komplexerer Beobachtungsdaten z. B. mit räumlichen Abhängigkeiten, erlauben.\n\n\nStatistik 6\nStatistik 6 führt in multivariat-deskriptive Methoden ein, die dazu dienen Datensätze mit multiplen abhängigen und multiplen unabhängigen Variablen effektiv zu analysieren. Dabei betonen Ordinationen kontinuierliche Gradienten und fokussieren auf zusammengehörende Variablen, während Cluster-Analysen Diskontinuitäten betonen und auf zusammengehörende Beobachtungen fokussieren. Es folgt eine konzeptionelle Einführung in die Idee von Ordinationen als einer Technik der deskriptiven Statistik, die Strukturen in multivariaten Datensätzen via Dimensionsreduktion visualisiert. Das Prinzip und die praktische Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse (PCA) erklärt. Danach folgen kurze Einführungen in weitere Ordinationstechniken für besondere Fälle, welche bestimmte Limitierungen der PCA überwinden, namentlich CA, DCA und NMDS.\n\n\nStatistik 7\nIn Statistik 7 beschäftigen wir uns zunächst damit, wie wir Ordinationsdiagramme informativer gestalten können, etwa durch die Beschriftung der Beobachtunge, post-hoc-Projektion der Prädiktorvariablen oder Response surfaces. Während wir bislang mit «unconstrained» Ordinationen gearbeitet haben, welche die Gesamtvariabilität in den Beobachtungen visualisieren, beschränken die jeweiligen «constrained»-Varianten derselben Ordinationsmethoden die Betrachtung auf den Teil der Variabilität, welcher durch eine Linearkombination der berücksichtigen Prädiktoren erklärt werden kann. Wir beschäftigen uns im Detail mit der Redundanz-Analyse (RDA), der «constrained»-Variante der PCA und gehen einen kompletten analytischen Ablauf mit Aufbereitung, Interpretation und Visualisierung der Ergebnisse am Beispiel eines gemeinschaftsökologischen Datensatzes (Fischgesellschaften und Umweltfaktoren im Jura-Fluss Doubs) durch\n\n\nStatistik 8\nIn Statistik 8 lernen die Studierenden Clusteranalysen/Klassifikationen als eine den Ordinationen komplementäre Technik der deskriptiven Statistik multivariater Datensätze kennen. Es gibt Partitionierungen (ohne Hierarchie), divisive und agglomerative Clusteranalysen (die jeweils eine Hierarchie produzieren). Etwas genauer gehen wir auf die k-means Clusteranalyse (eine Partitionierung) und eine Reihe von agglomerativen Clusterverfahren ein. Hierbei hat das gewählte Distanzmass und der Modus für die sukzessive Fusion von Clustern einen grossen Einfluss auf das Endergebnis. Wir besprechen ferner, wie man die Ergebnisse von Clusteranalysen adäquat visualisieren und mit anderen statistischen Prozeduren kombinieren kann. Im Abschluss von Statistik 8 werden wir dann die an den acht Statistiktagen behandelten Verfahren noch einmal rückblickend betrachten und thematisieren, welches Verfahren wann gewählt werden sollte. Ebenfalls ist Platz, um den adäquaten Ablauf statistischer Analysen vom Einlesen der Daten bis zur Verschriftlichung der Ergebnisse, einschliesslich der verschiedenen zu treffenden Entscheidungen, zu thematisieren.\n\n\n\n\n\n\n\n\n\n\n\nTitel\n\n\nDatum\n\n\nLesson\n\n\nThema\n\n\n\n\n\n\nStat5: Demo\n\n\n2022-11-14\n\n\nStat5\n\n\nVon linearen Modellen zu GLMMs\n\n\n\n\nStat5: Übung\n\n\n2022-11-14\n\n\nStat5\n\n\nVon linearen Modellen zu GLMMs\n\n\n\n\nStat5: Lösung 1\n\n\n2022-11-14\n\n\nStat5\n\n\nVon linearen Modellen zu GLMMs\n\n\n\n\nStat5: Lösung 2\n\n\n2022-11-14\n\n\nStat5\n\n\nVon linearen Modellen zu GLMMs\n\n\n\n\nStat6: Demo\n\n\n2022-11-15\n\n\nStat6\n\n\nEinführung in “multivariate” Methoden\n\n\n\n\nStat6: Übung\n\n\n2022-11-15\n\n\nStat6\n\n\nEinführung in “multivariate” Methoden\n\n\n\n\nStat6: Lösung\n\n\n2022-11-15\n\n\nStat6\n\n\nEinführung in “multivariate” Methoden\n\n\n\n\nStat7: Demo\n\n\n2022-11-21\n\n\nStat7\n\n\nOrdinationen II\n\n\n\n\nStat7: Übung\n\n\n2022-11-21\n\n\nStat7\n\n\nOrdinationen II\n\n\n\n\nStat7: Lösung\n\n\n2022-11-21\n\n\nStat7\n\n\nOrdinationen II\n\n\n\n\nStat8: Demo\n\n\n2022-11-22\n\n\nStat8\n\n\nClusteranalysen\n\n\n\n\nStat8: Übung\n\n\n2022-11-22\n\n\nStat8\n\n\nClusteranalysen\n\n\n\n\nStat8: Lösung\n\n\n2022-11-22\n\n\nStat8\n\n\nClusteranalysen\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "stat5-8/Statistik5_Demo.html#split-plot-anova",
    "href": "stat5-8/Statistik5_Demo.html#split-plot-anova",
    "title": "Stat5: Demo",
    "section": "Split-plot ANOVA",
    "text": "Split-plot ANOVA\nReaktionszeitenbeispiel aus Kapitel 14 von Logan (2010)\n\n# Daten laden\nspf &lt;- read.delim(\"datasets/statistik/spf.csv\", sep = \";\") \n# Daten anschauen\nhead(spf)\n\n     Signal  VP Messung Reaktion\n1 akustisch VP1      H1        3\n2 akustisch VP1      H2        4\n3 akustisch VP1      H3        7\n4 akustisch VP1      H4        7\n5 akustisch VP2      H1        6\n6 akustisch VP2      H2        5\n\n# LM mit random intercept = einfaches LMM\nspf.aov &lt;- aov(Reaktion~Signal * Messung + Error(VP), data = spf)\nsummary(spf.aov)\n\n\nError: VP\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nSignal     1  3.125   3.125       2  0.207\nResiduals  6  9.375   1.562               \n\nError: Within\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nMessung         3 194.50   64.83  127.89 2.52e-12 ***\nSignal:Messung  3  19.37    6.46   12.74 0.000105 ***\nResiduals      18   9.13    0.51                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Interaktion anschauen\ninteraction.plot(spf$Messung, spf$Signal, spf$Reaktion)\n\n# Nun als LMM\nif(!require(nlme)){install.packages(\"nlme\")}\n\n\n\nlibrary(nlme)\n# Mit random intercept (VP) und random slope (Messung)\nspf.lme.1 &lt;- lme(Reaktion~Signal * Messung, random = ~Messung | VP, data = spf)\n# Nur random intercept\nspf.lme.2 &lt;- lme(Reaktion~Signal * Messung, random = ~1 | VP, data = spf)\n# Modelle anschauen\nanova(spf.lme.1)\n\n               numDF denDF   F-value p-value\n(Intercept)        1    18 1488.1653  &lt;.0001\nSignal             1     6    2.0809  0.1993\nMessung            3    18   70.7887  &lt;.0001\nSignal:Messung     3    18   11.8592  0.0002\n\nanova(spf.lme.2)\n\n               numDF denDF  F-value p-value\n(Intercept)        1    18 591.6800  &lt;.0001\nSignal             1     6   2.0000  0.2070\nMessung            3    18 127.8904  &lt;.0001\nSignal:Messung     3    18  12.7397  0.0001\n\nsummary(spf.lme.1)\n\nLinear mixed-effects model fit by REML\n  Data: spf \n       AIC      BIC    logLik\n  97.63924 120.0223 -29.81962\n\nRandom effects:\n Formula: ~Messung | VP\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev    Corr                \n(Intercept) 1.0801438 (Intr) MssnH2 MssnH3\nMessungH2   0.6455542 -0.717              \nMessungH3   0.6455552 -0.837  0.600       \nMessungH4   1.3229046 -0.816  0.390  0.878\nResidual    0.2886123                     \n\nFixed effects:  Reaktion ~ Signal * Messung \n                        Value Std.Error DF   t-value p-value\n(Intercept)              3.75 0.5590187 18  6.708184  0.0000\nSignalvisuell           -2.00 0.7905718  6 -2.529814  0.0447\nMessungH2                0.25 0.3818816 18  0.654653  0.5210\nMessungH3                3.25 0.3818821 18  8.510481  0.0000\nMessungH4                4.25 0.6922194 18  6.139672  0.0000\nSignalvisuell:MessungH2  1.00 0.5400622 18  1.851639  0.0806\nSignalvisuell:MessungH3  0.50 0.5400628 18  0.925818  0.3668\nSignalvisuell:MessungH4  4.00 0.9789460 18  4.086027  0.0007\n Correlation: \n                        (Intr) Sgnlvs MssnH2 MssnH3 MssnH4 Sg:MH2 Sg:MH3\nSignalvisuell           -0.707                                          \nMessungH2               -0.683  0.483                                   \nMessungH3               -0.781  0.552  0.571                            \nMessungH4               -0.808  0.571  0.394  0.788                     \nSignalvisuell:MessungH2  0.483 -0.683 -0.707 -0.404 -0.279              \nSignalvisuell:MessungH3  0.552 -0.781 -0.404 -0.707 -0.557  0.571       \nSignalvisuell:MessungH4  0.571 -0.808 -0.279 -0.557 -0.707  0.394  0.788\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-0.8658379 -0.2664958 -0.0326311  0.2360380  0.9528579 \n\nNumber of Observations: 32\nNumber of Groups: 8 \n\nsummary(spf.lme.2)\n\nLinear mixed-effects model fit by REML\n  Data: spf \n       AIC      BIC    logLik\n  89.64876 101.4293 -34.82438\n\nRandom effects:\n Formula: ~1 | VP\n        (Intercept)  Residual\nStdDev:   0.5137012 0.7120003\n\nFixed effects:  Reaktion ~ Signal * Messung \n                        Value Std.Error DF   t-value p-value\n(Intercept)              3.75 0.4389856 18  8.542422  0.0000\nSignalvisuell           -2.00 0.6208194  6 -3.221549  0.0181\nMessungH2                0.25 0.5034602 18  0.496564  0.6255\nMessungH3                3.25 0.5034602 18  6.455326  0.0000\nMessungH4                4.25 0.5034602 18  8.441580  0.0000\nSignalvisuell:MessungH2  1.00 0.7120003 18  1.404494  0.1772\nSignalvisuell:MessungH3  0.50 0.7120003 18  0.702247  0.4915\nSignalvisuell:MessungH4  4.00 0.7120003 18  5.617975  0.0000\n Correlation: \n                        (Intr) Sgnlvs MssnH2 MssnH3 MssnH4 Sg:MH2 Sg:MH3\nSignalvisuell           -0.707                                          \nMessungH2               -0.573  0.405                                   \nMessungH3               -0.573  0.405  0.500                            \nMessungH4               -0.573  0.405  0.500  0.500                     \nSignalvisuell:MessungH2  0.405 -0.573 -0.707 -0.354 -0.354              \nSignalvisuell:MessungH3  0.405 -0.573 -0.354 -0.707 -0.354  0.500       \nSignalvisuell:MessungH4  0.405 -0.573 -0.354 -0.354 -0.707  0.500  0.500\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.34519292 -0.63943480 -0.06164167  0.41510594  2.15199656 \n\nNumber of Observations: 32\nNumber of Groups: 8"
  },
  {
    "objectID": "stat5-8/Statistik5_Demo.html#glmm",
    "href": "stat5-8/Statistik5_Demo.html#glmm",
    "title": "Stat5: Demo",
    "section": "GLMM",
    "text": "GLMM\n-&gt; Hirschparasitenbeispiel aus Kapitel 13 von Zuur et al. (2009)\n\n# Daten laden und für GLMM aufbereiten\nDeerEcervi &lt;- read.delim(\"datasets/statistik/DeerEcervi.txt\", sep = \"\", stringsAsFactors = TRUE)\n# Daten anschauen\nhead(DeerEcervi)\n\n  Farm Sex Length Ecervi\n1   AL   2    164   0.00\n2   AL   1    216   0.00\n3   AL   1    208   0.00\n4   AL   1    206  14.37\n5   AL   1    204   0.00\n6   AL   1    200   0.00\n\nsummary(DeerEcervi)\n\n      Farm          Sex            Length          Ecervi       \n MO     :209   Min.   :1.000   Min.   : 75.0   Min.   :   0.00  \n CB     : 85   1st Qu.:1.000   1st Qu.:151.0   1st Qu.:   0.00  \n QM     : 60   Median :1.000   Median :163.0   Median :   6.60  \n BA     : 50   Mean   :1.458   Mean   :161.8   Mean   :  45.42  \n PN     : 37   3rd Qu.:2.000   3rd Qu.:174.9   3rd Qu.:  35.79  \n MB     : 34   Max.   :2.000   Max.   :216.0   Max.   :2186.60  \n (Other):351                                                    \n\n# Anzahl Larven hier in Presence/Absence übersetzt\nDeerEcervi$Ecervi.01 &lt;- DeerEcervi$Ecervi\nDeerEcervi$Ecervi.01[DeerEcervi$Ecervi&gt;0] &lt;- 1\n# Numerische Geschlechtscodierung als Factor\nDeerEcervi$fSex &lt;- as.factor(DeerEcervi$Sex)\n# Hischlänge standardisieren\nDeerEcervi$CLength &lt;- DeerEcervi$Length - mean(DeerEcervi$Length)\n\n-&gt; Nun sind die Daten bereit:\n\nDie Parasitenbefalldaten wurden in Parasiten Präsenz/Absenz (1/0) übersetzt\nDie Hirschlänge wurde standardisiert, damit der Achsenabschnitt (intercept) des Modells interpretierbar ist, standardisierte entspricht nun der Achsenabschnitt einem durschnittlich langen Hirsch.\n\n\n# Zunächst als GLM\n# Interaktionen mit fFarm nicht berücksichtigt, da zu viele Freiheitsgrade verbraucht würden\nDE.glm &lt;- glm(Ecervi.01 ~ CLength * fSex + Farm, family = binomial, data = DeerEcervi)\n\ndrop1(DE.glm, test = \"Chi\")\n\nSingle term deletions\n\nModel:\nEcervi.01 ~ CLength * fSex + Farm\n             Df Deviance     AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;            745.50  799.50                      \nFarm         23  1003.72 1011.72 258.225 &lt; 2.2e-16 ***\nCLength:fSex  1   755.48  807.48   9.984  0.001579 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(DE.glm)\n\n\nCall:\nglm(formula = Ecervi.01 ~ CLength * fSex + Farm, family = binomial, \n    data = DeerEcervi)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8400  -0.7576   0.3556   0.6431   2.2964  \n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.796e+00  5.900e-01  -3.044 0.002336 ** \nCLength        4.062e-02  7.132e-03   5.695 1.24e-08 ***\nfSex2          6.280e-01  2.292e-01   2.740 0.006150 ** \nFarmAU         3.340e+00  7.841e-01   4.259 2.05e-05 ***\nFarmBA         3.510e+00  7.150e-01   4.908 9.19e-07 ***\nFarmBE         1.883e+01  6.216e+02   0.030 0.975831    \nFarmCB         3.012e+00  6.573e-01   4.583 4.58e-06 ***\nFarmCRC       -1.293e+01  2.400e+03  -0.005 0.995701    \nFarmHB        -2.364e-01  9.730e-01  -0.243 0.808045    \nFarmLN         3.831e+00  8.881e-01   4.314 1.60e-05 ***\nFarmMAN        1.046e+00  6.960e-01   1.503 0.132855    \nFarmMB         3.693e+00  8.152e-01   4.529 5.91e-06 ***\nFarmMO         9.722e-01  5.969e-01   1.629 0.103380    \nFarmNC         1.370e+00  6.904e-01   1.985 0.047169 *  \nFarmNV         2.098e+00  7.702e-01   2.725 0.006435 ** \nFarmPN         4.185e+00  8.584e-01   4.875 1.09e-06 ***\nFarmQM         3.975e+00  7.220e-01   5.506 3.68e-08 ***\nFarmRF         4.552e+00  1.050e+00   4.337 1.45e-05 ***\nFarmRN         8.706e-01  7.454e-01   1.168 0.242822    \nFarmRO         4.555e+00  9.556e-01   4.766 1.88e-06 ***\nFarmSAU       -1.545e+01  1.368e+03  -0.011 0.990986    \nFarmSE         2.785e+00  7.876e-01   3.536 0.000407 ***\nFarmTI         3.900e+00  1.166e+00   3.343 0.000828 ***\nFarmTN         3.102e+00  7.665e-01   4.046 5.21e-05 ***\nFarmVISO       3.720e+00  1.011e+00   3.679 0.000234 ***\nFarmVY         3.974e+00  1.257e+00   3.162 0.001565 ** \nCLength:fSex2  3.618e-02  1.168e-02   3.097 0.001953 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1073.1  on 825  degrees of freedom\nResidual deviance:  745.5  on 799  degrees of freedom\nAIC: 799.5\n\nNumber of Fisher Scoring iterations: 15\n\nanova(DE.glm)\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: Ecervi.01\n\nTerms added sequentially (first to last)\n\n             Df Deviance Resid. Df Resid. Dev\nNULL                           825    1073.13\nCLength       1   64.815       824    1008.31\nfSex          1    0.191       823    1008.12\nFarm         23  252.638       800     755.48\nCLength:fSex  1    9.984       799     745.50\n\n# Response curves für die einzelnen Farmen (Weibliche Tiere: fSex = \"1\" )\nplot(DeerEcervi$CLength, DeerEcervi$Ecervi.01,\n     xlab = \"Length\", ylab = \"Probability of \\\n     presence of E. cervi L1\")\nI &lt;- order(DeerEcervi$CLength)\nAllFarms &lt;- unique(DeerEcervi$Farm)\nfor (j in AllFarms){\n  mydata &lt;- data.frame(CLength=DeerEcervi$CLength, fSex = \"1\",\n                       Farm = j)\n  n &lt;- dim(mydata)[1]\n  if (n&gt;10){\n    P.DE2 &lt;- predict(DE.glm, mydata, type = \"response\")\n    lines(mydata$CLength[I], P.DE2[I])\n  }}\n\n\n\n# glmm \nif(!require(MASS)){install.packages(\"MASS\")}\nlibrary(MASS)\nDE.PQL &lt;- glmmPQL(Ecervi.01 ~ CLength * fSex,\n                random = ~ 1 | Farm, family = binomial, data = DeerEcervi)\nsummary(DE.PQL)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: DeerEcervi \n  AIC BIC logLik\n   NA  NA     NA\n\nRandom effects:\n Formula: ~1 | Farm\n        (Intercept)  Residual\nStdDev:    1.462108 0.9620576\n\nVariance function:\n Structure: fixed weights\n Formula: ~invwt \nFixed effects:  Ecervi.01 ~ CLength * fSex \n                  Value Std.Error  DF  t-value p-value\n(Intercept)   0.8883697 0.3373283 799 2.633547  0.0086\nCLength       0.0378608 0.0065269 799 5.800768  0.0000\nfSex2         0.6104570 0.2137293 799 2.856216  0.0044\nCLength:fSex2 0.0350666 0.0108558 799 3.230228  0.0013\n Correlation: \n              (Intr) CLngth fSex2 \nCLength       -0.108              \nfSex2         -0.191  0.230       \nCLength:fSex2  0.092 -0.522  0.235\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-6.3466592 -0.6387839  0.2978382  0.5218829  3.4912879 \n\nNumber of Observations: 826\nNumber of Groups: 24 \n\n# Modellvoraussagen berechnen \ng &lt;- 0.8883697 + 0.0378608 * DeerEcervi$CLength\n# Rücktransformieren aus Logit\np.averageFarm1 &lt;- exp(g)/(1 + exp(g))\n# Sortierung der Hirschgrössen für's Plotten\nI &lt;- order(DeerEcervi$CLength)\n# Plotten\nplot(DeerEcervi$CLength, DeerEcervi$Ecervi.01, xlab=\"Length\",\n     ylab = \"Probability of presence of E. cervi L1\")\nlines(DeerEcervi$CLength[I], p.averageFarm1[I],lwd = 3)\n# Vertrauensintervalle (CI) mit SD von Random factor berechnen\n# Generell CI = mean + 1.96*SD\np.Upp &lt;- exp(g + 1.96 * 1.462108)/(1 + exp(g + 1.96 * 1.462108))\np.Low &lt;- exp(g - 1.96 * 1.462108)/(1 + exp(g - 1.96 * 1.462108))\nlines(DeerEcervi$CLength[I], p.Upp[I])\nlines(DeerEcervi$CLength[I], p.Low[I])\n\n\n\n# Dasselbe mit dem lme4-Package\nif(!require(lme4)){install.packages(\"lme4\")}\nlibrary(lme4)\nDE.lme4 &lt;- glmer(Ecervi.01 ~ CLength * fSex + (1|Farm), \n                 family = binomial, data = DeerEcervi)\nsummary(DE.lme4)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Ecervi.01 ~ CLength * fSex + (1 | Farm)\n   Data: DeerEcervi\n\n     AIC      BIC   logLik deviance df.resid \n   832.6    856.1   -411.3    822.6      821 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.2678 -0.6090  0.2809  0.5022  3.4546 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n Farm   (Intercept) 2.391    1.546   \nNumber of obs: 826, groups:  Farm, 24\n\nFixed effects:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.938969   0.356002   2.638  0.00835 ** \nCLength       0.038964   0.006917   5.633 1.77e-08 ***\nfSex2         0.624487   0.222937   2.801  0.00509 ** \nCLength:fSex2 0.035859   0.011409   3.143  0.00167 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) CLngth fSex2 \nCLength     -0.107              \nfSex2       -0.189  0.238       \nCLngth:fSx2  0.091 -0.514  0.232\n\nif(!require(glmmML)){install.packages(\"glmmML\")}\nlibrary(glmmML)\nDE.glmmML &lt;- glmmML(Ecervi.01 ~ CLength * fSex,\n                  cluster = Farm, family = binomial, data = DeerEcervi)\nsummary(DE.glmmML)\n\n\nCall:  glmmML(formula = Ecervi.01 ~ CLength * fSex, family = binomial,      data = DeerEcervi, cluster = Farm) \n\n                 coef se(coef)     z Pr(&gt;|z|)\n(Intercept)   0.93968 0.357915 2.625 8.65e-03\nCLength       0.03898 0.006956 5.604 2.10e-08\nfSex2         0.62451 0.224251 2.785 5.35e-03\nCLength:fSex2 0.03586 0.011437 3.135 1.72e-03\n\nScale parameter in mixing distribution:  1.547 gaussian \nStd. Error:                              0.2975 \n\n        LR p-value for H_0: sigma = 0:  1.346e-41 \n\nResidual deviance: 822.6 on 821 degrees of freedom  AIC: 832.6"
  },
  {
    "objectID": "stat5-8/Statistik5_Uebung.html#aufgabe-5.1-split-plot-anova",
    "href": "stat5-8/Statistik5_Uebung.html#aufgabe-5.1-split-plot-anova",
    "title": "Stat5: Übung",
    "section": "Aufgabe 5.1: Split-plot ANOVA",
    "text": "Aufgabe 5.1: Split-plot ANOVA\nDatensatz splityield.csv\nVersuch zum Ernteertrag (yield) einer Kulturpflanze in Abhängigkeit der drei Faktoren Bewässerung (irrigated vs. control), Düngung (N, NP, P) und Aussaatdichten (low, medium, high). Es gab vier ganze Felder (block), die zwei Hälften mit den beiden Bewässerungstreatments, diese wiederum drei Drittel für die drei Saatdichten und diese schliesslich je drei Drittel für die drei Düngertreatments hatten.\n\nAufgaben\n\nBestimmt das minimal adäquate Modell\nStellt die Ergebnisse da"
  },
  {
    "objectID": "stat5-8/Statistik5_Uebung.html#aufgabe-5.2-glmm",
    "href": "stat5-8/Statistik5_Uebung.html#aufgabe-5.2-glmm",
    "title": "Stat5: Übung",
    "section": "Aufgabe 5.2: GLMM",
    "text": "Aufgabe 5.2: GLMM\nDatensatz Datensatz_novanimal_Uebung_Statistik5.2.csv\nFührt mit dem novanimal Datensatz (inviduelle Daten) eine logistische Regression durch, wobei ihr die einzelnen Käufer (single campus_card holder) als weitere randomisierte Variable mitberücksichtigt. Kann der Fleischkonsum durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden? Vergleich die Ergebnisse mit der eurem multiplen logistische Modell von Aufgabe 4.2.\n\nAufgaben\n\nBestimmt das minimal adäquate Modell\nStellt die Ergebnisse dar"
  },
  {
    "objectID": "stat5-8/Statistik5_Loesung_1.html#musterlösung-aufgabe-5.1-split-plot-anova",
    "href": "stat5-8/Statistik5_Loesung_1.html#musterlösung-aufgabe-5.1-split-plot-anova",
    "title": "Stat5: Lösung 1",
    "section": "Musterlösung Aufgabe 5.1: Split-plot ANOVA",
    "text": "Musterlösung Aufgabe 5.1: Split-plot ANOVA\n\nÜbungsaufgabe\n(hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\n\nLadet den Datensatz splityield.csv. Dieser enthält Versuchsergebnisse eines Experiments zum Ernteertrag (yield) einer Kulturpflanze in Abhängigkeit der drei Faktoren Bewässerung (irrigated vs. control), Düngung (N, NP, P) und Aussaatdichten (low, medium, high). Es gab vier ganze Felder (block), die zwei Hälften mit den beiden Bewässerungstreatments (irrigation), diese wiederum drei Drittel für die drei Saatdichten (density) und diese schliesslich je drei Drittel für die drei Düngertreatments (fertilizer) hatten.\nErmittelt das minimal adäquate statistische Modell, das den Ernteertrag in Abhängigkeit von den angegebenen Faktoren beschreibt.\nBitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu diesem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in das ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen\nExplorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten\nAuswahl und Begründung eines statistischen Verfahrens\nBestimmung des vollständigen/maximalen Models\nSelektion des/der besten Models/Modelle\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\nFormuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).\n\n\n\n\nKommentierter Lösungsweg\n\nsplityield &lt;- read.delim(\"datasets/statistik/splityield.csv\", sep = \",\", stringsAsFactors = T)\n\n\n# Checken der eingelesenen Daten\nsplityield\n##     X yield block irrigation density fertilizer\n## 1   1    90     A    control     low          N\n## 2   2    95     A    control     low          P\n## 3   3   107     A    control     low         NP\n## 4   4    92     A    control  medium          N\n## 5   5    89     A    control  medium          P\n## 6   6    92     A    control  medium         NP\n## 7   7    81     A    control    high          N\n## 8   8    92     A    control    high          P\n## 9   9    93     A    control    high         NP\n## 10 10    80     A  irrigated     low          N\n## 11 11    87     A  irrigated     low          P\n## 12 12   100     A  irrigated     low         NP\n## 13 13   121     A  irrigated  medium          N\n## 14 14   110     A  irrigated  medium          P\n## 15 15   119     A  irrigated  medium         NP\n## 16 16    78     A  irrigated    high          N\n## 17 17    98     A  irrigated    high          P\n## 18 18   122     A  irrigated    high         NP\n## 19 19    83     B    control     low          N\n## 20 20    80     B    control     low          P\n## 21 21    95     B    control     low         NP\n## 22 22    98     B    control  medium          N\n## 23 23    98     B    control  medium          P\n## 24 24   106     B    control  medium         NP\n## 25 25    74     B    control    high          N\n## 26 26    81     B    control    high          P\n## 27 27    74     B    control    high         NP\n## 28 28   102     B  irrigated     low          N\n## 29 29   109     B  irrigated     low          P\n## 30 30   105     B  irrigated     low         NP\n## 31 31    99     B  irrigated  medium          N\n## 32 32    94     B  irrigated  medium          P\n## 33 33   123     B  irrigated  medium         NP\n## 34 34   136     B  irrigated    high          N\n## 35 35   133     B  irrigated    high          P\n## 36 36   132     B  irrigated    high         NP\n## 37 37    85     C    control     low          N\n## 38 38    88     C    control     low          P\n## 39 39    88     C    control     low         NP\n## 40 40   112     C    control  medium          N\n## 41 41   104     C    control  medium          P\n## 42 42    91     C    control  medium         NP\n## 43 43    82     C    control    high          N\n## 44 44    78     C    control    high          P\n## 45 45    94     C    control    high         NP\n## 46 46    60     C  irrigated     low          N\n## 47 47   104     C  irrigated     low          P\n## 48 48   114     C  irrigated     low         NP\n## 49 49    90     C  irrigated  medium          N\n## 50 50   118     C  irrigated  medium          P\n## 51 51   113     C  irrigated  medium         NP\n## 52 52   119     C  irrigated    high          N\n## 53 53   122     C  irrigated    high          P\n## 54 54   136     C  irrigated    high         NP\n## 55 55    86     D    control     low          N\n## 56 56    78     D    control     low          P\n## 57 57    89     D    control     low         NP\n## 58 58    79     D    control  medium          N\n## 59 59    86     D    control  medium          P\n## 60 60    87     D    control  medium         NP\n## 61 61    85     D    control    high          N\n## 62 62    89     D    control    high          P\n## 63 63    83     D    control    high         NP\n## 64 64    73     D  irrigated     low          N\n## 65 65   114     D  irrigated     low          P\n## 66 66   114     D  irrigated     low         NP\n## 67 67   109     D  irrigated  medium          N\n## 68 68   131     D  irrigated  medium          P\n## 69 69   126     D  irrigated  medium         NP\n## 70 70   116     D  irrigated    high          N\n## 71 71   136     D  irrigated    high          P\n## 72 72   133     D  irrigated    high         NP\n\nMan sieht, dass das Design vollkommen balanciert ist, d.h. jede Kombination irrigation  density  fertilizer kommt genau 4x vor (in jedem der vier Blöcke A-D einmal).\n\nstr(splityield)\n## 'data.frame':    72 obs. of  6 variables:\n##  $ X         : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ yield     : int  90 95 107 92 89 92 81 92 93 80 ...\n##  $ block     : Factor w/ 4 levels \"A\",\"B\",\"C\",\"D\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ irrigation: Factor w/ 2 levels \"control\",\"irrigated\": 1 1 1 1 1 1 1 1 1 2 ...\n##  $ density   : Factor w/ 3 levels \"high\",\"low\",\"medium\": 2 2 2 3 3 3 1 1 1 2 ...\n##  $ fertilizer: Factor w/ 3 levels \"N\",\"NP\",\"P\": 1 3 2 1 3 2 1 3 2 1 ...\nsummary(splityield)\n##        X             yield        block      irrigation   density   fertilizer\n##  Min.   : 1.00   Min.   : 60.00   A:18   control  :36   high  :24   N :24     \n##  1st Qu.:18.75   1st Qu.: 86.00   B:18   irrigated:36   low   :24   NP:24     \n##  Median :36.50   Median : 95.00   C:18                  medium:24   P :24     \n##  Mean   :36.50   Mean   : 99.72   D:18                                        \n##  3rd Qu.:54.25   3rd Qu.:114.00                                               \n##  Max.   :72.00   Max.   :136.00\nsplityield$density &lt;- ordered(splityield$density, levels = c(\"low\", \"medium\", \"high\"))\nsplityield$density\n##  [1] low    low    low    medium medium medium high   high   high   low   \n## [11] low    low    medium medium medium high   high   high   low    low   \n## [21] low    medium medium medium high   high   high   low    low    low   \n## [31] medium medium medium high   high   high   low    low    low    medium\n## [41] medium medium high   high   high   low    low    low    medium medium\n## [51] medium high   high   high   low    low    low    medium medium medium\n## [61] high   high   high   low    low    low    medium medium medium high  \n## [71] high   high  \n## Levels: low &lt; medium &lt; high\n\nMan sieht, dass die Variable yield metrisch ist, während die vier anderen Variablen schon korrekt als kategoriale Variablen (factors) kodiert sind\n\n# Explorative Datenanalyse (auf Normalverteilung, Varianzhomogenität)\nboxplot(yield ~ fertilizer, data = splityield)\n\n\n\nboxplot(yield ~ irrigation, data = splityield)\n\n\n\nboxplot(yield ~ density, data = splityield)\n\n\n\nboxplot(yield ~ irrigation * density * fertilizer, data = splityield)\n\n\n\n\nDie Boxplots sind generell hinreichend symmetrisch, so dass man davon ausgehen kann, dass keine problematische Abweichung von der Normalverteilung vorliegt. Die Varianzhomogenität sieht für den Gesamtboxplot sowie für fertilizer und density bestens aus, für irrigation und für die 3-fach-Interaktion deuten sich aber gewisse Varianzheterogenitäten an, d. h. die Boxen (Interquartil-Spannen) sind deutlich unterschiedlich lang. Da das Design aber vollkommen „balanciert“ war, wie wir von oben wissen, sind selbst relativ stark divergierende Varianzen nicht besonders problematisch. Der Boxplot der Dreifachinteraktion zeigt zudem, dass grössere Varianzen (~Boxen) mal bei kleinen, mal bei grossen Mittelwerten vorkommen, womit wir bedenkenlos weitermachen können (Wenn die grossen Varianzen immer bei grossen Mittelwerten aufgetreten wären, hätten wir eine log- oder Wurzeltransformation von yield in Betracht ziehen müssen).\n\nboxplot(log10(yield) ~ irrigation * density * fertilizer, data = splityield)  # bringt keine Verbesserung\n\n\n\naov.1 &lt;- aov(yield ~ irrigation * density * fertilizer + Error(block/irrigation/density),\n    data = splityield)\n\nDas schwierigste an der Analyse ist hier die Definition des Splitt-Plot ANOVA-Modells. Hier machen wir es mit der einfachsten Möglichkeit, dem aov-Befehl. Um diesen richtig zu spezifieren, muss man verstanden haben, welches der „random“-Faktor war und wie die „fixed“ factors ineinander geschachtelt waren. In diesem Fall ist block der random Faktor, in den zunächst irrigation und dann density geschachtelt sind (die unterste Ebene fertilizer muss man nicht mehr angeben, da diese in der nächsthöheren nicht repliziert ist).\n(Übrigens: das simple 3-faktorielle ANOVA-Modell aov(yield~irrigationdensityfertilizer,data=splityield) würde unterstellen, dass alle 72 subplot unabhängig von allen anderen angeordnet sind, also nicht in Blöcken. Man kann ausprobieren, wie sich das Ergebnis mit dieser Einstellung unterscheidet)\n\nsummary(aov.1)\n## \n## Error: block\n##           Df Sum Sq Mean Sq F value Pr(&gt;F)\n## Residuals  3  194.4   64.81               \n## \n## Error: block:irrigation\n##            Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## irrigation  1   8278    8278   17.59 0.0247 *\n## Residuals   3   1412     471                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: block:irrigation:density\n##                    Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## density             2   1758   879.2   3.784 0.0532 .\n## irrigation:density  2   2747  1373.5   5.912 0.0163 *\n## Residuals          12   2788   232.3                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: Within\n##                               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## fertilizer                     2 1977.4   988.7  11.449 0.000142 ***\n## irrigation:fertilizer          2  953.4   476.7   5.520 0.008108 ** \n## density:fertilizer             4  304.9    76.2   0.883 0.484053    \n## irrigation:density:fertilizer  4  234.7    58.7   0.680 0.610667    \n## Residuals                     36 3108.8    86.4                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWir bekommen p-Werte für die drei Einzeltreatments, die drei 2-fach-Interaktionen und die 3- fach Interaktion. Keinen p-Wert gibt es dagegen für block, da dieser als „random“ Faktor spezifiziert wurde. Signifikant sind für sich genommen irrigation und fertilizer sowie die Interaktionen irrigation:density und irrigation:fertilizer.\n\n# Modelvereinfachung\naov.2 &lt;- aov(yield ~ irrigation + density + fertilizer + irrigation:density + irrigation:fertilizer +\n    density:fertilizer + Error(block/irrigation/density), data = splityield)\nsummary(aov.2)\n## \n## Error: block\n##           Df Sum Sq Mean Sq F value Pr(&gt;F)\n## Residuals  3  194.4   64.81               \n## \n## Error: block:irrigation\n##            Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## irrigation  1   8278    8278   17.59 0.0247 *\n## Residuals   3   1412     471                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: block:irrigation:density\n##                    Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## density             2   1758   879.2   3.784 0.0532 .\n## irrigation:density  2   2747  1373.5   5.912 0.0163 *\n## Residuals          12   2788   232.3                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: Within\n##                       Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## fertilizer             2   1977   988.7  11.828 9.21e-05 ***\n## irrigation:fertilizer  2    953   476.7   5.703  0.00662 ** \n## density:fertilizer     4    305    76.2   0.912  0.46639    \n## Residuals             40   3344    83.6                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naov.3 &lt;- aov(yield ~ irrigation + density + fertilizer + irrigation:density + irrigation:fertilizer +\n    Error(block/irrigation/density), data = splityield)\nsummary(aov.3)\n## \n## Error: block\n##           Df Sum Sq Mean Sq F value Pr(&gt;F)\n## Residuals  3  194.4   64.81               \n## \n## Error: block:irrigation\n##            Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## irrigation  1   8278    8278   17.59 0.0247 *\n## Residuals   3   1412     471                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: block:irrigation:density\n##                    Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## density             2   1758   879.2   3.784 0.0532 .\n## irrigation:density  2   2747  1373.5   5.912 0.0163 *\n## Residuals          12   2788   232.3                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Error: Within\n##                       Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## fertilizer             2   1977   988.7  11.924 7.28e-05 ***\n## irrigation:fertilizer  2    953   476.7   5.749  0.00605 ** \n## Residuals             44   3648    82.9                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nJetzt muss man nur noch herausfinden, wie irrigation und fertilizer wirken und wie die Interaktionen aussehen. Bei multiplen ANOVAs macht man das am besten visuell:\n\n# Visualisierung der Ergebnisse\nboxplot(yield ~ fertilizer, data = splityield)\n\n\n\nboxplot(yield ~ irrigation, data = splityield)\n\n\n\n\ninteraction.plot(splityield$fertilizer, splityield$irrigation, splityield$yield,\n    xlab = \"fertilizer\", ylab = \"mean of splityield\", trace.label = \"irrigation\")\n\n\n\ninteraction.plot(splityield$density, splityield$irrigation, splityield$yield, xlab = \"fertilizer\",\n    ylab = \"mean of splityield\", trace.label = \"irrigation\")"
  },
  {
    "objectID": "stat5-8/Statistik5_Loesung_2.html#musterlösung-übung-5.2-glmm",
    "href": "stat5-8/Statistik5_Loesung_2.html#musterlösung-übung-5.2-glmm",
    "title": "Stat5: Lösung 2",
    "section": "Musterlösung Übung 5.2: GLMM",
    "text": "Musterlösung Übung 5.2: GLMM\n\nLese-Empfehlung Kapitel 4.3.1 von Christopher Molnar\nInteressierte hier oder hier\n\n\nKommentierter Lösungsweg\n\n# lade datei\ndf &lt;- read_csv2(\"datasets/statistik/Datensatz_novanimal_Uebung_Statistik5.2.csv\")\n\n# sieht euch die Verteilung zwischen Fleisch und kein Fleisch an, beide\n# Kategorien kommen nicht gleich häufig vor, aber nicht super tragisch\nprop.table(table(df$meat))  # gibt die Prozente an\n## \n##         0         1 \n## 0.3959218 0.6040782\ntable(df$meat)  # gibt die absoluten Werte an\n## \n##     0     1 \n##  7087 10813\n\n# definiert das logistische Modell mit ccrs als random intercept und wendet es\n# auf den Datensatz an\n\n# Exkurs für Neugierige check out ICC:\n# https://www.datanovia.com/en/lessons/intraclass-correlation-coefficient-in-r/\n# attention: however data needs to be wide format\n\nlibrary(lme4)\n# dauert ein paar sekunden\nmod0 &lt;- glmer(meat ~ gender + member + age_group + (1 | ccrs), data = df, binomial(\"logit\"))\n\n# lasst euch das Modell anzeigen: sieht so aus, als ob v.a. Geschlecht eine\n# Rolle spielt Wahrnmeldung kann vernachlässigt werden (aufgrund der unicode\n# resp.  Umlaute in den Variablen)\nsummary(mod0)\n## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: binomial  ( logit )\n## Formula: meat ~ gender + member + age_group + (1 | ccrs)\n##    Data: df\n## \n##      AIC      BIC   logLik deviance df.resid \n##  21124.3  21178.8 -10555.1  21110.3    17893 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -3.8652 -0.7267  0.4335  0.6449  3.4115 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  ccrs   (Intercept) 1.487    1.219   \n## Number of obs: 17900, groups:  ccrs, 1427\n## \n## Fixed effects:\n##                           Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)               -0.12569    0.14484  -0.868   0.3855    \n## genderM                    0.94466    0.08386  11.265   &lt;2e-16 ***\n## memberStudierende         -0.17822    0.12655  -1.408   0.1591    \n## age_group26 bis 34-jährig -0.16855    0.10175  -1.657   0.0976 .  \n## age_group35 bis 49-jährig -0.02040    0.15634  -0.131   0.8962    \n## age_group50 bis 64-jährig  0.09752    0.20159   0.484   0.6286    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##             (Intr) gendrM mmbrSt a_26b3 a_35b4\n## genderM     -0.337                            \n## membrStdrnd -0.834 -0.023                     \n## ag_gr26b34- -0.537 -0.025  0.349              \n## ag_gr35b49- -0.737  0.043  0.662  0.463       \n## ag_gr50b64- -0.637  0.018  0.597  0.389  0.515\n\n## erste Interpretation: Geschlecht (Referenzkategorie: Mann) und Alter\n## (Referenzkategorie: junge Personen) scheinen den Fleischkonsum positiv zu\n## beeinflussen + Hochschulzugehörigkeit spielt keien Rolle d.h. könnte man\n## vernachlässigen. Ich lasse aus inhaltlichen Gründen aber im Modell drin\n\n# Pseudo R^2\nlibrary(MuMIn)\nr.squaredGLMM(mod0)\n##                    R2m       R2c\n## theoretical 0.04069363 0.3392838\n## delta       0.03537644 0.2949516\n# das marginale R^2 (r2m) gibt uns die erklärte Varianz der fixen Effekte: hier\n# 4% (das ist sehr wenig) das conditionale R^2 (r2c) gibt uns die erklärte\n# Varianz für das ganze Modell (mit fixen und variablen Effekten): hier 29%\n# (ganz ok, aber auch nicht sehr hoch) für weitere Informationen:\n# https://rdrr.io/cran/MuMIn/man/r.squaredGLMM.html\n\n# zusätzliche Informationen, welche für die Interpretation gut sein kann\n# berechnet den Standardfehler (mehr infos:\n# https://www.youtube.com/watch?v=r-txC-dpI-E oder hier:\n# https://mgimond.github.io/Stats-in-R/CI.html) weitere info:\n# https://stats.stackexchange.com/questions/26650/how-do-i-reference-a-regression-models-coefficients-standard-errors\nse &lt;- sqrt(diag(vcov(mod0)))\n\n# zeigt eine Tabelle der Schätzer mit 95% Konfidenzintervall (KI) =&gt;\n# Faustregel: falls 0 im KI enthalten ist, dann ist der Unterschied statistisch\n# NICHT signifikant\ntab1 &lt;- cbind(Est = fixef(mod0), LL = fixef(mod0) - 1.96 * se, UL = fixef(mod0) +\n    1.96 * se)\n\n\n# erzeugt die Odds Ratios\ntab2 &lt;- exp(tab1)\n\n\n\nMethoden\nDie Responsevariable “Fleischkonsum” ist eine binäre Variable. Demnach wird eine multiple logistische Regression mit den Prädiktoren “Alter (Gruppen)”, “Geschlecht” und “Hochschulzugehörigkeit” gerechnet. Da in den Daten gewisse Individuen mehrmals vorkommen, wird das Individuum (Variable ccrs) als variabler Effekt in das Modell aufgenommen.\n\n\nErgebnisse\nDas Geschlecht und das Alter nehmen einen signifikanten Einfluss auf den Fleischkonsum (siehe Table 1): Männer kaufen signifikant häufiger ein fleischhaltiges Gericht als Frauen; junge Personen (15 bis 25-jährig) kaufen signifikant häufiger (p &gt; .1) ein fleischhaltiges Gericht in der Mensa. Es sieht so aus, als ob die Hochschulzugehörigkeit auf den ersten Blick keinen Einfluss nimmt. Aber man müsste auch die Interaktion zwischen Geschlecht und Hochschulzugehörigkeit berücksichtigen, um ein abschliessendes Bild zu bekommen. Das kleine marginale Pseudo-R^2 zeigt auf, dass es nicht das “beste” Modell ist. Insbesondere die tiefe Varianzaufklärung für die randomisierte Variable (r2c; ccrs) scheint mit (nur) 4% sehr gering. Das sind Hinweise dafür, dass das Modell ggf. noch weitere Ebenen haben könnte (z.B. Standort Mensa).\n\n\n\nModellschätzer (Coefficients) mit dazugehörigem 95% Konfidenzintervall\n\n\n\nCoefficients\nLower Limit (LL)\nUppewr Limit (UL)\n\n\n\n\nIntercept\n-0.13\n-0.41\n0.16\n\n\nMänner\n0.94\n0.78\n1.11\n\n\nMitarbeitende\n-0.18\n-0.43\n0.07\n\n\n26 bis 34-jährig\n-0.17\n-0.37\n0.03\n\n\n35 bis 49-jährig\n-0.02\n-0.33\n0.29\n\n\n50 bis 64-jährig\n0.10\n-0.30\n0.49\n\n\n\n\n\nDie Chance, dass Männer ein fleischhaltiges Gericht kaufen ist 2.57mal (+157%) höher als bei Frauen (siehe Table 2). Die Chance, dass 26 bis 34-jährige Personen ein fleischhaltiges Gericht kaufen ist kleiner (-16%) als bei den 15 bis 25-jährigen Personen.\n\n\n\nOdds Ratio (OR) mit dazugehörigem 95% Konfidenzintervall\n\n\n\nOR\nLower Limit (LL)\nUppewr Limit (UL)\n\n\n\n\nIntercept\n0.88\n0.66\n1.17\n\n\nMänner\n2.57\n2.18\n3.03\n\n\nMitarbeitende\n0.84\n0.65\n1.07\n\n\n26 bis 34-jährig\n0.84\n0.69\n1.03\n\n\n35 bis 49-jährig\n0.98\n0.72\n1.33\n\n\n50 bis 64-jährig\n1.10\n0.74\n1.64"
  },
  {
    "objectID": "stat5-8/Statistik6_Demo.html#ordinationen-i",
    "href": "stat5-8/Statistik6_Demo.html#ordinationen-i",
    "title": "Stat6: Demo",
    "section": "Ordinationen I",
    "text": "Ordinationen I\n\nPCA\n\n\nif (!require(labdsv)) {\n    install.packages(\"labdsv\")\n}\nlibrary(labdsv)\n\n# Für Ordinationen benötigen wir Matrizen, nicht Dataframes Generieren von\n# Daten\nraw &lt;- matrix(c(1, 2, 2.5, 2.5, 1, 0.5, 0, 1, 2, 4, 3, 1), nrow = 6)\ncolnames(raw) &lt;- c(\"Art1\", \"Art2\")\nrownames(raw) &lt;- c(\"Ort1\", \"Ort2\", \"Ort3\", \"Ort4\", \"Ort5\", \"Ort6\")\nraw\n##      Art1 Art2\n## Ort1  1.0    0\n## Ort2  2.0    1\n## Ort3  2.5    2\n## Ort4  2.5    4\n## Ort5  1.0    3\n## Ort6  0.5    1\n\n# Originaldaten für Plot separieren\nx1 &lt;- raw[, 1]\ny1 &lt;- raw[, 2]\nz &lt;- c(rep(1:6))\n\n# Plot Abhängigkeit der Arten vom Umweltgradienten\nplot(c(x1, y1) ~ c(z, z), type = \"n\", axes = T, bty = \"l\", las = 1, xlim = c(1, 6),\n    ylim = c(0, 5), xlab = \"Umweltgradient\", ylab = \"Deckung der Arten\")\npoints(x1 ~ z, pch = 21, type = \"b\")\npoints(y1 ~ z, pch = 16, type = \"b\")\ntext(1.5, 1.5, \"Art 1\", col = \"darkgray\")\ntext(1.5, 0.5, \"Art 2\", col = \"darkgray\")\n\n\n\n\n# Daten zentrieren d.h. transformieren so, dass Mittelwert = 0\ncent &lt;- scale(raw, scale = FALSE)\nx2 &lt;- cent[, 1]  # für nachfolgenden Plot speichern\ny2 &lt;- cent[, 2]  # für nachfolgenden Plot speichern\n\n# Daten zusätzlich rotieren PCA zentriert und rotiert Daten\no.pca &lt;- pca(raw)\nx3 &lt;- o.pca$scores[, 1]  # für nachfolgenden Plot speichern\ny3 &lt;- o.pca$scores[, 2]  # für nachfolgenden Plot speichern\n\n# Visualisierung der Schritte im Ordinationsraum\nplot(c(y1, y2, y3) ~ c(x1, x2, x3), type = \"n\", axes = T, bty = \"l\", las = 1, xlim = c(-4,\n    4), ylim = c(-4, 4), xlab = \"Art 1\", ylab = \"Art 2\")\npoints(y1 ~ x1, pch = 21, type = \"b\", col = \"green\", lwd = 2)\ntext(-2.5, 4, \"Originaldaten\", col = \"green\")\npoints(y2 ~ x2, pch = 16, type = \"b\", col = \"red\", lwd = 2)\ntext(-2.5, 3.5, \"Zentriert\", col = \"red\")\npoints(y3 ~ x3, pch = 17, type = \"b\", col = \"blue\", lwd = 2)\ntext(-2.5, 3, \"Zentriert & rotiert\", col = \"blue\")\n\n\n\n\n\n# Durchführung der PCA\no.pca &lt;- pca(raw)\nplot(o.pca)\n\n\n\n\n# Koordinaten im Ordinationsraum\no.pca$scores\n##             PC1         PC2\n## Ort1 -1.9216223 -0.09357697\n## Ort2 -0.6353776 -0.68143293\n## Ort3  0.4762699 -0.80076373\n## Ort4  2.3503705 -0.10237502\n## Ort5  0.8895287  0.95400610\n## Ort6 -1.1591692  0.72414255\n\n# Korrelationen der Variablen mit den Ordinationsachsen\no.pca$loadings\n##            PC1        PC2\n## Art1 0.3491944 -0.9370503\n## Art2 0.9370503  0.3491944\n\n# Erklärte Varianz der Achsen\nE &lt;- o.pca$sdev^2/o.pca$totdev * 100\nE\n## [1] 82.40009 17.59991\n\n# Visualisieren mit prcomp\npca.2 &lt;- prcomp(raw, scale = F)\nsummary(pca.2)\n## Importance of components:\n##                          PC1    PC2\n## Standard deviation     1.548 0.7154\n## Proportion of Variance 0.824 0.1760\n## Cumulative Proportion  0.824 1.0000\nplot(pca.2)  # \n\n\n\nbiplot(pca.2)\n\n\n\n\n# mit vegan\nif (!require(vegan)) {\n    install.packages(\"vegan\")\n}\nlibrary(\"vegan\")\n# Die Funktion rda führt ein PCA aus an wenn nicht Artdaten UND Umweltdaten\n# definiert werden\npca.3 &lt;- rda(raw, scale = FALSE)\n# scores(pca.3, display = c('sites')) scores(pca.3, display = c('species'))\nsummary(pca.3, axes = 0)\n## \n## Call:\n## rda(X = raw, scale = FALSE) \n## \n## Partitioning of variance:\n##               Inertia Proportion\n## Total           2.908          1\n## Unconstrained   2.908          1\n## \n## Eigenvalues, and their contribution to the variance \n## \n## Importance of components:\n##                         PC1    PC2\n## Eigenvalue            2.396 0.5119\n## Proportion Explained  0.824 0.1760\n## Cumulative Proportion 0.824 1.0000\n## \n## Scaling 2 for species and site scores\n## * Species are scaled proportional to eigenvalues\n## * Sites are unscaled: weighted dispersion equal on all dimensions\n## * General scaling constant of scores:\nbiplot(pca.3)\n\n\n\n\n\n# Mit Beispieldaten aus Wildi\nif (!require(dave)) {\n    install.packages(\"dave\")\n}\nlibrary(dave)\n# | eval: false\nstr(sveg)\n## 'data.frame':    63 obs. of  119 variables:\n##  $ Vaccinium.myrtillus       : int  0 1 1 1 1 1 1 0 0 1 ...\n##  $ Vaccinium.uliginosum      : int  0 2 1 0 1 1 1 1 0 0 ...\n##  $ Vaccinium.oxycoccos       : int  2 1 1 1 1 2 1 1 1 1 ...\n##  $ Calluna.vulgaris          : int  1 3 0 0 1 1 0 0 0 0 ...\n##  $ Carex.rostrata            : int  0 0 0 1 0 0 0 2 0 0 ...\n##  $ Carex.lasiocarpa          : int  0 2 2 1 2 2 1 1 1 1 ...\n##  $ Carex.nigra               : int  2 1 0 0 1 1 1 2 1 1 ...\n##  $ Carex.davalliana          : int  0 0 0 0 0 0 0 0 0 1 ...\n##  $ Carex.hostiana            : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Carex.pauciflora          : int  0 0 0 1 0 0 0 0 0 0 ...\n##  $ Carex.echinata            : int  2 0 1 2 1 2 1 1 3 2 ...\n##  $ Carex.panicea             : int  0 0 1 0 1 0 1 0 0 0 ...\n##  $ Trichophorum.caespitosum  : int  2 2 1 0 0 1 0 1 1 1 ...\n##  $ Trichophorum.alpinum      : int  1 0 1 0 0 0 0 0 0 1 ...\n##  $ Eriophorum.vaginatum      : int  0 1 0 1 0 0 0 0 0 0 ...\n##  $ Eriophorum.latifolium     : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Eriophorum.angustifolium  : int  1 2 1 1 1 1 0 1 1 1 ...\n##  $ Tofieldia.calyculata      : int  0 0 0 0 0 1 0 0 0 0 ...\n##  $ Pinguicula.vulgaris       : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Parnassia.palustris       : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Rhynchospora.alba         : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Drosera.rotundifolia      : int  1 0 0 1 1 1 0 1 0 2 ...\n##  $ Arnica.montana            : int  1 0 2 1 1 1 0 1 2 1 ...\n##  $ Dactylorhiza.maculata     : int  1 1 1 1 0 1 1 1 1 1 ...\n##  $ Polytrichum.strictum      : int  1 1 1 1 1 1 2 0 1 0 ...\n##  $ Succisa.pratensis         : int  0 1 1 0 1 1 1 0 0 1 ...\n##  $ Anthoxanthum.odoratum     : int  1 1 1 1 1 1 1 0 1 1 ...\n##  $ Molinia.caerulea          : int  1 2 2 1 1 2 1 1 1 1 ...\n##  $ Gentiana.asclepiadea      : int  1 1 1 1 1 1 0 0 1 1 ...\n##  $ Luzula.multiflora         : int  0 1 1 0 0 0 1 0 0 0 ...\n##  $ Festuca.rubra             : int  0 0 0 0 0 0 1 0 0 1 ...\n##  $ Potentilla.erecta         : int  2 1 1 2 1 2 2 1 1 2 ...\n##  $ Sphagnum.magellanicum     : int  2 0 1 1 3 3 2 1 1 2 ...\n##  $ Sphagnum.papillosum       : int  1 1 1 1 1 0 0 0 0 1 ...\n##  $ Equisetum.palustre        : int  1 1 2 2 1 1 1 3 1 1 ...\n##  $ Aulacomnium.palustre      : int  2 1 1 1 1 1 2 1 1 1 ...\n##  $ Euphrasia.rostkoviana     : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Nardus.stricta            : int  0 1 1 0 1 1 1 1 1 1 ...\n##  $ Carex.pulicaris           : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Climacium.dendroides      : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Carex.pallescens          : int  0 0 1 0 0 0 0 0 0 0 ...\n##  $ Carex.lepidocarpa         : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Briza.media               : int  0 0 1 0 1 0 1 0 0 0 ...\n##  $ Sphagnum.platyphyllum     : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Sphagnum.tenellum         : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Sphagnum.recurvum         : int  5 2 6 6 2 3 5 6 6 3 ...\n##  $ Sphagnum.rubellum         : int  1 1 2 1 2 1 0 0 1 4 ...\n##  $ Sphagnum.centrale         : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Sphagnum.subsecundum      : int  0 0 0 0 0 0 0 0 0 1 ...\n##  $ Juncus.alpinoarticulatus  : int  0 0 0 0 0 0 0 0 0 1 ...\n##  $ Solidago.virgaurea        : int  2 1 1 1 0 1 1 1 0 1 ...\n##  $ Agrostis.canina           : int  1 0 1 1 1 0 0 0 1 1 ...\n##  $ Juncus.effusus            : int  1 0 1 1 1 0 0 0 1 0 ...\n##  $ Frangula.alnus            : int  0 0 1 1 0 0 1 0 0 0 ...\n##  $ Anemone.nemorosa          : int  0 0 0 0 2 1 0 0 0 0 ...\n##  $ Holcus.lanatus            : int  0 0 1 0 1 1 0 0 0 0 ...\n##  $ Drepanocladus.exannulatus : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Homogyne.alpina           : int  0 1 0 0 0 0 0 0 0 0 ...\n##  $ Polygala.vulgaris         : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Trollius.europaeus        : int  0 0 0 0 0 0 1 0 0 0 ...\n##  $ Polygonum.bistorta        : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Lotus.pedunculatus        : int  0 0 0 0 1 0 0 0 0 0 ...\n##  $ Trifolium.pratense        : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Taraxacum.palustre        : int  0 0 1 0 0 0 0 0 0 0 ...\n##  $ Campylium.stellatum       : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Cirsium.palustre          : int  0 0 1 0 0 0 0 0 0 0 ...\n##  $ Lysimachia.vulgaris       : int  0 1 1 1 0 0 0 0 1 1 ...\n##  $ Maianthemum.bifolium      : int  1 1 1 1 1 1 1 1 0 0 ...\n##  $ Equisetum.sylvaticum      : int  0 1 1 1 0 0 0 1 0 0 ...\n##  $ Centaurea.jacea           : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Prunella.grandiflora      : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Viola.palustris           : int  0 0 1 0 0 0 0 0 0 1 ...\n##  $ Angelica.sylvestris       : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Filipendula.ulmaria       : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Thuidium.delicatulum      : int  0 1 0 0 0 1 0 0 0 1 ...\n##  $ Sanguisorba.officinalis   : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Galium.uliginosum         : int  0 0 1 0 0 0 0 0 0 0 ...\n##  $ Epipactis.palustris       : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Pleurozium.schreberi      : int  1 2 1 1 1 0 1 0 0 1 ...\n##  $ Calliergon.stramineum     : int  1 0 1 1 1 0 0 0 1 0 ...\n##  $ Aster.bellidiastrum       : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Hypnum.bambergeri         : int  0 0 0 0 0 0 0 0 0 1 ...\n##  $ Drepanocladus.revolvens   : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Dicranum.bonjeanii        : int  1 0 0 0 0 1 1 0 0 0 ...\n##  $ Dicranum.scoparium        : int  0 1 0 0 0 0 0 0 0 0 ...\n##  $ Hylocomium.splendens      : int  1 1 0 1 1 1 1 0 0 1 ...\n##  $ Rhytidiadelphus.squarrosus: int  1 1 1 1 1 1 1 0 0 1 ...\n##  $ Calliergonella.cuspidata  : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Fissidens.adianthoides    : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Primula.veris             : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Primula.farinosa          : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Hieracium.lachenalii      : int  0 1 0 0 1 1 1 1 1 1 ...\n##  $ Crepis.paludosa           : int  0 0 0 0 1 0 1 0 0 1 ...\n##  $ Leontodon.hispidus        : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Lotus.corniculatus        : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Stachys.officinalis       : int  0 0 0 0 0 0 0 0 0 1 ...\n##  $ Carex.flacca              : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Gentiana.verna            : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Geranium.sylvaticum       : int  0 0 0 0 0 0 0 0 0 0 ...\n##   [list output truncated]\nsummary(sveg)\n##  Vaccinium.myrtillus Vaccinium.uliginosum Vaccinium.oxycoccos Calluna.vulgaris\n##  Min.   :0.0000      Min.   :0.0000       Min.   :0.0000      Min.   :0.0000  \n##  1st Qu.:0.0000      1st Qu.:0.0000       1st Qu.:0.0000      1st Qu.:0.0000  \n##  Median :0.0000      Median :0.0000       Median :0.0000      Median :0.0000  \n##  Mean   :0.1905      Mean   :0.2063       Mean   :0.5079      Mean   :0.3333  \n##  3rd Qu.:0.0000      3rd Qu.:0.0000       3rd Qu.:1.0000      3rd Qu.:1.0000  \n##  Max.   :1.0000      Max.   :2.0000       Max.   :2.0000      Max.   :3.0000  \n##  Carex.rostrata    Carex.lasiocarpa  Carex.nigra     Carex.davalliana\n##  Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n##  1st Qu.:0.00000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n##  Median :0.00000   Median :1.0000   Median :1.0000   Median :0.0000  \n##  Mean   :0.07937   Mean   :0.9365   Mean   :0.6667   Mean   :0.4444  \n##  3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n##  Max.   :2.00000   Max.   :2.0000   Max.   :2.0000   Max.   :2.0000  \n##  Carex.hostiana   Carex.pauciflora  Carex.echinata  Carex.panicea   \n##  Min.   :0.0000   Min.   :0.00000   Min.   :0.000   Min.   :0.0000  \n##  1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.500   1st Qu.:0.0000  \n##  Median :0.0000   Median :0.00000   Median :1.000   Median :1.0000  \n##  Mean   :0.2698   Mean   :0.03175   Mean   :1.317   Mean   :0.6349  \n##  3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:2.000   3rd Qu.:1.0000  \n##  Max.   :2.0000   Max.   :1.00000   Max.   :3.000   Max.   :2.0000  \n##  Trichophorum.caespitosum Trichophorum.alpinum Eriophorum.vaginatum\n##  Min.   :0.000            Min.   :0.0000       Min.   :0.0000      \n##  1st Qu.:1.000            1st Qu.:0.0000       1st Qu.:0.0000      \n##  Median :1.000            Median :0.0000       Median :0.0000      \n##  Mean   :1.413            Mean   :0.4762       Mean   :0.1587      \n##  3rd Qu.:2.000            3rd Qu.:1.0000       3rd Qu.:0.0000      \n##  Max.   :3.000            Max.   :2.0000       Max.   :2.0000      \n##  Eriophorum.latifolium Eriophorum.angustifolium Tofieldia.calyculata\n##  Min.   :0.000         Min.   :0.0000           Min.   :0.0000      \n##  1st Qu.:0.000         1st Qu.:1.0000           1st Qu.:0.0000      \n##  Median :0.000         Median :1.0000           Median :0.0000      \n##  Mean   :0.254         Mean   :0.8413           Mean   :0.3968      \n##  3rd Qu.:0.000         3rd Qu.:1.0000           3rd Qu.:1.0000      \n##  Max.   :2.000         Max.   :2.0000           Max.   :1.0000      \n##  Pinguicula.vulgaris Parnassia.palustris Rhynchospora.alba Drosera.rotundifolia\n##  Min.   :0.000       Min.   :0.0000      Min.   :0.00000   Min.   :0.0000      \n##  1st Qu.:0.000       1st Qu.:0.0000      1st Qu.:0.00000   1st Qu.:0.0000      \n##  Median :0.000       Median :0.0000      Median :0.00000   Median :1.0000      \n##  Mean   :0.381       Mean   :0.3968      Mean   :0.03175   Mean   :0.8095      \n##  3rd Qu.:1.000       3rd Qu.:1.0000      3rd Qu.:0.00000   3rd Qu.:1.0000      \n##  Max.   :2.000       Max.   :1.0000      Max.   :1.00000   Max.   :2.0000      \n##  Arnica.montana   Dactylorhiza.maculata Polytrichum.strictum Succisa.pratensis\n##  Min.   :0.0000   Min.   :0.000         Min.   :0.0000       Min.   :0.0000   \n##  1st Qu.:0.0000   1st Qu.:0.000         1st Qu.:0.0000       1st Qu.:0.0000   \n##  Median :0.0000   Median :1.000         Median :0.0000       Median :1.0000   \n##  Mean   :0.6984   Mean   :0.746         Mean   :0.4127       Mean   :0.7143   \n##  3rd Qu.:1.0000   3rd Qu.:1.000         3rd Qu.:1.0000       3rd Qu.:1.0000   \n##  Max.   :3.0000   Max.   :2.000         Max.   :3.0000       Max.   :1.0000   \n##  Anthoxanthum.odoratum Molinia.caerulea Gentiana.asclepiadea Luzula.multiflora\n##  Min.   :0.0000        Min.   :0.00     Min.   :0.0000       Min.   :0.0000   \n##  1st Qu.:0.0000        1st Qu.:1.00     1st Qu.:1.0000       1st Qu.:0.0000   \n##  Median :1.0000        Median :1.00     Median :1.0000       Median :0.0000   \n##  Mean   :0.6349        Mean   :1.19     Mean   :0.7937       Mean   :0.1905   \n##  3rd Qu.:1.0000        3rd Qu.:1.00     3rd Qu.:1.0000       3rd Qu.:0.0000   \n##  Max.   :1.0000        Max.   :2.00     Max.   :2.0000       Max.   :1.0000   \n##  Festuca.rubra    Potentilla.erecta Sphagnum.magellanicum Sphagnum.papillosum\n##  Min.   :0.0000   Min.   :0.000     Min.   :0.000         Min.   :0.0000     \n##  1st Qu.:0.0000   1st Qu.:1.000     1st Qu.:0.000         1st Qu.:0.0000     \n##  Median :1.0000   Median :1.000     Median :1.000         Median :1.0000     \n##  Mean   :0.5397   Mean   :1.286     Mean   :1.016         Mean   :0.9206     \n##  3rd Qu.:1.0000   3rd Qu.:2.000     3rd Qu.:1.000         3rd Qu.:1.0000     \n##  Max.   :1.0000   Max.   :2.000     Max.   :5.000         Max.   :5.0000     \n##  Equisetum.palustre Aulacomnium.palustre Euphrasia.rostkoviana Nardus.stricta  \n##  Min.   :0.000      Min.   :0.000        Min.   :0.0000        Min.   :0.0000  \n##  1st Qu.:1.000      1st Qu.:1.000        1st Qu.:0.0000        1st Qu.:0.0000  \n##  Median :1.000      Median :1.000        Median :0.0000        Median :1.0000  \n##  Mean   :1.079      Mean   :1.095        Mean   :0.3968        Mean   :0.6349  \n##  3rd Qu.:1.000      3rd Qu.:1.000        3rd Qu.:1.0000        3rd Qu.:1.0000  \n##  Max.   :3.000      Max.   :3.000        Max.   :2.0000        Max.   :2.0000  \n##  Carex.pulicaris Climacium.dendroides Carex.pallescens Carex.lepidocarpa\n##  Min.   :0.000   Min.   :0.0000       Min.   :0.0000   Min.   :0.000    \n##  1st Qu.:0.000   1st Qu.:0.0000       1st Qu.:0.0000   1st Qu.:0.000    \n##  Median :0.000   Median :0.0000       Median :0.0000   Median :0.000    \n##  Mean   :0.381   Mean   :0.5397       Mean   :0.1746   Mean   :0.127    \n##  3rd Qu.:1.000   3rd Qu.:1.0000       3rd Qu.:0.0000   3rd Qu.:0.000    \n##  Max.   :2.000   Max.   :3.0000       Max.   :1.0000   Max.   :1.000    \n##   Briza.media    Sphagnum.platyphyllum Sphagnum.tenellum Sphagnum.recurvum\n##  Min.   :0.000   Min.   :0.0000        Min.   :0.00000   Min.   :0.000    \n##  1st Qu.:0.000   1st Qu.:0.0000        1st Qu.:0.00000   1st Qu.:0.000    \n##  Median :0.000   Median :0.0000        Median :0.00000   Median :1.000    \n##  Mean   :0.381   Mean   :0.2381        Mean   :0.06349   Mean   :1.571    \n##  3rd Qu.:1.000   3rd Qu.:0.0000        3rd Qu.:0.00000   3rd Qu.:2.500    \n##  Max.   :1.000   Max.   :3.0000        Max.   :1.00000   Max.   :6.000    \n##  Sphagnum.rubellum Sphagnum.centrale Sphagnum.subsecundum\n##  Min.   :0.000     Min.   :0.00000   Min.   :0.0000      \n##  1st Qu.:0.000     1st Qu.:0.00000   1st Qu.:0.0000      \n##  Median :1.000     Median :0.00000   Median :0.0000      \n##  Mean   :1.222     Mean   :0.03175   Mean   :0.8254      \n##  3rd Qu.:2.000     3rd Qu.:0.00000   3rd Qu.:1.0000      \n##  Max.   :6.000     Max.   :1.00000   Max.   :6.0000      \n##  Juncus.alpinoarticulatus Solidago.virgaurea Agrostis.canina  Juncus.effusus  \n##  Min.   :0.0000           Min.   :0.0000     Min.   :0.0000   Min.   :0.0000  \n##  1st Qu.:0.0000           1st Qu.:0.0000     1st Qu.:0.0000   1st Qu.:0.0000  \n##  Median :0.0000           Median :0.0000     Median :0.0000   Median :0.0000  \n##  Mean   :0.2857           Mean   :0.2381     Mean   :0.3968   Mean   :0.1429  \n##  3rd Qu.:1.0000           3rd Qu.:0.0000     3rd Qu.:1.0000   3rd Qu.:0.0000  \n##  Max.   :1.0000           Max.   :2.0000     Max.   :1.0000   Max.   :1.0000  \n##  Frangula.alnus   Anemone.nemorosa Holcus.lanatus   Drepanocladus.exannulatus\n##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000          \n##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000          \n##  Median :0.0000   Median :1.0000   Median :0.0000   Median :0.00000          \n##  Mean   :0.4921   Mean   :0.6032   Mean   :0.2063   Mean   :0.04762          \n##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.00000          \n##  Max.   :2.0000   Max.   :2.0000   Max.   :1.0000   Max.   :1.00000          \n##  Homogyne.alpina   Polygala.vulgaris Trollius.europaeus Polygonum.bistorta\n##  Min.   :0.00000   Min.   :0.0000    Min.   :0.0000     Min.   :0.00000   \n##  1st Qu.:0.00000   1st Qu.:0.0000    1st Qu.:0.0000     1st Qu.:0.00000   \n##  Median :0.00000   Median :0.0000    Median :0.0000     Median :0.00000   \n##  Mean   :0.04762   Mean   :0.1905    Mean   :0.3333     Mean   :0.06349   \n##  3rd Qu.:0.00000   3rd Qu.:0.0000    3rd Qu.:1.0000     3rd Qu.:0.00000   \n##  Max.   :2.00000   Max.   :1.0000    Max.   :1.0000     Max.   :2.00000   \n##  Lotus.pedunculatus Trifolium.pratense Taraxacum.palustre Campylium.stellatum\n##  Min.   :0.00000    Min.   :0.0000     Min.   :0.0000     Min.   :0.0000     \n##  1st Qu.:0.00000    1st Qu.:0.0000     1st Qu.:0.0000     1st Qu.:0.0000     \n##  Median :0.00000    Median :0.0000     Median :0.0000     Median :0.0000     \n##  Mean   :0.06349    Mean   :0.2063     Mean   :0.3333     Mean   :0.5079     \n##  3rd Qu.:0.00000    3rd Qu.:0.0000     3rd Qu.:1.0000     3rd Qu.:1.0000     \n##  Max.   :2.00000    Max.   :1.0000     Max.   :1.0000     Max.   :5.0000     \n##  Cirsium.palustre Lysimachia.vulgaris Maianthemum.bifolium Equisetum.sylvaticum\n##  Min.   :0.0000   Min.   :0.0000      Min.   :0.000        Min.   :0.0000      \n##  1st Qu.:0.0000   1st Qu.:0.0000      1st Qu.:0.000        1st Qu.:0.0000      \n##  Median :0.0000   Median :0.0000      Median :1.000        Median :0.0000      \n##  Mean   :0.1429   Mean   :0.4603      Mean   :0.619        Mean   :0.1587      \n##  3rd Qu.:0.0000   3rd Qu.:1.0000      3rd Qu.:1.000        3rd Qu.:0.0000      \n##  Max.   :1.0000   Max.   :1.0000      Max.   :2.000        Max.   :1.0000      \n##  Centaurea.jacea  Prunella.grandiflora Viola.palustris  Angelica.sylvestris\n##  Min.   :0.0000   Min.   :0.0000       Min.   :0.0000   Min.   :0.00000    \n##  1st Qu.:0.0000   1st Qu.:0.0000       1st Qu.:0.0000   1st Qu.:0.00000    \n##  Median :0.0000   Median :0.0000       Median :0.0000   Median :0.00000    \n##  Mean   :0.2857   Mean   :0.2063       Mean   :0.3333   Mean   :0.03175    \n##  3rd Qu.:0.5000   3rd Qu.:0.0000       3rd Qu.:1.0000   3rd Qu.:0.00000    \n##  Max.   :2.0000   Max.   :1.0000       Max.   :2.0000   Max.   :1.00000    \n##  Filipendula.ulmaria Thuidium.delicatulum Sanguisorba.officinalis\n##  Min.   :0.0000      Min.   :0.0000       Min.   :0.00000        \n##  1st Qu.:0.0000      1st Qu.:0.0000       1st Qu.:0.00000        \n##  Median :0.0000      Median :1.0000       Median :0.00000        \n##  Mean   :0.1111      Mean   :0.6508       Mean   :0.04762        \n##  3rd Qu.:0.0000      3rd Qu.:1.0000       3rd Qu.:0.00000        \n##  Max.   :2.0000      Max.   :4.0000       Max.   :1.00000        \n##  Galium.uliginosum Epipactis.palustris Pleurozium.schreberi\n##  Min.   :0.0000    Min.   :0.0000      Min.   :0.000       \n##  1st Qu.:0.0000    1st Qu.:0.0000      1st Qu.:0.000       \n##  Median :0.0000    Median :0.0000      Median :0.000       \n##  Mean   :0.2857    Mean   :0.1905      Mean   :0.619       \n##  3rd Qu.:1.0000    3rd Qu.:0.0000      3rd Qu.:1.000       \n##  Max.   :1.0000    Max.   :1.0000      Max.   :6.000       \n##  Calliergon.stramineum Aster.bellidiastrum Hypnum.bambergeri\n##  Min.   :0.000         Min.   :0.0000      Min.   :0.0000   \n##  1st Qu.:0.000         1st Qu.:0.0000      1st Qu.:0.0000   \n##  Median :1.000         Median :0.0000      Median :0.0000   \n##  Mean   :0.619         Mean   :0.2698      Mean   :0.4762   \n##  3rd Qu.:1.000         3rd Qu.:0.0000      3rd Qu.:1.0000   \n##  Max.   :2.000         Max.   :2.0000      Max.   :5.0000   \n##  Drepanocladus.revolvens Dicranum.bonjeanii Dicranum.scoparium\n##  Min.   :0.0000          Min.   :0.0000     Min.   :0.00000   \n##  1st Qu.:0.0000          1st Qu.:0.0000     1st Qu.:0.00000   \n##  Median :0.0000          Median :0.0000     Median :0.00000   \n##  Mean   :0.1587          Mean   :0.4127     Mean   :0.07937   \n##  3rd Qu.:0.0000          3rd Qu.:1.0000     3rd Qu.:0.00000   \n##  Max.   :3.0000          Max.   :1.0000     Max.   :1.00000   \n##  Hylocomium.splendens Rhytidiadelphus.squarrosus Calliergonella.cuspidata\n##  Min.   :0.000        Min.   :0.0000             Min.   :0.000           \n##  1st Qu.:0.000        1st Qu.:0.0000             1st Qu.:0.000           \n##  Median :1.000        Median :0.0000             Median :0.000           \n##  Mean   :1.063        Mean   :0.4444             Mean   :0.127           \n##  3rd Qu.:1.000        3rd Qu.:1.0000             3rd Qu.:0.000           \n##  Max.   :5.000        Max.   :2.0000             Max.   :1.000           \n##  Fissidens.adianthoides Primula.veris   Primula.farinosa  Hieracium.lachenalii\n##  Min.   :0.000          Min.   :0.000   Min.   :0.00000   Min.   :0.0000      \n##  1st Qu.:0.000          1st Qu.:0.000   1st Qu.:0.00000   1st Qu.:0.0000      \n##  Median :0.000          Median :0.000   Median :0.00000   Median :1.0000      \n##  Mean   :0.127          Mean   :0.127   Mean   :0.09524   Mean   :0.6508      \n##  3rd Qu.:0.000          3rd Qu.:0.000   3rd Qu.:0.00000   3rd Qu.:1.0000      \n##  Max.   :1.000          Max.   :1.000   Max.   :1.00000   Max.   :1.0000      \n##  Crepis.paludosa  Leontodon.hispidus Lotus.corniculatus Stachys.officinalis\n##  Min.   :0.0000   Min.   :0.0000     Min.   :0.0000     Min.   :0.0000     \n##  1st Qu.:0.0000   1st Qu.:0.0000     1st Qu.:0.0000     1st Qu.:0.0000     \n##  Median :0.0000   Median :0.0000     Median :0.0000     Median :0.0000     \n##  Mean   :0.2381   Mean   :0.2063     Mean   :0.1905     Mean   :0.1587     \n##  3rd Qu.:0.0000   3rd Qu.:0.0000     3rd Qu.:0.0000     3rd Qu.:0.0000     \n##  Max.   :1.0000   Max.   :1.0000     Max.   :1.0000     Max.   :1.0000     \n##   Carex.flacca    Gentiana.verna    Geranium.sylvaticum Ranunculus.nemorosus\n##  Min.   :0.0000   Min.   :0.00000   Min.   :0.00000     Min.   :0.000       \n##  1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000     1st Qu.:0.000       \n##  Median :0.0000   Median :0.00000   Median :0.00000     Median :0.000       \n##  Mean   :0.1111   Mean   :0.03175   Mean   :0.04762     Mean   :0.381       \n##  3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000     3rd Qu.:1.000       \n##  Max.   :1.0000   Max.   :1.00000   Max.   :1.00000     Max.   :1.000       \n##  Linum.catharticum Cirsium.oleraceum Colchicum.autumnale Polygala.amarella\n##  Min.   :0.000     Min.   :0.00000   Min.   :0.00000     Min.   :0.00000  \n##  1st Qu.:0.000     1st Qu.:0.00000   1st Qu.:0.00000     1st Qu.:0.00000  \n##  Median :0.000     Median :0.00000   Median :0.00000     Median :0.00000  \n##  Mean   :0.127     Mean   :0.03175   Mean   :0.03175     Mean   :0.04762  \n##  3rd Qu.:0.000     3rd Qu.:0.00000   3rd Qu.:0.00000     3rd Qu.:0.00000  \n##  Max.   :1.000     Max.   :1.00000   Max.   :1.00000     Max.   :1.00000  \n##  Polygala.serpyllifolia Equisetum.arvense Deschampsia.cespitosa\n##  Min.   :0.00000        Min.   :0.0000    Min.   :0.00000      \n##  1st Qu.:0.00000        1st Qu.:0.0000    1st Qu.:0.00000      \n##  Median :0.00000        Median :0.0000    Median :0.00000      \n##  Mean   :0.03175        Mean   :0.2063    Mean   :0.06349      \n##  3rd Qu.:0.00000        3rd Qu.:0.0000    3rd Qu.:0.00000      \n##  Max.   :1.00000        Max.   :1.0000    Max.   :1.00000      \n##  Rhinanthus.minor  Gymnadenia.conopsea Melampyrum.pratense  Salix.spec.     \n##  Min.   :0.00000   Min.   :0.0000      Min.   :0.00000     Min.   :0.00000  \n##  1st Qu.:0.00000   1st Qu.:0.0000      1st Qu.:0.00000     1st Qu.:0.00000  \n##  Median :0.00000   Median :0.0000      Median :0.00000     Median :0.00000  \n##  Mean   :0.06349   Mean   :0.1429      Mean   :0.03175     Mean   :0.03175  \n##  3rd Qu.:0.00000   3rd Qu.:0.0000      3rd Qu.:0.00000     3rd Qu.:0.00000  \n##  Max.   :1.00000   Max.   :1.0000      Max.   :1.00000     Max.   :1.00000  \n##  Caltha.palustris  Dactylorhiza.fistulosa Platanthera.bifolia\n##  Min.   :0.00000   Min.   :0.00000        Min.   :0.00000    \n##  1st Qu.:0.00000   1st Qu.:0.00000        1st Qu.:0.00000    \n##  Median :0.00000   Median :0.00000        Median :0.00000    \n##  Mean   :0.04762   Mean   :0.06349        Mean   :0.09524    \n##  3rd Qu.:0.00000   3rd Qu.:0.00000        3rd Qu.:0.00000    \n##  Max.   :1.00000   Max.   :1.00000        Max.   :1.00000    \n##  Acer.pseudoplatanus Danthonia.decumbens Drepanocladus.spec. Tomentypnum.nitens\n##  Min.   :0.00000     Min.   :0.00000     Min.   :0.00000     Min.   :0.00000   \n##  1st Qu.:0.00000     1st Qu.:0.00000     1st Qu.:0.00000     1st Qu.:0.00000   \n##  Median :0.00000     Median :0.00000     Median :0.00000     Median :0.00000   \n##  Mean   :0.06349     Mean   :0.06349     Mean   :0.03175     Mean   :0.03175   \n##  3rd Qu.:0.00000     3rd Qu.:0.00000     3rd Qu.:0.00000     3rd Qu.:0.00000   \n##  Max.   :1.00000     Max.   :1.00000     Max.   :1.00000     Max.   :1.00000   \n##  Ctenidium.molluscum\n##  Min.   :0.00000    \n##  1st Qu.:0.00000    \n##  Median :0.00000    \n##  Mean   :0.03175    \n##  3rd Qu.:0.00000    \n##  Max.   :1.00000\nnames(sveg)\n##   [1] \"Vaccinium.myrtillus\"        \"Vaccinium.uliginosum\"      \n##   [3] \"Vaccinium.oxycoccos\"        \"Calluna.vulgaris\"          \n##   [5] \"Carex.rostrata\"             \"Carex.lasiocarpa\"          \n##   [7] \"Carex.nigra\"                \"Carex.davalliana\"          \n##   [9] \"Carex.hostiana\"             \"Carex.pauciflora\"          \n##  [11] \"Carex.echinata\"             \"Carex.panicea\"             \n##  [13] \"Trichophorum.caespitosum\"   \"Trichophorum.alpinum\"      \n##  [15] \"Eriophorum.vaginatum\"       \"Eriophorum.latifolium\"     \n##  [17] \"Eriophorum.angustifolium\"   \"Tofieldia.calyculata\"      \n##  [19] \"Pinguicula.vulgaris\"        \"Parnassia.palustris\"       \n##  [21] \"Rhynchospora.alba\"          \"Drosera.rotundifolia\"      \n##  [23] \"Arnica.montana\"             \"Dactylorhiza.maculata\"     \n##  [25] \"Polytrichum.strictum\"       \"Succisa.pratensis\"         \n##  [27] \"Anthoxanthum.odoratum\"      \"Molinia.caerulea\"          \n##  [29] \"Gentiana.asclepiadea\"       \"Luzula.multiflora\"         \n##  [31] \"Festuca.rubra\"              \"Potentilla.erecta\"         \n##  [33] \"Sphagnum.magellanicum\"      \"Sphagnum.papillosum\"       \n##  [35] \"Equisetum.palustre\"         \"Aulacomnium.palustre\"      \n##  [37] \"Euphrasia.rostkoviana\"      \"Nardus.stricta\"            \n##  [39] \"Carex.pulicaris\"            \"Climacium.dendroides\"      \n##  [41] \"Carex.pallescens\"           \"Carex.lepidocarpa\"         \n##  [43] \"Briza.media\"                \"Sphagnum.platyphyllum\"     \n##  [45] \"Sphagnum.tenellum\"          \"Sphagnum.recurvum\"         \n##  [47] \"Sphagnum.rubellum\"          \"Sphagnum.centrale\"         \n##  [49] \"Sphagnum.subsecundum\"       \"Juncus.alpinoarticulatus\"  \n##  [51] \"Solidago.virgaurea\"         \"Agrostis.canina\"           \n##  [53] \"Juncus.effusus\"             \"Frangula.alnus\"            \n##  [55] \"Anemone.nemorosa\"           \"Holcus.lanatus\"            \n##  [57] \"Drepanocladus.exannulatus\"  \"Homogyne.alpina\"           \n##  [59] \"Polygala.vulgaris\"          \"Trollius.europaeus\"        \n##  [61] \"Polygonum.bistorta\"         \"Lotus.pedunculatus\"        \n##  [63] \"Trifolium.pratense\"         \"Taraxacum.palustre\"        \n##  [65] \"Campylium.stellatum\"        \"Cirsium.palustre\"          \n##  [67] \"Lysimachia.vulgaris\"        \"Maianthemum.bifolium\"      \n##  [69] \"Equisetum.sylvaticum\"       \"Centaurea.jacea\"           \n##  [71] \"Prunella.grandiflora\"       \"Viola.palustris\"           \n##  [73] \"Angelica.sylvestris\"        \"Filipendula.ulmaria\"       \n##  [75] \"Thuidium.delicatulum\"       \"Sanguisorba.officinalis\"   \n##  [77] \"Galium.uliginosum\"          \"Epipactis.palustris\"       \n##  [79] \"Pleurozium.schreberi\"       \"Calliergon.stramineum\"     \n##  [81] \"Aster.bellidiastrum\"        \"Hypnum.bambergeri\"         \n##  [83] \"Drepanocladus.revolvens\"    \"Dicranum.bonjeanii\"        \n##  [85] \"Dicranum.scoparium\"         \"Hylocomium.splendens\"      \n##  [87] \"Rhytidiadelphus.squarrosus\" \"Calliergonella.cuspidata\"  \n##  [89] \"Fissidens.adianthoides\"     \"Primula.veris\"             \n##  [91] \"Primula.farinosa\"           \"Hieracium.lachenalii\"      \n##  [93] \"Crepis.paludosa\"            \"Leontodon.hispidus\"        \n##  [95] \"Lotus.corniculatus\"         \"Stachys.officinalis\"       \n##  [97] \"Carex.flacca\"               \"Gentiana.verna\"            \n##  [99] \"Geranium.sylvaticum\"        \"Ranunculus.nemorosus\"      \n## [101] \"Linum.catharticum\"          \"Cirsium.oleraceum\"         \n## [103] \"Colchicum.autumnale\"        \"Polygala.amarella\"         \n## [105] \"Polygala.serpyllifolia\"     \"Equisetum.arvense\"         \n## [107] \"Deschampsia.cespitosa\"      \"Rhinanthus.minor\"          \n## [109] \"Gymnadenia.conopsea\"        \"Melampyrum.pratense\"       \n## [111] \"Salix.spec.\"                \"Caltha.palustris\"          \n## [113] \"Dactylorhiza.fistulosa\"     \"Platanthera.bifolia\"       \n## [115] \"Acer.pseudoplatanus\"        \"Danthonia.decumbens\"       \n## [117] \"Drepanocladus.spec.\"        \"Tomentypnum.nitens\"        \n## [119] \"Ctenidium.molluscum\"\n\n\n# PCA: Deckungen Wurzeltransformiert, cor=TRUE erzwingt Nutzung der\n# Korrelationsmatrix\npca.5 &lt;- pca(sveg^0.25, cor = TRUE)\n\n\n# Koordinaten im Ordinationsraum\npca.5$scores\n\n# Korrelationen der Variablen mit den Ordinationsachsen\npca.5$loadings\n\n\n\n# Erklärte Varianz der Achsen in Prozent (sdev ist die Wurzel daraus)\nE &lt;- pca.5$sdev^2/pca.5$totdev * 100\nE\n##  [1] 2.061885e+01 8.098205e+00 6.070537e+00 3.666650e+00 3.322363e+00\n##  [6] 3.128942e+00 3.003875e+00 2.634636e+00 2.605558e+00 2.449637e+00\n## [11] 2.339344e+00 2.265430e+00 2.116464e+00 2.046578e+00 1.969912e+00\n## [16] 1.871020e+00 1.777063e+00 1.693483e+00 1.524015e+00 1.503332e+00\n## [21] 1.434245e+00 1.378271e+00 1.329404e+00 1.291336e+00 1.251895e+00\n## [26] 1.186157e+00 1.109340e+00 1.068661e+00 1.044385e+00 9.891552e-01\n## [31] 9.764586e-01 8.869747e-01 8.451212e-01 8.049318e-01 7.603242e-01\n## [36] 7.311274e-01 6.945830e-01 6.339064e-01 6.063542e-01 5.502527e-01\n## [41] 5.411059e-01 4.956931e-01 4.795188e-01 4.601244e-01 3.936176e-01\n## [46] 3.477631e-01 3.402128e-01 3.165971e-01 2.951856e-01 2.728882e-01\n## [51] 2.635725e-01 2.233500e-01 2.125542e-01 1.989449e-01 1.681852e-01\n## [56] 1.555571e-01 1.485298e-01 1.271079e-01 9.164615e-02 7.880113e-02\n## [61] 5.913306e-02 5.113452e-02 4.066351e-30\nE[1:5]\n## [1] 20.618848  8.098205  6.070537  3.666650  3.322363\nplot(pca.5)\n\n\n\n# PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nplot(pca.5$scores[, 1], pca.5$scores[, 2], type = \"n\", asp = 1, xlab = \"PC1\", ylab = \"PC2\")\npoints(pca.5$scores[, 1], pca.5$scores[, 2], pch = 18)\n\n\n\n\n# Subjektive Auswahl von Arten zur Darstellung\nsel.sp &lt;- c(3, 11, 23, 39, 46, 72, 77, 96)\nsnames &lt;- names(sveg[, sel.sp])\nsnames\n## [1] \"Vaccinium.oxycoccos\" \"Carex.echinata\"      \"Arnica.montana\"     \n## [4] \"Carex.pulicaris\"     \"Sphagnum.recurvum\"   \"Viola.palustris\"    \n## [7] \"Galium.uliginosum\"   \"Stachys.officinalis\"\n\n# PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h)\nx &lt;- pca.5$loadings[, 1]\ny &lt;- pca.5$loadings[, 2]\nplot(x, y, type = \"n\", asp = 1)\narrows(0, 0, x[sel.sp], y[sel.sp], length = 0.08)\ntext(x[sel.sp], y[sel.sp], snames, pos = 1, cex = 0.6)\n\n\n\n\n# Mit vegan\npca.6 &lt;- rda(sveg^0.25, scale = TRUE)\n# Erklärte Varianz der Achsen\nsummary(pca.6, axes = 0)\n## \n## Call:\n## rda(X = sveg^0.25, scale = TRUE) \n## \n## Partitioning of correlations:\n##               Inertia Proportion\n## Total             119          1\n## Unconstrained     119          1\n## \n## Eigenvalues, and their contribution to the correlations \n## \n## Importance of components:\n##                           PC1     PC2     PC3     PC4     PC5     PC6     PC7\n## Eigenvalue            24.5364 9.63686 7.22394 4.36331 3.95361 3.72344 3.57461\n## Proportion Explained   0.2062 0.08098 0.06071 0.03667 0.03322 0.03129 0.03004\n## Cumulative Proportion  0.2062 0.28717 0.34788 0.38454 0.41777 0.44906 0.47909\n##                           PC8     PC9   PC10    PC11    PC12    PC13    PC14\n## Eigenvalue            3.13522 3.10061 2.9151 2.78382 2.69586 2.51859 2.43543\n## Proportion Explained  0.02635 0.02606 0.0245 0.02339 0.02265 0.02116 0.02047\n## Cumulative Proportion 0.50544 0.53150 0.5560 0.57939 0.60204 0.62320 0.64367\n##                         PC15    PC16    PC17    PC18    PC19    PC20    PC21\n## Eigenvalue            2.3442 2.22651 2.11470 2.01524 1.81358 1.78896 1.70675\n## Proportion Explained  0.0197 0.01871 0.01777 0.01693 0.01524 0.01503 0.01434\n## Cumulative Proportion 0.6634 0.68208 0.69985 0.71679 0.73203 0.74706 0.76140\n##                          PC22    PC23    PC24    PC25    PC26    PC27    PC28\n## Eigenvalue            1.64014 1.58199 1.53669 1.48976 1.41153 1.32011 1.27171\n## Proportion Explained  0.01378 0.01329 0.01291 0.01252 0.01186 0.01109 0.01069\n## Cumulative Proportion 0.77518 0.78848 0.80139 0.81391 0.82577 0.83687 0.84755\n##                          PC29     PC30     PC31    PC32     PC33     PC34\n## Eigenvalue            1.24282 1.177095 1.161986 1.05550 1.005694 0.957869\n## Proportion Explained  0.01044 0.009892 0.009765 0.00887 0.008451 0.008049\n## Cumulative Proportion 0.85800 0.867887 0.877652 0.88652 0.894973 0.903022\n##                           PC35     PC36     PC37     PC38     PC39     PC40\n## Eigenvalue            0.904786 0.870042 0.826554 0.754349 0.721562 0.654801\n## Proportion Explained  0.007603 0.007311 0.006946 0.006339 0.006064 0.005503\n## Cumulative Proportion 0.910626 0.917937 0.924883 0.931222 0.937285 0.942788\n##                           PC41     PC42     PC43     PC44     PC45     PC46\n## Eigenvalue            0.643916 0.589875 0.570627 0.547548 0.468405 0.413838\n## Proportion Explained  0.005411 0.004957 0.004795 0.004601 0.003936 0.003478\n## Cumulative Proportion 0.948199 0.953156 0.957951 0.962552 0.966488 0.969966\n##                           PC47     PC48     PC49     PC50     PC51     PC52\n## Eigenvalue            0.404853 0.376750 0.351271 0.324737 0.313651 0.265787\n## Proportion Explained  0.003402 0.003166 0.002952 0.002729 0.002636 0.002234\n## Cumulative Proportion 0.973368 0.976534 0.979486 0.982215 0.984851 0.987084\n##                           PC53     PC54     PC55     PC56     PC57     PC58\n## Eigenvalue            0.252939 0.236744 0.200140 0.185113 0.176750 0.151258\n## Proportion Explained  0.002126 0.001989 0.001682 0.001556 0.001485 0.001271\n## Cumulative Proportion 0.989210 0.991199 0.992881 0.994436 0.995922 0.997193\n##                            PC59     PC60      PC61      PC62\n## Eigenvalue            0.1090589 0.093773 0.0703683 0.0608501\n## Proportion Explained  0.0009165 0.000788 0.0005913 0.0005113\n## Cumulative Proportion 0.9981093 0.998897 0.9994887 1.0000000\n## \n## Scaling 2 for species and site scores\n## * Species are scaled proportional to eigenvalues\n## * Sites are unscaled: weighted dispersion equal on all dimensions\n## * General scaling constant of scores:\n# PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nbiplot(pca.6, display = \"sites\", type = \"points\", scaling = 1)\n\n\n\n# Subjektive Auswahl von Arten zur Darstellung\nsel.sp &lt;- c(3, 11, 23, 39, 46, 72, 77, 96)\nsnames &lt;- names(sveg[, sel.sp])\nsnames\n## [1] \"Vaccinium.oxycoccos\" \"Carex.echinata\"      \"Arnica.montana\"     \n## [4] \"Carex.pulicaris\"     \"Sphagnum.recurvum\"   \"Viola.palustris\"    \n## [7] \"Galium.uliginosum\"   \"Stachys.officinalis\"\n# PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h)\nscores &lt;- scores(pca.6, display = \"species\")\nx &lt;- scores[, 1]\ny &lt;- scores[, 2]\nplot(x, y, type = \"n\", asp = 1)\narrows(0, 0, x[sel.sp], y[sel.sp], length = 0.08)\ntext(x[sel.sp], y[sel.sp], snames, pos = 1, cex = 0.6)\n\n\n\n\n# Mit angepassten Achsen\nplot(x, y, type = \"n\", asp = 1, xlim = c(-1, 1), ylim = c(-0.6, 0.6))\narrows(0, 0, x[sel.sp], y[sel.sp], length = 0.08)\ntext(x[sel.sp], y[sel.sp], snames, pos = 1, cex = 0.6)\n\n\n\n\n\n\nCA\n\n\nca.1 &lt;- cca(sveg^0.5)\n# Arten (o) und Communities (+) plotten\nplot(ca.1)\n\n\n\n\n# Nur Arten plotten\nplot(ca.1, display = \"species\", type = \"points\")\n\n\n\n\n\n# Anteilige Varianz, die durch die ersten beiden Achsen erklärt wird\nca.1$CA$eig[1:2]/sum(ca.1$CA$eig)\n##       CA1       CA2 \n## 0.1938717 0.0784178\n\nsummary(eigenvals(ca.1))\n## Importance of components:\n##                          CA1     CA2     CA3     CA4     CA5     CA6     CA7\n## Eigenvalue            0.4248 0.17182 0.12995 0.09102 0.07954 0.07274 0.06705\n## Proportion Explained  0.1939 0.07842 0.05931 0.04154 0.03630 0.03320 0.03060\n## Cumulative Proportion 0.1939 0.27229 0.33160 0.37314 0.40944 0.44264 0.47324\n##                           CA8     CA9    CA10    CA11    CA12    CA13   CA14\n## Eigenvalue            0.06245 0.05811 0.05348 0.05261 0.05133 0.04868 0.0480\n## Proportion Explained  0.02850 0.02652 0.02441 0.02401 0.02343 0.02222 0.0219\n## Cumulative Proportion 0.50174 0.52826 0.55267 0.57668 0.60010 0.62232 0.6442\n##                          CA15    CA16    CA17    CA18    CA19    CA20    CA21\n## Eigenvalue            0.04421 0.04279 0.03913 0.03752 0.03699 0.03412 0.03309\n## Proportion Explained  0.02018 0.01953 0.01786 0.01712 0.01688 0.01557 0.01510\n## Cumulative Proportion 0.66440 0.68393 0.70179 0.71892 0.73580 0.75137 0.76647\n##                          CA22    CA23    CA24    CA25    CA26    CA27    CA28\n## Eigenvalue            0.03253 0.03033 0.02963 0.02718 0.02621 0.02486 0.02372\n## Proportion Explained  0.01485 0.01384 0.01352 0.01241 0.01196 0.01135 0.01083\n## Cumulative Proportion 0.78132 0.79516 0.80869 0.82109 0.83305 0.84440 0.85523\n##                          CA29     CA30     CA31     CA32     CA33     CA34\n## Eigenvalue            0.02262 0.021397 0.020274 0.018805 0.018216 0.017737\n## Proportion Explained  0.01032 0.009765 0.009253 0.008582 0.008314 0.008095\n## Cumulative Proportion 0.86555 0.875318 0.884571 0.893153 0.901467 0.909561\n##                           CA35    CA36     CA37     CA38     CA39     CA40\n## Eigenvalue            0.016855 0.01422 0.014044 0.013002 0.011367 0.011185\n## Proportion Explained  0.007693 0.00649 0.006409 0.005934 0.005188 0.005105\n## Cumulative Proportion 0.917254 0.92374 0.930153 0.936087 0.941275 0.946379\n##                           CA41     CA42     CA43     CA44     CA45     CA46\n## Eigenvalue            0.010417 0.010172 0.009513 0.009183 0.008162 0.007993\n## Proportion Explained  0.004754 0.004643 0.004342 0.004191 0.003725 0.003648\n## Cumulative Proportion 0.951133 0.955776 0.960118 0.964308 0.968033 0.971681\n##                           CA47     CA48     CA49     CA50    CA51     CA52\n## Eigenvalue            0.006900 0.006684 0.006108 0.005493 0.00515 0.004995\n## Proportion Explained  0.003149 0.003051 0.002788 0.002507 0.00235 0.002279\n## Cumulative Proportion 0.974830 0.977881 0.980668 0.983176 0.98553 0.987805\n##                           CA53     CA54     CA55     CA56     CA57     CA58\n## Eigenvalue            0.004426 0.004011 0.003517 0.003455 0.003059 0.002279\n## Proportion Explained  0.002020 0.001830 0.001605 0.001577 0.001396 0.001040\n## Cumulative Proportion 0.989825 0.991656 0.993261 0.994837 0.996233 0.997274\n##                            CA59      CA60      CA61      CA62\n## Eigenvalue            0.0019296 0.0017784 0.0011904 0.0010752\n## Proportion Explained  0.0008807 0.0008116 0.0005433 0.0004907\n## Cumulative Proportion 0.9981544 0.9989660 0.9995093 1.0000000\n\n\n\nDCA\n\n\nlibrary(vegan)\ndca.1 &lt;- decorana(sveg, mk = 10)\nplot(dca.1, display = \"sites\", type = \"point\")\n\n\n\n\ndca.2 &lt;- decorana(sveg, mk = 100)\nplot(dca.2, display = \"sites\", type = \"point\")\n\n\n\n\n\n\nNMDS\n\n# Distanzmatrix als Start erzeugen (PCA)\nmde &lt;- vegdist(sveg, method = \"euclidean\")\n\n# Alternative mit einem für Vegetationsdaten häufig verwendeten\n# Dissimilarity-index\nmde &lt;- vegdist(sveg, method = \"bray\")\n\n# Z wei verschiedene NMDS-Methoden\nif (!require(MASS)) {\n    install.packages(\"MASS\")\n}\nlibrary(MASS)\nset.seed(1)  # macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\nimds &lt;- isoMDS(mde, k = 2)\n## initial  value 16.524491 \n## iter   5 value 12.518681\n## iter  10 value 12.025808\n## iter  10 value 12.020751\n## iter  10 value 12.020751\n## final  value 12.020751 \n## converged\nset.seed(1)\nmmds &lt;- metaMDS(mde, k = 2)\n## Run 0 stress 0.1179909 \n## Run 1 stress 0.1179909 \n## ... Procrustes: rmse 1.11122e-05  max resid 4.697213e-05 \n## ... Similar to previous best\n## Run 2 stress 0.170918 \n## Run 3 stress 0.1529993 \n## Run 4 stress 0.1179909 \n## ... Procrustes: rmse 2.030177e-06  max resid 1.189668e-05 \n## ... Similar to previous best\n## Run 5 stress 0.1252011 \n## Run 6 stress 0.1583424 \n## Run 7 stress 0.1181212 \n## ... Procrustes: rmse 0.006525662  max resid 0.04396629 \n## Run 8 stress 0.1596312 \n## Run 9 stress 0.1630026 \n## Run 10 stress 0.1179909 \n## ... New best solution\n## ... Procrustes: rmse 3.47582e-06  max resid 2.360888e-05 \n## ... Similar to previous best\n## Run 11 stress 0.1538119 \n## Run 12 stress 0.1252011 \n## Run 13 stress 0.1500845 \n## Run 14 stress 0.1251634 \n## Run 15 stress 0.1251634 \n## Run 16 stress 0.1179909 \n## ... Procrustes: rmse 5.655652e-06  max resid 1.960818e-05 \n## ... Similar to previous best\n## Run 17 stress 0.1179909 \n## ... Procrustes: rmse 7.036899e-06  max resid 2.755273e-05 \n## ... Similar to previous best\n## Run 18 stress 0.1179909 \n## ... Procrustes: rmse 1.0129e-05  max resid 3.793497e-05 \n## ... Similar to previous best\n## Run 19 stress 0.1251572 \n## Run 20 stress 0.1179909 \n## ... Procrustes: rmse 5.011736e-06  max resid 2.261906e-05 \n## ... Similar to previous best\n## *** Best solution repeated 5 times\n\nplot(imds$points)\n\n\n\nplot(mmds$points)\n\n\n\n\n# Stress = S² = Abweichung der zweidimensionalen NMDS-Lösung von der originalen\n# Distanzmatrix\nstressplot(imds, mde)\n\n\n\nstressplot(mmds, mde)"
  },
  {
    "objectID": "stat5-8/Statistik6_Uebung.html#übung-6.1-pca-naturwissenschaftlich",
    "href": "stat5-8/Statistik6_Uebung.html#übung-6.1-pca-naturwissenschaftlich",
    "title": "Stat6: Übung",
    "section": "Übung 6.1: PCA (naturwissenschaftlich)",
    "text": "Übung 6.1: PCA (naturwissenschaftlich)\n\nDatensatz: Doubs.RData\n\nLädt den Datensatz Doubs.RData mit dem folgenden Befehl ins R: load(“Doubs.RData”)\nDie Umweltvariablen findet ihr im data.frame env die Abundanzen im data.frame spe. Im data.frame fishtrait findet ihr die Vollständigen Namen der Fische\nDer Datensatz enthält Daten zum Vorkommen von Fischarten und den zugehörigen Umweltvariablen im Fluss Doubs (Jura). Es gibt 30 Probestellen (sites), an denen jeweils die Abundanzen von 27 Fischarten (auf einer Skalen von 0 bis 5) sowie 11 Umweltvariablen erhoben wurden:\n\ndfs = Distance from source (km)\nele = Elevation (m a.s.l.)\nslo = Slope (‰)\ndis = Mean annual discharge (m3 s-1)\npH = pH of water\nhar = Hardness (Ca concentration) (mg L-1)\npho = Phosphate concentration (mg L-1)\nnit = Nitrate concentration (mg L-1)\namm = Ammonium concentration (mg L-1)\noxy = Dissolved oxygen (mg L-1)\nbod = Biological oxygen demand (mg L-1)\n\nEure Aufgabe ist nun, in einem ersten Schritt eine PCA für die 11 Umweltvariablen zu rechnen. Da die einzelnen Variablen auf ganz unterschiedlichen Skalen gemessen wurden, ist dazu eine Standardisierung nötig (pca mit der Funktion rda, scale=TRUE). Überlegt, wie viele Achsen wichtig sind und für was sie jeweils stehen.\nIn einem zweiten Schritt sollen dann die vollständig unkorrelierten PCA-Achsen als Prädiktoren einer multiplen Regression zur Erklärung der Fischartenzahl (Anzahl kann z.B. kann mit dem Befehl specnumber(spe) ermittel werden) verwendet werden (wahlweise lm oder glm). Gebt das minimal adäquate Modell an und interpretiert dieses (wahlweise im frequentist oder information theoretician approach). (Wer noch mehr probieren möchte, kann zum Vergleich noch eine multiple Regression mit den Originaldaten rechnen)."
  },
  {
    "objectID": "stat5-8/Statistik6_Loesung.html",
    "href": "stat5-8/Statistik6_Loesung.html",
    "title": "Stat6: Lösung",
    "section": "",
    "text": "Download dieses Lösungsscript via “&lt;/&gt;Code” (oben rechts)\nLösungstext als Download\n\n\nload(\"datasets/statistik/Doubs.RData\")\nsummary(env)\n##       dfs              ele             slo              dis       \n##  Min.   :  0.30   Min.   :172.0   Min.   : 0.200   Min.   : 0.84  \n##  1st Qu.: 54.45   1st Qu.:248.0   1st Qu.: 0.525   1st Qu.: 4.20  \n##  Median :175.20   Median :395.0   Median : 1.200   Median :22.10  \n##  Mean   :188.23   Mean   :481.6   Mean   : 3.497   Mean   :22.20  \n##  3rd Qu.:301.73   3rd Qu.:782.0   3rd Qu.: 2.875   3rd Qu.:28.57  \n##  Max.   :453.00   Max.   :934.0   Max.   :48.000   Max.   :69.00  \n##        pH             har              pho              nit       \n##  Min.   :7.700   Min.   : 40.00   Min.   :0.0100   Min.   :0.150  \n##  1st Qu.:7.925   1st Qu.: 84.25   1st Qu.:0.1250   1st Qu.:0.505  \n##  Median :8.000   Median : 89.00   Median :0.2850   Median :1.600  \n##  Mean   :8.050   Mean   : 86.10   Mean   :0.5577   Mean   :1.654  \n##  3rd Qu.:8.100   3rd Qu.: 96.75   3rd Qu.:0.5600   3rd Qu.:2.425  \n##  Max.   :8.600   Max.   :110.00   Max.   :4.2200   Max.   :6.200  \n##       amm              oxy              bod        \n##  Min.   :0.0000   Min.   : 4.100   Min.   : 1.300  \n##  1st Qu.:0.0000   1st Qu.: 8.025   1st Qu.: 2.725  \n##  Median :0.1000   Median :10.200   Median : 4.150  \n##  Mean   :0.2093   Mean   : 9.390   Mean   : 5.117  \n##  3rd Qu.:0.2000   3rd Qu.:10.900   3rd Qu.: 5.275  \n##  Max.   :1.8000   Max.   :12.400   Max.   :16.700\nsummary(spe)\n##       Cogo           Satr           Phph            Babl            Thth     \n##  Min.   :0.00   Min.   :0.00   Min.   :0.000   Min.   :0.000   Min.   :0.00  \n##  1st Qu.:0.00   1st Qu.:0.00   1st Qu.:0.000   1st Qu.:1.000   1st Qu.:0.00  \n##  Median :0.00   Median :1.00   Median :3.000   Median :2.000   Median :0.00  \n##  Mean   :0.50   Mean   :1.90   Mean   :2.267   Mean   :2.433   Mean   :0.50  \n##  3rd Qu.:0.75   3rd Qu.:3.75   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:0.75  \n##  Max.   :3.00   Max.   :5.00   Max.   :5.000   Max.   :5.000   Max.   :4.00  \n##       Teso             Chna          Pato             Lele      \n##  Min.   :0.0000   Min.   :0.0   Min.   :0.0000   Min.   :0.000  \n##  1st Qu.:0.0000   1st Qu.:0.0   1st Qu.:0.0000   1st Qu.:0.000  \n##  Median :0.0000   Median :0.0   Median :0.0000   Median :1.000  \n##  Mean   :0.6333   Mean   :0.6   Mean   :0.8667   Mean   :1.433  \n##  3rd Qu.:0.7500   3rd Qu.:1.0   3rd Qu.:2.0000   3rd Qu.:2.000  \n##  Max.   :5.0000   Max.   :3.0   Max.   :4.0000   Max.   :5.000  \n##       Sqce            Baba            Albi          Gogo            Eslu      \n##  Min.   :0.000   Min.   :0.000   Min.   :0.0   Min.   :0.000   Min.   :0.000  \n##  1st Qu.:1.000   1st Qu.:0.000   1st Qu.:0.0   1st Qu.:0.000   1st Qu.:0.000  \n##  Median :2.000   Median :0.000   Median :0.0   Median :1.000   Median :1.000  \n##  Mean   :1.867   Mean   :1.433   Mean   :0.9   Mean   :1.833   Mean   :1.333  \n##  3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:1.0   3rd Qu.:3.750   3rd Qu.:2.000  \n##  Max.   :5.000   Max.   :5.000   Max.   :5.0   Max.   :5.000   Max.   :5.000  \n##       Pefl          Rham          Legi             Scer          Cyca       \n##  Min.   :0.0   Min.   :0.0   Min.   :0.0000   Min.   :0.0   Min.   :0.0000  \n##  1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.0000   1st Qu.:0.0   1st Qu.:0.0000  \n##  Median :0.5   Median :0.0   Median :0.0000   Median :0.0   Median :0.0000  \n##  Mean   :1.2   Mean   :1.1   Mean   :0.9667   Mean   :0.7   Mean   :0.8333  \n##  3rd Qu.:2.0   3rd Qu.:2.0   3rd Qu.:1.7500   3rd Qu.:1.0   3rd Qu.:1.0000  \n##  Max.   :5.0   Max.   :5.0   Max.   :5.0000   Max.   :5.0   Max.   :5.0000  \n##       Titi          Abbr             Icme          Gyce            Ruru    \n##  Min.   :0.0   Min.   :0.0000   Min.   :0.0   Min.   :0.000   Min.   :0.0  \n##  1st Qu.:0.0   1st Qu.:0.0000   1st Qu.:0.0   1st Qu.:0.000   1st Qu.:0.0  \n##  Median :1.0   Median :0.0000   Median :0.0   Median :0.000   Median :1.0  \n##  Mean   :1.5   Mean   :0.8667   Mean   :0.6   Mean   :1.267   Mean   :2.1  \n##  3rd Qu.:3.0   3rd Qu.:1.0000   3rd Qu.:0.0   3rd Qu.:2.000   3rd Qu.:5.0  \n##  Max.   :5.0   Max.   :5.0000   Max.   :5.0   Max.   :5.000   Max.   :5.0  \n##       Blbj            Alal          Anan     \n##  Min.   :0.000   Min.   :0.0   Min.   :0.00  \n##  1st Qu.:0.000   1st Qu.:0.0   1st Qu.:0.00  \n##  Median :0.000   Median :0.0   Median :0.00  \n##  Mean   :1.033   Mean   :1.9   Mean   :0.90  \n##  3rd Qu.:1.750   3rd Qu.:5.0   3rd Qu.:1.75  \n##  Max.   :5.000   Max.   :5.0   Max.   :5.00\n\n# Die Dataframes env und spe enthalten die Umwelt- respective die Artdaten\n\nif (!require(vegan)) {\n    install.packages(\"vegan\")\n}\nlibrary(\"vegan\")\n\nDie PCA wird im Package vegan mit dem Befehl rda ausgeführt, wobei in diesem scale = TRUE gesetzt werden muss, da die Umweltdaten mit ganz unterschiedlichen Einheiten und Wertebereichen daherkommen\n\nenv.pca &lt;- rda(env, scale = TRUE)\nenv.pca\n## Call: rda(X = env, scale = TRUE)\n## \n##               Inertia Rank\n## Total              11     \n## Unconstrained      11   11\n## Inertia is correlations \n## \n## Eigenvalues for unconstrained axes:\n##   PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11 \n## 5.969 2.164 1.065 0.739 0.400 0.336 0.173 0.108 0.024 0.017 0.006\n# In env.pca sieht man, dass es bei 11 Umweltvariablen logischerweise 11\n# orthogonale Principle Components gibt\n\nsummary(env.pca, axes = 0)\n## \n## Call:\n## rda(X = env, scale = TRUE) \n## \n## Partitioning of correlations:\n##               Inertia Proportion\n## Total              11          1\n## Unconstrained      11          1\n## \n## Eigenvalues, and their contribution to the correlations \n## \n## Importance of components:\n##                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\n## Eigenvalue            5.9687 2.1639 1.06517 0.73875 0.40019 0.33563 0.17263\n## Proportion Explained  0.5426 0.1967 0.09683 0.06716 0.03638 0.03051 0.01569\n## Cumulative Proportion 0.5426 0.7393 0.83616 0.90332 0.93970 0.97022 0.98591\n##                            PC8      PC9     PC10     PC11\n## Eigenvalue            0.108228 0.023701 0.017083 0.005983\n## Proportion Explained  0.009839 0.002155 0.001553 0.000544\n## Cumulative Proportion 0.995748 0.997903 0.999456 1.000000\n## \n## Scaling 2 for species and site scores\n## * Species are scaled proportional to eigenvalues\n## * Sites are unscaled: weighted dispersion equal on all dimensions\n## * General scaling constant of scores:\n\n# Hier sieht man auch die Übersetzung der Eigenvalues in erklärte Varianzen der\n# einzelnen Principle Components\n\nsummary(env.pca)\n## \n## Call:\n## rda(X = env, scale = TRUE) \n## \n## Partitioning of correlations:\n##               Inertia Proportion\n## Total              11          1\n## Unconstrained      11          1\n## \n## Eigenvalues, and their contribution to the correlations \n## \n## Importance of components:\n##                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\n## Eigenvalue            5.9687 2.1639 1.06517 0.73875 0.40019 0.33563 0.17263\n## Proportion Explained  0.5426 0.1967 0.09683 0.06716 0.03638 0.03051 0.01569\n## Cumulative Proportion 0.5426 0.7393 0.83616 0.90332 0.93970 0.97022 0.98591\n##                            PC8      PC9     PC10     PC11\n## Eigenvalue            0.108228 0.023701 0.017083 0.005983\n## Proportion Explained  0.009839 0.002155 0.001553 0.000544\n## Cumulative Proportion 0.995748 0.997903 0.999456 1.000000\n## \n## Scaling 2 for species and site scores\n## * Species are scaled proportional to eigenvalues\n## * Sites are unscaled: weighted dispersion equal on all dimensions\n## * General scaling constant of scores:  4.226177 \n## \n## \n## Species scores\n## \n##          PC1     PC2      PC3      PC4      PC5      PC6\n## dfs  1.08657  0.5342 -0.27333 -0.13477  0.07336  0.22566\n## ele -1.04396 -0.6148  0.20712  0.12854  0.14610  0.02111\n## slo -0.57703 -0.4893 -0.63490 -0.71684  0.33349 -0.11782\n## dis  0.95843  0.6608 -0.32456 -0.16183  0.11542  0.13935\n## pH  -0.06364  0.4629  1.01317 -0.58606  0.17094  0.07360\n## har  0.90118  0.5850  0.06449  0.25696  0.30995 -0.53390\n## pho  1.05821 -0.6014  0.13866 -0.17883 -0.11125 -0.13751\n## nit  1.15013 -0.1005 -0.05167 -0.24537 -0.35105  0.02145\n## amm  1.00679 -0.6969  0.14077 -0.14684 -0.19200 -0.11904\n## oxy -0.97459  0.4991 -0.09017 -0.31040 -0.38066 -0.36500\n## bod  0.97315 -0.7148  0.15145  0.07193  0.23633 -0.05540\n## \n## \n## Site scores (weighted sums of species scores)\n## \n##         PC1      PC2      PC3      PC4       PC5       PC6\n## 1  -1.41274 -1.40098 -2.03484 -2.67759  1.117150 -0.184951\n## 2  -1.03725 -0.77955  0.24400  0.25635 -1.192043  1.849810\n## 3  -0.94507 -0.46765  1.25042 -0.49330 -0.234194  1.319198\n## 4  -0.87371 -0.26988  0.19304  0.51979 -0.494639 -0.116092\n## 5  -0.42088 -0.66944  0.83191  0.71729  0.867751 -0.112219\n## 6  -0.77224 -0.72067 -0.07357  0.77902 -0.386130  0.654273\n## 7  -0.77466 -0.08103  0.39630  0.19224  0.416470 -1.026304\n## 8  -0.28840 -0.60589  0.83822  1.01440  1.707316 -0.295861\n## 9  -0.28305 -0.47710  0.39908  1.13075  0.882098 -0.002961\n## 10 -0.48714 -0.41860 -1.27555  0.90267  0.013704 -0.542270\n## 11 -0.26940  0.45384  0.09119 -0.15127 -0.233814 -1.157483\n## 12 -0.43834  0.36049 -0.52352  0.57279 -0.650095 -0.817673\n## 13 -0.37794  0.70379  0.10339  0.06127 -0.101571 -1.376623\n## 14 -0.23878  0.75522  0.83648 -0.55822 -0.011527 -1.221217\n## 15 -0.30425  0.95026  1.80274 -1.48211  0.135021 -0.031795\n## 16 -0.13354  0.33951 -0.23252  0.19177 -0.667112 -0.227348\n## 17  0.10111  0.32379 -0.20380  0.18495 -0.676546 -0.364915\n## 18  0.06913  0.37913 -0.25881  0.06998 -0.851379 -0.289054\n## 19  0.05746  0.43915  0.04566 -0.32171 -0.899449  0.090759\n## 20  0.17478  0.39927 -0.36244 -0.15647 -1.300718  0.093396\n## 21  0.16944  0.35608 -0.73929  0.42751 -0.509249  0.653892\n## 22  0.14898  0.55339 -0.08008 -0.04972  0.196636  0.621753\n## 23  1.39778 -1.19102  0.66424 -0.46178  0.252908 -0.573369\n## 24  0.99357 -0.52036  0.07186  0.48088  1.068785  0.373991\n## 25  2.22002 -2.03168  0.17940 -0.52606 -1.148014 -0.786506\n## 26  0.89388 -0.10410 -0.61440  0.42034  0.343649  0.800522\n## 27  0.64866  0.41296 -0.17444 -0.26105  0.274443  1.259099\n## 28  0.77100  0.82592  0.43387 -1.00092 -0.001674  0.703378\n## 29  0.66413  1.11562 -1.58043  0.65099  0.650327  0.020001\n## 30  0.74743  1.36955 -0.22810 -0.43281  1.431895  0.686570\n# Hier das ausführliche Summary mit den Art- und Umweltkorrelationen auf den\n# ersten sechs Achsen\n\nscreeplot(env.pca, bstick = TRUE, npcs = length(env.pca$CA$eig))\n\n\n\n# Visualisierung der Anteile erklärter Varianz, auch im Vergleich zu einem\n# Broken-Stick-Modell\n\n\nDie Anteile fallen steil ab. Nur die ersten vier Achsen erklären jeweils mehr als 5 % (und zusammen über 90 %)\nDas Broken-stick-Modell würde sogar nur die ersten beiden Achsen als relevant vorschlagen\nDa die Relevanz für das Datenmuster in den Umweltdaten nicht notwendig die Relevanz für die Erklärung der Artenzahlen ist, nehmen wir ins globale Modell grosszügig die ersten vier Achsen rein (PC1-PC4) Die Bedeutung der Achsen (benötigt man später für die Interpretation!) findet man in den “species scores” (da so, wie wir die PCA hier gerechnet haben, die Umweltdaten die Arten sind. Zusätzlich oder alternative kann man sich die ersten vier Achsen auch visualisieren, indem man PC2 vs. PC1 (ohne choices), PC3 vs. PC1 oder PC4 vs. PC1 plottet.\n\n\npar(mfrow = c(2, 2))\nbiplot(env.pca, scaling = 1)\nbiplot(env.pca, choices = c(1, 3), scaling = 1)\nbiplot(env.pca, choices = c(1, 4), scaling = 1)\n\n\n\n\n\nPC1 steht v.a. für Nitrat (positiv), Sauerstoff (negativ)\nPC2 steht v.a. für pH (positiv)\nPC3 steht v.a. für pH (positiv) und slo (negativ)\nPC4 steht v.a. für pH (negativ) und slo (negativ)\n\n\n# Wir extrahieren nun die ersten vier PC-Scores aller Aufnahmeflächen\n\nscores &lt;- scores(env.pca, c(1:4), display = c(\"sites\"))\nscores\n##            PC1         PC2         PC3         PC4\n## 1  -1.41273883 -1.40097880 -2.03483870 -2.67758838\n## 2  -1.03724733 -0.77955354  0.24400009  0.25634696\n## 3  -0.94506998 -0.46765361  1.25042488 -0.49329701\n## 4  -0.87371164 -0.26988488  0.19304045  0.51979381\n## 5  -0.42087585 -0.66943957  0.83190665  0.71729089\n## 6  -0.77223581 -0.72066623 -0.07357441  0.77902331\n## 7  -0.77466085 -0.08103491  0.39629959  0.19223674\n## 8  -0.28839689 -0.60588978  0.83822295  1.01439781\n## 9  -0.28305399 -0.47710013  0.39908190  1.13074537\n## 10 -0.48714448 -0.41859737 -1.27554791  0.90267450\n## 11 -0.26940072  0.45383527  0.09118967 -0.15126579\n## 12 -0.43834427  0.36049094 -0.52351661  0.57279309\n## 13 -0.37793587  0.70379486  0.10338604  0.06127189\n## 14 -0.23878321  0.75521955  0.83648481 -0.55822243\n## 15 -0.30424687  0.95025522  1.80274307 -1.48210897\n## 16 -0.13353523  0.33951332 -0.23252035  0.19177453\n## 17  0.10111086  0.32378989 -0.20379779  0.18495051\n## 18  0.06913015  0.37912929 -0.25881042  0.06998196\n## 19  0.05745832  0.43915445  0.04566423 -0.32171096\n## 20  0.17478169  0.39926644 -0.36244421 -0.15647276\n## 21  0.16944233  0.35608121 -0.73929343  0.42751253\n## 22  0.14898095  0.55338567 -0.08008399 -0.04971692\n## 23  1.39778235 -1.19101965  0.66424125 -0.46178368\n## 24  0.99357418 -0.52036211  0.07185912  0.48087634\n## 25  2.22001746 -2.03168135  0.17940028 -0.52606378\n## 26  0.89388246 -0.10410321 -0.61440303  0.42034205\n## 27  0.64865976  0.41296206 -0.17444493 -0.26104930\n## 28  0.77099833  0.82591720  0.43386950 -1.00091524\n## 29  0.66413124  1.11561620 -1.58043410  0.65099411\n## 30  0.74743174  1.36955357 -0.22810459 -0.43281119\n## attr(,\"const\")\n## [1] 4.226177\n\n# Berechnung der Artenzahl mittels specnumber; Artenzahl und Scores werden zum\n# Dataframe für die Regressionsanalyse hinzugefügt\ndoubs &lt;- data.frame(env, scores, species_richness = specnumber(spe))\ndoubs\n##      dfs ele  slo   dis  pH har  pho  nit  amm  oxy  bod         PC1\n## 1    0.3 934 48.0  0.84 7.9  45 0.01 0.20 0.00 12.2  2.7 -1.41273883\n## 2    2.2 932  3.0  1.00 8.0  40 0.02 0.20 0.10 10.3  1.9 -1.03724733\n## 3   10.2 914  3.7  1.80 8.3  52 0.05 0.22 0.05 10.5  3.5 -0.94506998\n## 4   18.5 854  3.2  2.53 8.0  72 0.10 0.21 0.00 11.0  1.3 -0.87371164\n## 5   21.5 849  2.3  2.64 8.1  84 0.38 0.52 0.20  8.0  6.2 -0.42087585\n## 6   32.4 846  3.2  2.86 7.9  60 0.20 0.15 0.00 10.2  5.3 -0.77223581\n## 7   36.8 841  6.6  4.00 8.1  88 0.07 0.15 0.00 11.1  2.2 -0.77466085\n## 8   49.1 792  2.5  1.30 8.1  94 0.20 0.41 0.12  7.0  8.1 -0.28839689\n## 9   70.5 752  1.2  4.80 8.0  90 0.30 0.82 0.12  7.2  5.2 -0.28305399\n## 10  99.0 617  9.9 10.00 7.7  82 0.06 0.75 0.01 10.0  4.3 -0.48714448\n## 11 123.4 483  4.1 19.90 8.1  96 0.30 1.60 0.00 11.5  2.7 -0.26940072\n## 12 132.4 477  1.6 20.00 7.9  86 0.04 0.50 0.00 12.2  3.0 -0.43834427\n## 13 143.6 450  2.1 21.10 8.1  98 0.06 0.52 0.00 12.4  2.4 -0.37793587\n## 14 152.2 434  1.2 21.20 8.3  98 0.27 1.23 0.00 12.3  3.8 -0.23878321\n## 15 164.5 415  0.5 23.00 8.6  86 0.40 1.00 0.00 11.7  2.1 -0.30424687\n## 16 185.9 375  2.0 16.10 8.0  88 0.20 2.00 0.05 10.3  2.7 -0.13353523\n## 17 198.5 349  0.5 24.30 8.0  92 0.20 2.50 0.20 10.2  4.6  0.10111086\n## 18 211.0 333  0.8 25.00 8.0  90 0.50 2.20 0.20 10.3  2.8  0.06913015\n## 19 224.6 310  0.5 25.90 8.1  84 0.60 2.20 0.15 10.6  3.3  0.05745832\n## 20 247.7 286  0.8 26.80 8.0  86 0.30 3.00 0.30 10.3  2.8  0.17478169\n## 21 282.1 262  1.0 27.20 7.9  85 0.20 2.20 0.10  9.0  4.1  0.16944233\n## 22 294.0 254  1.4 27.90 8.1  88 0.20 1.62 0.07  9.1  4.8  0.14898095\n## 23 304.3 246  1.2 28.80 8.1  97 2.60 3.50 1.15  6.3 16.4  1.39778235\n## 24 314.7 241  0.3 29.76 8.0  99 1.40 2.50 0.60  5.2 12.3  0.99357418\n## 25 327.8 231  0.5 38.70 7.9 100 4.22 6.20 1.80  4.1 16.7  2.22001746\n## 26 356.9 214  0.5 39.10 7.9  94 1.43 3.00 0.30  6.2  8.9  0.89388246\n## 27 373.2 206  1.2 39.60 8.1  90 0.58 3.00 0.26  7.2  6.3  0.64865976\n## 28 394.7 195  0.3 43.20 8.3 100 0.74 4.00 0.30  8.1  4.5  0.77099833\n## 29 422.0 183  0.6 67.70 7.8 110 0.45 1.62 0.10  9.0  4.2  0.66413124\n## 30 453.0 172  0.2 69.00 8.2 109 0.65 1.60 0.10  8.2  4.4  0.74743174\n##            PC2         PC3         PC4 species_richness\n## 1  -1.40097880 -2.03483870 -2.67758838                1\n## 2  -0.77955354  0.24400009  0.25634696                3\n## 3  -0.46765361  1.25042488 -0.49329701                4\n## 4  -0.26988488  0.19304045  0.51979381                8\n## 5  -0.66943957  0.83190665  0.71729089               11\n## 6  -0.72066623 -0.07357441  0.77902331               10\n## 7  -0.08103491  0.39629959  0.19223674                5\n## 8  -0.60588978  0.83822295  1.01439781                0\n## 9  -0.47710013  0.39908190  1.13074537                5\n## 10 -0.41859737 -1.27554791  0.90267450                6\n## 11  0.45383527  0.09118967 -0.15126579                6\n## 12  0.36049094 -0.52351661  0.57279309                6\n## 13  0.70379486  0.10338604  0.06127189                6\n## 14  0.75521955  0.83648481 -0.55822243               10\n## 15  0.95025522  1.80274307 -1.48210897               11\n## 16  0.33951332 -0.23252035  0.19177453               17\n## 17  0.32378989 -0.20379779  0.18495051               22\n## 18  0.37912929 -0.25881042  0.06998196               23\n## 19  0.43915445  0.04566423 -0.32171096               23\n## 20  0.39926644 -0.36244421 -0.15647276               22\n## 21  0.35608121 -0.73929343  0.42751253               23\n## 22  0.55338567 -0.08008399 -0.04971692               22\n## 23 -1.19101965  0.66424125 -0.46178368                3\n## 24 -0.52036211  0.07185912  0.48087634                8\n## 25 -2.03168135  0.17940028 -0.52606378                8\n## 26 -0.10410321 -0.61440303  0.42034205               21\n## 27  0.41296206 -0.17444493 -0.26104930               22\n## 28  0.82591720  0.43386950 -1.00091524               22\n## 29  1.11561620 -1.58043410  0.65099411               26\n## 30  1.36955357 -0.22810459 -0.43281119               21\nstr(doubs)\n## 'data.frame':    30 obs. of  16 variables:\n##  $ dfs             : num  0.3 2.2 10.2 18.5 21.5 32.4 36.8 49.1 70.5 99 ...\n##  $ ele             : int  934 932 914 854 849 846 841 792 752 617 ...\n##  $ slo             : num  48 3 3.7 3.2 2.3 3.2 6.6 2.5 1.2 9.9 ...\n##  $ dis             : num  0.84 1 1.8 2.53 2.64 2.86 4 1.3 4.8 10 ...\n##  $ pH              : num  7.9 8 8.3 8 8.1 7.9 8.1 8.1 8 7.7 ...\n##  $ har             : int  45 40 52 72 84 60 88 94 90 82 ...\n##  $ pho             : num  0.01 0.02 0.05 0.1 0.38 0.2 0.07 0.2 0.3 0.06 ...\n##  $ nit             : num  0.2 0.2 0.22 0.21 0.52 0.15 0.15 0.41 0.82 0.75 ...\n##  $ amm             : num  0 0.1 0.05 0 0.2 0 0 0.12 0.12 0.01 ...\n##  $ oxy             : num  12.2 10.3 10.5 11 8 10.2 11.1 7 7.2 10 ...\n##  $ bod             : num  2.7 1.9 3.5 1.3 6.2 5.3 2.2 8.1 5.2 4.3 ...\n##  $ PC1             : num  -1.413 -1.037 -0.945 -0.874 -0.421 ...\n##  $ PC2             : num  -1.401 -0.78 -0.468 -0.27 -0.669 ...\n##  $ PC3             : num  -2.035 0.244 1.25 0.193 0.832 ...\n##  $ PC4             : num  -2.678 0.256 -0.493 0.52 0.717 ...\n##  $ species_richness: int  1 3 4 8 11 10 5 0 5 6 ...\n\n## Lösung mit lm (alternativ ginge Poisson-glm) und frequentist approach\n## (alternativ ginge Multimodelinference mit AICc)\nlm.pc.0 &lt;- lm(species_richness ~ PC1 + PC2 + PC3 + PC4, data = doubs)\nsummary(lm.pc.0)\n## \n## Call:\n## lm(formula = species_richness ~ PC1 + PC2 + PC3 + PC4, data = doubs)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.2359 -4.3792  0.4256  4.5453  7.5058 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  12.5000     0.9869  12.667 2.24e-12 ***\n## PC1           4.4035     1.2790   3.443  0.00204 ** \n## PC2           6.6729     1.2790   5.217 2.13e-05 ***\n## PC3          -2.9645     1.2790  -2.318  0.02893 *  \n## PC4           0.1674     1.2790   0.131  0.89694    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.405 on 25 degrees of freedom\n## Multiple R-squared:  0.6401, Adjusted R-squared:  0.5825 \n## F-statistic: 11.12 on 4 and 25 DF,  p-value: 2.55e-05\n\n# Modellvereinfachung: PC4 ist nicht signifikant und wird entfernt\nlm.pc.1 &lt;- lm(species_richness ~ PC1 + PC2 + PC3, data = doubs)\nsummary(lm.pc.1)  # jetzt sind alle Achsen signifikant und werden in das minimal adäquate Modell aufgenommen\n## \n## Call:\n## lm(formula = species_richness ~ PC1 + PC2 + PC3, data = doubs)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -9.226 -4.457  0.403  4.545  7.452 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   12.500      0.968  12.913 8.11e-13 ***\n## PC1            4.404      1.255   3.510  0.00165 ** \n## PC2            6.673      1.255   5.319 1.45e-05 ***\n## PC3           -2.965      1.255  -2.363  0.02589 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.302 on 26 degrees of freedom\n## Multiple R-squared:  0.6399, Adjusted R-squared:  0.5983 \n## F-statistic:  15.4 on 3 and 26 DF,  p-value: 5.853e-06\n\n# Modelldiagnostik/Modellvalidierung\npar(mfrow = c(2, 2))\nplot(lm.pc.1)\n\n\n\n\nNicht besonders toll, ginge aber gerade noch. Da wir aber ohnehin Zähldaten haben, können wir es mit einem Poisson-GLM versuchen\nAlternativ mit glm\n\nglm.pc.1 &lt;- glm(species_richness ~ PC1 + PC2 + PC3 + PC4, family = \"poisson\", data = doubs)\nsummary(glm.pc.1)\n## \n## Call:\n## glm(formula = species_richness ~ PC1 + PC2 + PC3 + PC4, family = \"poisson\", \n##     data = doubs)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.6063  -1.4535  -0.1915   1.3852   2.2701  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  2.38447    0.05889  40.491  &lt; 2e-16 ***\n## PC1          0.39601    0.08240   4.806 1.54e-06 ***\n## PC2          0.54840    0.07550   7.263 3.78e-13 ***\n## PC3         -0.15174    0.08345  -1.818    0.069 .  \n## PC4          0.06053    0.09661   0.627    0.531    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 179.812  on 29  degrees of freedom\n## Residual deviance:  76.312  on 25  degrees of freedom\n## AIC: 206.97\n## \n## Number of Fisher Scoring iterations: 5\nglm.pc.2 &lt;- glm(species_richness ~ PC1 + PC2 + PC3, family = \"poisson\", data = doubs)\nsummary(glm.pc.2)\n## \n## Call:\n## glm(formula = species_richness ~ PC1 + PC2 + PC3, family = \"poisson\", \n##     data = doubs)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.4821  -1.3539  -0.2734   1.4039   2.2096  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  2.38670    0.05858  40.742  &lt; 2e-16 ***\n## PC1          0.38609    0.07941   4.862 1.16e-06 ***\n## PC2          0.53665    0.07161   7.494 6.70e-14 ***\n## PC3         -0.17669    0.07106  -2.486   0.0129 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 179.812  on 29  degrees of freedom\n## Residual deviance:  76.722  on 26  degrees of freedom\n## AIC: 205.38\n## \n## Number of Fisher Scoring iterations: 5\npar(mfrow = c(2, 2))\nplot(glm.pc.2)  # sieht nicht besser aus als LM, die Normalverteilung ist sogar schlechter\n\n\n\n\nLM oder GLM sind für die Analyse möglich, Modellwahl nach Gusto. Man muss jetzt noch die Ergebnisse adäquat aus all den erzielten Outputs zusammenstellen (siehe Ergebnistext). In dieser Aufgabe haben wir ja die PC-Achsen als Alternative zur direkten Modellierung mit den originalen Umweltvariablen ausprobiert. Deshalb (war nicht Teil der Aufgabe), kommt hier noch eine Lösung, wie wir es bisher gemacht hätten.\nZum Vergleich die Modellierung mit den Originaldaten\n\n# Korrelationen zwischen Prädiktoren\ncor &lt;- cor(doubs[, 1:11])\ncor[abs(cor) &lt; 0.7] &lt;- 0\ncor\n\nDie Korrelationsmatrix betrachtet man am besten in Excel. Es zeigt sich, dass es zwei grosse Gruppen von untereinander hochkorrelierten Variablen gibt: zum einen dfs-ele-dis-har-nit, zum anderen pho-nit-amm-oxy-bod, während slo und pH mit jeweils keiner anderen Variablen hochkorreliert sind. Insofern behalten wir eine aus der ersten Gruppe (ele), eine aus der zweiten Gruppe (pho) und die beiden «unabhängigen».\n\n# Globalmodell (als hinreichend unabhängige Variablen werden ele, slo, pH und\n# pho aufgenommen)\nlm.orig.1 &lt;- lm(species_richness ~ ele + slo + pH + pho, data = doubs)\nsummary(lm.orig.1)\n## \n## Call:\n## lm(formula = species_richness ~ ele + slo + pH + pho, data = doubs)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -9.784 -3.265  1.869  3.375  7.664 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 74.19236   47.29223   1.569  0.12926    \n## ele         -0.02645    0.00441  -5.997 2.91e-06 ***\n## slo         -0.09597    0.12988  -0.739  0.46684    \n## pH          -5.75643    5.84799  -0.984  0.33438    \n## pho         -4.09089    1.25657  -3.256  0.00324 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.285 on 25 degrees of freedom\n## Multiple R-squared:  0.6559, Adjusted R-squared:  0.6009 \n## F-statistic: 11.92 on 4 and 25 DF,  p-value: 1.485e-05\n\n# Modellvereinfachung: slo als am wenigsten signifikante Variable gestrichen\nlm.orig.2 &lt;- lm(species_richness ~ ele + pH + pho, data = doubs)\nsummary(lm.orig.2)\n## \n## Call:\n## lm(formula = species_richness ~ ele + pH + pho, data = doubs)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -9.446 -3.323  1.485  3.562  8.209 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 66.416530  45.702262   1.453  0.15812    \n## ele         -0.027744   0.004011  -6.917 2.41e-07 ***\n## pH          -4.756146   5.639266  -0.843  0.40670    \n## pho         -4.068860   1.245202  -3.268  0.00305 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.239 on 26 degrees of freedom\n## Multiple R-squared:  0.6484, Adjusted R-squared:  0.6079 \n## F-statistic: 15.98 on 3 and 26 DF,  p-value: 4.305e-06\n\n# Modellvereinfachung: pH ist immer noch nicht signifikant und wird gestrichen\nlm.orig.3 &lt;- lm(species_richness ~ ele + pho, data = doubs)\nsummary(lm.orig.3)\n## \n## Call:\n## lm(formula = species_richness ~ ele + pho, data = doubs)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -9.334 -4.548  1.058  3.717  7.889 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 27.929234   2.490667  11.214 1.15e-11 ***\n## ele         -0.027463   0.003976  -6.908 2.01e-07 ***\n## pho         -3.951980   1.230833  -3.211  0.00341 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.211 on 27 degrees of freedom\n## Multiple R-squared:  0.6388, Adjusted R-squared:  0.6121 \n## F-statistic: 23.88 on 2 and 27 DF,  p-value: 1.07e-06\n\n# Modelldiagnostik\npar(mfrow = c(2, 2))\nplot(lm.orig.3)  # nicht so gut, besonders die Bananenform in der linken obereren Abbildung\n\n\n\n\n# Nach Modellvereinfachung bleiben zwei signifikante Variablen, ele und pho.\n\n# Da das nicht so gut aussieht, versuchen wir es mit dem theoretisch\n# angemesseneren Modell, einem Poisson-GLM.\n\n# Versuch mit glm\nglm.orig.1 &lt;- glm(species_richness ~ ele + pho + pH + slo, family = \"poisson\", data = doubs)\nsummary(glm.orig.1)\n## \n## Call:\n## glm(formula = species_richness ~ ele + pho + pH + slo, family = \"poisson\", \n##     data = doubs)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.3800  -0.7094   0.1622   0.8079   2.4435  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  7.855498   2.550786   3.080  0.00207 ** \n## ele         -0.002277   0.000321  -7.094  1.3e-12 ***\n## pho         -0.362280   0.082384  -4.397  1.1e-05 ***\n## pH          -0.506330   0.316934  -1.598  0.11013    \n## slo         -0.054200   0.027685  -1.958  0.05026 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 179.812  on 29  degrees of freedom\n## Residual deviance:  55.128  on 25  degrees of freedom\n## AIC: 185.79\n## \n## Number of Fisher Scoring iterations: 5\n\nglm.orig.2 &lt;- glm(species_richness ~ ele + pho + slo, family = \"poisson\", data = doubs)\nsummary(glm.orig.2)\n## \n## Call:\n## glm(formula = species_richness ~ ele + pho + slo, family = \"poisson\", \n##     data = doubs)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.3902  -0.8159   0.2153   0.8648   2.4389  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  3.7819203  0.1356022  27.890  &lt; 2e-16 ***\n## ele         -0.0023363  0.0003167  -7.377 1.62e-13 ***\n## pho         -0.3563681  0.0835094  -4.267 1.98e-05 ***\n## slo         -0.0446686  0.0246618  -1.811   0.0701 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 179.812  on 29  degrees of freedom\n## Residual deviance:  57.752  on 26  degrees of freedom\n## AIC: 186.41\n## \n## Number of Fisher Scoring iterations: 5\n\nglm.orig.3 &lt;- glm(species_richness ~ ele + pho, family = \"poisson\", data = doubs)\nsummary(glm.orig.3)\n## \n## Call:\n## glm(formula = species_richness ~ ele + pho, family = \"poisson\", \n##     data = doubs)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.1946  -0.9256   0.0642   0.8567   2.8093  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  3.8381921  0.1342437  28.591  &lt; 2e-16 ***\n## ele         -0.0026994  0.0002795  -9.658  &lt; 2e-16 ***\n## pho         -0.3525967  0.0829766  -4.249 2.14e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 179.812  on 29  degrees of freedom\n## Residual deviance:  63.336  on 27  degrees of freedom\n## AIC: 190\n## \n## Number of Fisher Scoring iterations: 5\nplot(glm.orig.3)\n\n\n\n\n# Das sieht deutlich besser aus als beim LM. Wir müssen aber noch prüfen, ob\n# evtl. Overdispersion vorliegt.\n\nif (!require(AER)) {\n    install.packages(\"AER\")\n}\nlibrary(AER)\ndispersiontest(glm.orig.3)  #signifikante Überdispersion\n## \n##  Overdispersion test\n## \n## data:  glm.orig.3\n## z = 2.1816, p-value = 0.01457\n## alternative hypothesis: true dispersion is greater than 1\n## sample estimates:\n## dispersion \n##   1.967504\n\n# Ja, es gibt signifikante Overdispersion (obwohl der Dispersionparameter sogar\n# unter 2 ist, also nicht extrem). Wir können nun entweder quasipoisson oder\n# negativebinomial nehmen.\n\nglmq.orig.3 &lt;- glm(species_richness ~ ele + pho, family = \"quasipoisson\", data = doubs)\nsummary(glmq.orig.3)\n## \n## Call:\n## glm(formula = species_richness ~ ele + pho, family = \"quasipoisson\", \n##     data = doubs)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.1946  -0.9256   0.0642   0.8567   2.8093  \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.8381921  0.1996453  19.225  &lt; 2e-16 ***\n## ele         -0.0026994  0.0004157  -6.494 5.81e-07 ***\n## pho         -0.3525967  0.1234016  -2.857  0.00813 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for quasipoisson family taken to be 2.211722)\n## \n##     Null deviance: 179.812  on 29  degrees of freedom\n## Residual deviance:  63.336  on 27  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 5\n\n# Parameterschätzung bleiben gleich, aber Signifikanzen sind niedriger als beim\n# GLM ohne Overdispersion.\nplot(glmq.orig.3)\n\n\n\n\nSieht gut aus, wir hätten hier also unser finales Modell.\nIm Vergleich der beiden Vorgehensweisen (PC-Achsen vs. Umweltdaten direkt) scheint in diesem Fall die direkte Modellierung der Umweltachsen informativer: Man kommt mit zwei Prädiktoren aus, die jeweils direkt für eine der Hauptvariablen stehen – Meereshöhe und Phosphor – zugleich aber jeweils eine grössere Gruppe von Variablen mit hohen Korrelationen inkludieren, im ersten Fall Variablen, die sich im Flusslauf von oben nach unten systematisch ändern, im zweiten Masse der Nährstoffbelastung des Gewässers. Bei der PCA-Lösung kamen drei signifikante Komponenten heraus, die allerdings nicht so leicht zu interpretieren sind. Dies insbesondere, weil in diesem Fall auf der Ebene PC2 vs. PC1 die Mehrzahl der Umweltparameter ungefähr in 45-Grad-Winkeln angeordnet sind. Im allgemeinen Fall kann aber die Nutzung von PC-Achsen durchaus eine gute Lösung sein."
  },
  {
    "objectID": "stat5-8/Statistik7_Demo.html#ordinationen-ii",
    "href": "stat5-8/Statistik7_Demo.html#ordinationen-ii",
    "title": "Stat7: Demo",
    "section": "Ordinationen II",
    "text": "Ordinationen II\n\nInterpretation von Ordinationen\nWildi pp. 96 et seq.\n\n# Plot Arten\nif(!require(dave)){install.packages(\"dave\")}\nlibrary(dave)\n\n# Daten anschauen\ndim(sveg) # Vegetationsaufnahmen\n## [1]  63 119\nsveg[1:3, 1:3]\n##     Vaccinium.myrtillus Vaccinium.uliginosum Vaccinium.oxycoccos\n## 501                   0                    0                   2\n## 502                   1                    2                   1\n## 503                   1                    1                   1\ndim(ssit) # Umweltvariablen\n## [1] 63 20\nssit[1:3, 1:3]\n##     pH.peat log.ash.perc Ca_peat\n## 501     3.9         1.62     7.2\n## 502     3.6         1.30     8.8\n## 503     4.4         1.77    14.1\n\n# CA rechnen\nca &lt;- cca(sveg^0.5)\n\n## Plot mit ausgewählten Arten\nsel.spec &lt;- c(3, 11, 23, 31, 39, 46, 72, 77, 96)\nsnames &lt;- names(sveg[,sel.spec])\nsnames\n## [1] \"Vaccinium.oxycoccos\" \"Carex.echinata\"      \"Arnica.montana\"     \n## [4] \"Festuca.rubra\"       \"Carex.pulicaris\"     \"Sphagnum.recurvum\"  \n## [7] \"Viola.palustris\"     \"Galium.uliginosum\"   \"Stachys.officinalis\"\nscores &lt;- scores(ca, display = \"species\", scaling = \"sites\")\nsx &lt;- scores[sel.spec, 1]\nsy &lt;- scores[sel.spec, 2]\nplot(ca, display = \"sites\", type = \"point\")\npoints(sx, sy, pch = 16)\nsnames &lt;- make.cepnames(snames)\ntext(sx, sy, snames, pos = c(1,2,1,1,3,2,4,3,1), cex = 0.8)\n\n\n\n\n# Plot \"response surfaces\" in der CA\npar(mfrow=c(1,2))\nplot(ca, display = \"sites\", type = \"point\")\nordisurf(ca, ssit$pH.peat, add = TRUE, col = \"red\")\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## y ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n## \n## Estimated degrees of freedom:\n## 4.63  total = 5.63 \n## \n## REML score: 28.14791\ntext(-1.5, 2, \"pH\", col = \"red\")\nplot(ca, display = \"sites\", type = \"points\")\nordisurf(ca, ssit$Waterlev.av, add = TRUE, col = \"blue\")\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## y ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n## \n## Estimated degrees of freedom:\n## 5.07  total = 6.07 \n## \n## REML score: 161.492\ntext(-1.5, 2, \"Wasserstand\", col = \"blue\")\n\n\n\n\n# Daselbe mit einer DCA\npar(mfrow=c(1,2))\ndca &lt;- decorana(sveg)\nplot(dca, display = \"sites\", type = \"points\")\nordisurf(dca, ssit$pH.peat, add = TRUE)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## y ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n## \n## Estimated degrees of freedom:\n## 2.61  total = 3.61 \n## \n## REML score: 29.47878\ntext(-1, 1.5, \"pH\", col = \"red\")\nplot(dca, display = \"sites\", type = \"points\")\nordisurf(dca, ssit$Waterlev.av, add = TRUE, col = \"blue\")\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## y ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n## \n## Estimated degrees of freedom:\n## 6.23  total = 7.23 \n## \n## REML score: 161.1293\ntext(-1, 1.5, \"Wasserstand\", col = \"blue\")\n\n\n\n\n## Dasselbe mit NMDS\nmde &lt;- vegdist(sveg, method = \"euclidean\")\nmmds &lt;- metaMDS(mde)\n## Run 0 stress 0.1478603 \n## Run 1 stress 0.1462959 \n## ... New best solution\n## ... Procrustes: rmse 0.02519273  max resid 0.1450183 \n## Run 2 stress 0.148944 \n## Run 3 stress 0.1489369 \n## Run 4 stress 0.1675565 \n## Run 5 stress 0.1478603 \n## Run 6 stress 0.1757065 \n## Run 7 stress 0.1471649 \n## Run 8 stress 0.2046861 \n## Run 9 stress 0.1879661 \n## Run 10 stress 0.171699 \n## Run 11 stress 0.1489369 \n## Run 12 stress 0.1932706 \n## Run 13 stress 0.1478582 \n## Run 14 stress 0.1462813 \n## ... New best solution\n## ... Procrustes: rmse 0.002057664  max resid 0.01262876 \n## Run 15 stress 0.1770556 \n## Run 16 stress 0.171699 \n## Run 17 stress 0.1673558 \n## Run 18 stress 0.1802728 \n## Run 19 stress 0.1616687 \n## Run 20 stress 0.1923261 \n## *** Best solution was not repeated -- monoMDS stopping criteria:\n##     14: stress ratio &gt; sratmax\n##      6: scale factor of the gradient &lt; sfgrmin\nif(!require(MASS)){install.packages(\"MASS\")}\nlibrary(MASS)\nimds &lt;- isoMDS(mde)\n## initial  value 21.981028 \n## iter   5 value 15.595142\n## iter  10 value 15.269201\n## final  value 15.229997 \n## converged\n\npar(mfrow=c(2,2))\nplot(mmds$points)\nordisurf(mmds, ssit$pH.peat, add = TRUE)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## y ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n## \n## Estimated degrees of freedom:\n## 5.99  total = 6.99 \n## \n## REML score: 41.84455\ntext(-4, 4, \"pH\", col = \"red\")\nplot(mmds$points)\nordisurf(mmds, ssit$Waterlev.av,add = TRUE, col = \"blue\")\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## y ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n## \n## Estimated degrees of freedom:\n## 6.32  total = 7.32 \n## \n## REML score: 168.983\ntext(-4, 4, \"Wasserstand\", col = \"blue\")\nplot(imds$points)\nordisurf(imds, ssit$pH.peat, add = TRUE)\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## y ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n## \n## Estimated degrees of freedom:\n## 7.06  total = 8.06 \n## \n## REML score: 37.68641\ntext(-4, 4, \"pH\", col = \"red\")\nplot(imds$points)\nordisurf(imds, ssit$Waterlev.av, add = T, col = \"blue\")\n## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## y ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n## \n## Estimated degrees of freedom:\n## 6.01  total = 7.01 \n## \n## REML score: 167.6801\ntext(-4, 4, \"Wasserstand\", col = \"blue\")\n\n\n\n\n\n\nConstrained ordination\n\n# Umweltvariablen wäheln, durch die die Ordination constrained werden soll\nnames(ssit)\n##  [1] \"pH.peat\"        \"log.ash.perc\"   \"Ca_peat\"        \"Mg_peat\"       \n##  [5] \"Na_peat\"        \"K_peat\"         \"Acidity.peat\"   \"CEC.peat\"      \n##  [9] \"Base.sat.perc\"  \"P.peat\"         \"Waterlev.max\"   \"Waterlev.av\"   \n## [13] \"Waterlev.min\"   \"log.peat.lev\"   \"log.slope.deg\"  \"pH.water\"      \n## [17] \"log.cond.water\" \"log.Ca.water\"   \"x.axis\"         \"y.axis\"\n\n# 5 Variablen wählen\ns5 &lt;- c(\"pH.peat\", \"P.peat\", \"Waterlev.av\", \"CEC.peat\", \"Acidity.peat\")\nssit5 &lt;- ssit[s5]\n\n\n\npar(mfrow = c(1,2))\n\n# RDA = constrained PCA\nrda &lt;- rda(sveg~., ssit5)\nplot(rda)\n\n# CCA = constrained CA\ncca &lt;- cca(sveg~., ssit5)\nplot(cca)\n\n\n\n\n# Unconstrained and constrained variance\ntot &lt;- cca$tot.chi\nconstr &lt;- cca$CCA$tot.chi\nconstr / tot # Erklärte Varianz\n## [1] 0.2831132\n\n\n\nRedundancy analysis (RDA)\nMehr Details zu RDA aus Borcard u. a. (2011)\n\n# Datensatz Doubs in den workspace laden\nload(\"datasets/statistik/Doubs.RData\")  \n\n\n# Daten anschauen\nsummary(spe)\n##       Cogo           Satr           Phph            Babl            Thth     \n##  Min.   :0.00   Min.   :0.00   Min.   :0.000   Min.   :0.000   Min.   :0.00  \n##  1st Qu.:0.00   1st Qu.:0.00   1st Qu.:0.000   1st Qu.:1.000   1st Qu.:0.00  \n##  Median :0.00   Median :1.00   Median :3.000   Median :2.000   Median :0.00  \n##  Mean   :0.50   Mean   :1.90   Mean   :2.267   Mean   :2.433   Mean   :0.50  \n##  3rd Qu.:0.75   3rd Qu.:3.75   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:0.75  \n##  Max.   :3.00   Max.   :5.00   Max.   :5.000   Max.   :5.000   Max.   :4.00  \n##       Teso             Chna          Pato             Lele      \n##  Min.   :0.0000   Min.   :0.0   Min.   :0.0000   Min.   :0.000  \n##  1st Qu.:0.0000   1st Qu.:0.0   1st Qu.:0.0000   1st Qu.:0.000  \n##  Median :0.0000   Median :0.0   Median :0.0000   Median :1.000  \n##  Mean   :0.6333   Mean   :0.6   Mean   :0.8667   Mean   :1.433  \n##  3rd Qu.:0.7500   3rd Qu.:1.0   3rd Qu.:2.0000   3rd Qu.:2.000  \n##  Max.   :5.0000   Max.   :3.0   Max.   :4.0000   Max.   :5.000  \n##       Sqce            Baba            Albi          Gogo            Eslu      \n##  Min.   :0.000   Min.   :0.000   Min.   :0.0   Min.   :0.000   Min.   :0.000  \n##  1st Qu.:1.000   1st Qu.:0.000   1st Qu.:0.0   1st Qu.:0.000   1st Qu.:0.000  \n##  Median :2.000   Median :0.000   Median :0.0   Median :1.000   Median :1.000  \n##  Mean   :1.867   Mean   :1.433   Mean   :0.9   Mean   :1.833   Mean   :1.333  \n##  3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:1.0   3rd Qu.:3.750   3rd Qu.:2.000  \n##  Max.   :5.000   Max.   :5.000   Max.   :5.0   Max.   :5.000   Max.   :5.000  \n##       Pefl          Rham          Legi             Scer          Cyca       \n##  Min.   :0.0   Min.   :0.0   Min.   :0.0000   Min.   :0.0   Min.   :0.0000  \n##  1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.0000   1st Qu.:0.0   1st Qu.:0.0000  \n##  Median :0.5   Median :0.0   Median :0.0000   Median :0.0   Median :0.0000  \n##  Mean   :1.2   Mean   :1.1   Mean   :0.9667   Mean   :0.7   Mean   :0.8333  \n##  3rd Qu.:2.0   3rd Qu.:2.0   3rd Qu.:1.7500   3rd Qu.:1.0   3rd Qu.:1.0000  \n##  Max.   :5.0   Max.   :5.0   Max.   :5.0000   Max.   :5.0   Max.   :5.0000  \n##       Titi          Abbr             Icme          Gyce            Ruru    \n##  Min.   :0.0   Min.   :0.0000   Min.   :0.0   Min.   :0.000   Min.   :0.0  \n##  1st Qu.:0.0   1st Qu.:0.0000   1st Qu.:0.0   1st Qu.:0.000   1st Qu.:0.0  \n##  Median :1.0   Median :0.0000   Median :0.0   Median :0.000   Median :1.0  \n##  Mean   :1.5   Mean   :0.8667   Mean   :0.6   Mean   :1.267   Mean   :2.1  \n##  3rd Qu.:3.0   3rd Qu.:1.0000   3rd Qu.:0.0   3rd Qu.:2.000   3rd Qu.:5.0  \n##  Max.   :5.0   Max.   :5.0000   Max.   :5.0   Max.   :5.000   Max.   :5.0  \n##       Blbj            Alal          Anan     \n##  Min.   :0.000   Min.   :0.0   Min.   :0.00  \n##  1st Qu.:0.000   1st Qu.:0.0   1st Qu.:0.00  \n##  Median :0.000   Median :0.0   Median :0.00  \n##  Mean   :1.033   Mean   :1.9   Mean   :0.90  \n##  3rd Qu.:1.750   3rd Qu.:5.0   3rd Qu.:1.75  \n##  Max.   :5.000   Max.   :5.0   Max.   :5.00\nsummary(env)\n##       dfs              ele             slo              dis       \n##  Min.   :  0.30   Min.   :172.0   Min.   : 0.200   Min.   : 0.84  \n##  1st Qu.: 54.45   1st Qu.:248.0   1st Qu.: 0.525   1st Qu.: 4.20  \n##  Median :175.20   Median :395.0   Median : 1.200   Median :22.10  \n##  Mean   :188.23   Mean   :481.6   Mean   : 3.497   Mean   :22.20  \n##  3rd Qu.:301.73   3rd Qu.:782.0   3rd Qu.: 2.875   3rd Qu.:28.57  \n##  Max.   :453.00   Max.   :934.0   Max.   :48.000   Max.   :69.00  \n##        pH             har              pho              nit       \n##  Min.   :7.700   Min.   : 40.00   Min.   :0.0100   Min.   :0.150  \n##  1st Qu.:7.925   1st Qu.: 84.25   1st Qu.:0.1250   1st Qu.:0.505  \n##  Median :8.000   Median : 89.00   Median :0.2850   Median :1.600  \n##  Mean   :8.050   Mean   : 86.10   Mean   :0.5577   Mean   :1.654  \n##  3rd Qu.:8.100   3rd Qu.: 96.75   3rd Qu.:0.5600   3rd Qu.:2.425  \n##  Max.   :8.600   Max.   :110.00   Max.   :4.2200   Max.   :6.200  \n##       amm              oxy              bod        \n##  Min.   :0.0000   Min.   : 4.100   Min.   : 1.300  \n##  1st Qu.:0.0000   1st Qu.: 8.025   1st Qu.: 2.725  \n##  Median :0.1000   Median :10.200   Median : 4.150  \n##  Mean   :0.2093   Mean   : 9.390   Mean   : 5.117  \n##  3rd Qu.:0.2000   3rd Qu.:10.900   3rd Qu.: 5.275  \n##  Max.   :1.8000   Max.   :12.400   Max.   :16.700\nsummary(spa)\n##        X                Y         \n##  Min.   :  0.00   Min.   : 20.00  \n##  1st Qu.: 80.94   1st Qu.: 42.13  \n##  Median : 96.56   Median : 73.14  \n##  Mean   : 97.58   Mean   : 66.57  \n##  3rd Qu.:130.03   3rd Qu.: 89.13  \n##  Max.   :159.44   Max.   :105.43\n\n\n## Entfernen der Untersuchungsfläche ohne Arten\nspe &lt;- spe[-8, ]\nenv &lt;- env[-8, ]\nspa &lt;- spa[-8, ]\n\n## Karten für 4 Fischarten\npar(mfrow = c(2, 2))\nplot(spa, asp = 1, col = \"brown\", cex = spe$Satr, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Brown trout\")\nlines(spa, col = \"light blue\")\nplot(spa, asp = 1, col = \"brown\", cex = spe$Thth, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Grayling\")\nlines(spa, col = \"light blue\")\nplot(spa, asp = 1, col = \"brown\", cex = spe$Alal, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Bleak\")\nlines(spa, col = \"light blue\")\nplot(spa, asp = 1, col = \"brown\", cex = spe$Titi, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Tench\")\nlines(spa, col = \"light blue\")\n\n\n\n\n# Set aside the variable 'dfs' (distance from the source) for later use\ndfs &lt;- env[, 1]\n# Remove the 'dfs' variable from the env data frame\nenv2 &lt;- env[, -1]\n\n# Recode the slope variable (slo) into a factor (qualitative)\n# variable to show how these are handled in the ordinations\nslo2 &lt;- rep(\".very_steep\", nrow(env))\nslo2[env$slo &lt;= quantile(env$slo)[4]] &lt;- \".steep\"\nslo2[env$slo &lt;= quantile(env$slo)[3]] &lt;- \".moderate\"\nslo2[env$slo &lt;= quantile(env$slo)[2]] &lt;- \".low\"\nslo2 &lt;- factor(slo2, levels = c(\".low\", \".moderate\", \".steep\", \".very_steep\"))\ntable(slo2)\n## slo2\n##        .low   .moderate      .steep .very_steep \n##           8           8           6           7\n\n# Create an env3 data frame with slope as a qualitative variable\nenv3 &lt;- env2\nenv3$slo &lt;- slo2\n\n# Create two subsets of explanatory variables\n# Physiography (upstream-downstream gradient)\nenvtopo &lt;- env2[, c(1 : 3)]\nnames(envtopo)\n## [1] \"ele\" \"slo\" \"dis\"\n# Water quality\nenvchem &lt;- env2[, c(4 : 10)]\nnames(envchem)\n## [1] \"pH\"  \"har\" \"pho\" \"nit\" \"amm\" \"oxy\" \"bod\"\n\n# Hellinger-transform the species dataset\nlibrary(vegan)\nspe.hel &lt;- decostand(spe, \"hellinger\")\n\n\nspe.hel\n\n\n# Redundancy analysis (RDA)\n# RDA of the Hellinger-transformed fish species data, constrained\n# by all the environmental variables contained in env3\nspe.rda &lt;- rda(spe.hel ~ ., env3) # Observe the shortcut formula\n\n\nspe.rda\nsummary(spe.rda)    # Scaling 2 (default)\n\n\n## Canonical coefficients from the rda object\ncoef(spe.rda)\n\n\n## Unadjusted R^2 und Adjusted R^2\n(R2 &lt;- RsquareAdj(spe.rda))\n## $r.squared\n## [1] 0.7270922\n## \n## $adj.r.squared\n## [1] 0.5224114\n\n### Triplots of the rda results (lc scores)\n### Site scores as linear combinations of the environmental variables\n## dev.new(title = \"RDA scaling 1 and 2 + lc\", width = 12, height = 6, noRStudioGD = TRUE)\npar(mfrow = c(1, 2))\n## Scaling 1\nplot(spe.rda,scaling = 1, display = c(\"sp\", \"lc\", \"cn\"), main = \"Triplot RDA spe.hel ~ env3 - scaling 1 - lc scores\")\nspe.sc1 &lt;- scores(spe.rda, choices = 1:2, scaling = 1, display = \"sp\")\narrows(0, 0, spe.sc1[, 1] * 0.92, spe.sc1[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\ntext(-0.75, 0.7, \"a\", cex = 1.5)\n## Scaling 2\nplot(spe.rda, display = c(\"sp\", \"lc\", \"cn\"), main = \"Triplot RDA spe.hel ~ env3 - scaling 2 - lc scores\")\nspe.sc2 &lt;- scores(spe.rda, choices = 1:2, display = \"sp\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0, lty = 1, col = \"red\")\ntext(-0.82, 0.55, \"b\", cex = 1.5)\n\n\n\n\n### Triplots of the rda results (wa scores)\n### Site scores as weighted averages (vegan's default)\n## Scaling 1 :  distance triplot\n##dev.new(title = \"RDA plot\", width = 12, height = 6, noRStudioGD = TRUE)\npar(mfrow = c(1, 2))\nplot(spe.rda, scaling = 1, main = \"Triplot RDA spe.hel ~ env3 - scaling 1 - wa scores\")\narrows(0, 0, spe.sc1[, 1] * 0.92, spe.sc1[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\n## Scaling 2 (default) :  correlation triplot\nplot(spe.rda, main = \"Triplot RDA spe.hel ~ env3 - scaling 2 - wa scores\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\n\n\n\n\n## Select species with goodness-of-fit at least 0.6 in the \n## ordination plane formed by axes 1 and 2\nspe.good &lt;- goodness(spe.rda)\nsel.sp &lt;- which(spe.good[, 2] &gt;= 0.6)\nsel.sp\n## Satr Phph Chna Baba Albi Rham Legi Cyca Abbr Gyce Ruru Blbj Alal Anan \n##    2    3    7   11   12   16   17   19   21   23   24   25   26   27\n\n## Global test of the RDA result\nanova(spe.rda, permutations = how(nperm = 999))\n## Permutation test for rda under reduced model\n## Permutation: free\n## Number of permutations: 999\n## \n## Model: rda(formula = spe.hel ~ ele + slo + dis + pH + har + pho + nit + amm + oxy + bod, data = env3)\n##          Df Variance      F Pr(&gt;F)    \n## Model    12  0.36537 3.5523  0.001 ***\n## Residual 16  0.13714                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Tests of all canonical axes\nanova(spe.rda, by = \"axis\", permutations = how(nperm = 999))\n## Permutation test for rda under reduced model\n## Forward tests for axes\n## Permutation: free\n## Number of permutations: 999\n## \n## Model: rda(formula = spe.hel ~ ele + slo + dis + pH + har + pho + nit + amm + oxy + bod, data = env3)\n##          Df Variance       F Pr(&gt;F)    \n## RDA1      1 0.228083 26.6105  0.001 ***\n## RDA2      1 0.053698  6.2649  0.004 ** \n## RDA3      1 0.032119  3.7473  0.341    \n## RDA4      1 0.023206  2.7074  0.769    \n## RDA5      1 0.008699  1.0149  1.000    \n## RDA6      1 0.007218  0.8421  1.000    \n## RDA7      1 0.004869  0.5681  1.000    \n## RDA8      1 0.002924  0.3412  1.000    \n## RDA9      1 0.002141  0.2498  1.000    \n## RDA10     1 0.001160  0.1353  1.000    \n## RDA11     1 0.000914  0.1066  1.000    \n## RDA12     1 0.000341  0.0397  1.000    \n## Residual 16 0.137139                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n### Partial RDA: effect of water chemistry, holding physiography\n### constant\n\n## Simple syntax; X and W may be in separate tables of quantitative \n## variables\n(spechem.physio &lt;- rda(spe.hel, envchem, envtopo))\n## Call: rda(X = spe.hel, Y = envchem, Z = envtopo)\n## \n##               Inertia Proportion Rank\n## Total          0.5025     1.0000     \n## Conditional    0.2087     0.4152    3\n## Constrained    0.1602     0.3189    7\n## Unconstrained  0.1336     0.2659   18\n## Inertia is variance \n## \n## Eigenvalues for constrained axes:\n##    RDA1    RDA2    RDA3    RDA4    RDA5    RDA6    RDA7 \n## 0.09136 0.04590 0.00928 0.00625 0.00387 0.00214 0.00142 \n## \n## Eigenvalues for unconstrained axes:\n##     PC1     PC2     PC3     PC4     PC5     PC6     PC7     PC8 \n## 0.04643 0.02071 0.01746 0.01326 0.00975 0.00588 0.00512 0.00400 \n## (Showing 8 of 18 unconstrained eigenvalues)\n\n\nsummary(spechem.physio)\n\n\n## Formula interface; X and W variables must be in the same \n## data frame\n(spechem.physio2 &lt;- rda(spe.hel ~ pH + har + pho + nit + amm + oxy + bod \n        + Condition(ele + slo + dis), data = env2))\n## Call: rda(formula = spe.hel ~ pH + har + pho + nit + amm + oxy + bod +\n## Condition(ele + slo + dis), data = env2)\n## \n##               Inertia Proportion Rank\n## Total          0.5025     1.0000     \n## Conditional    0.2087     0.4152    3\n## Constrained    0.1602     0.3189    7\n## Unconstrained  0.1336     0.2659   18\n## Inertia is variance \n## \n## Eigenvalues for constrained axes:\n##    RDA1    RDA2    RDA3    RDA4    RDA5    RDA6    RDA7 \n## 0.09136 0.04590 0.00928 0.00625 0.00387 0.00214 0.00142 \n## \n## Eigenvalues for unconstrained axes:\n##     PC1     PC2     PC3     PC4     PC5     PC6     PC7     PC8 \n## 0.04643 0.02071 0.01746 0.01326 0.00975 0.00588 0.00512 0.00400 \n## (Showing 8 of 18 unconstrained eigenvalues)\n\n## Test of the partial RDA, using the results with the formula \n## interface to allow the tests of the axes to be run\nanova(spechem.physio2, permutations = how(nperm = 999))\n## Permutation test for rda under reduced model\n## Permutation: free\n## Number of permutations: 999\n## \n## Model: rda(formula = spe.hel ~ pH + har + pho + nit + amm + oxy + bod + Condition(ele + slo + dis), data = env2)\n##          Df Variance      F Pr(&gt;F)    \n## Model     7  0.16023 3.0836  0.001 ***\n## Residual 18  0.13362                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(spechem.physio2, permutations = how(nperm = 999), by = \"axis\")\n## Permutation test for rda under reduced model\n## Forward tests for axes\n## Permutation: free\n## Number of permutations: 999\n## \n## Model: rda(formula = spe.hel ~ pH + har + pho + nit + amm + oxy + bod + Condition(ele + slo + dis), data = env2)\n##          Df Variance       F Pr(&gt;F)    \n## RDA1      1 0.091363 12.3078  0.001 ***\n## RDA2      1 0.045904  6.1839  0.008 ** \n## RDA3      1 0.009277  1.2497  0.967    \n## RDA4      1 0.006250  0.8420  0.994    \n## RDA5      1 0.003868  0.5210  0.999    \n## RDA6      1 0.002145  0.2890  1.000    \n## RDA7      1 0.001424  0.1919  0.997    \n## Residual 18 0.133617                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nVariation partitionig\n\n### Variation partitioning with two sets of explanatory variables\n\n## Explanation of fraction labels (two, three and four explanatory \n## matrices) with optional colours\npar(mfrow = c(1, 3), mar = c(1, 1, 1, 1))\nshowvarparts(2, bg = c(\"red\", \"blue\"))\nshowvarparts(3, bg = c(\"red\", \"blue\", \"yellow\"))\nshowvarparts(4, bg = c(\"red\", \"blue\", \"yellow\", \"green\"))\n\n\n\n\n### 1. Variation partitioning with all explanatory variables\n###    (except dfs)\n(spe.part.all &lt;- varpart(spe.hel, envchem, envtopo))\n## \n## Partition of variance in RDA \n## \n## Call: varpart(Y = spe.hel, X = envchem, envtopo)\n## \n## Explanatory tables:\n## X1:  envchem\n## X2:  envtopo \n## \n## No. of explanatory tables: 2 \n## Total variation (SS): 14.07 \n##             Variance: 0.50251 \n## No. of observations: 29 \n## \n## Partition table:\n##                      Df R.squared Adj.R.squared Testable\n## [a+c] = X1            7   0.60579       0.47439     TRUE\n## [b+c] = X2            3   0.41524       0.34507     TRUE\n## [a+b+c] = X1+X2      10   0.73410       0.58638     TRUE\n## Individual fractions                                    \n## [a] = X1|X2           7                 0.24131     TRUE\n## [b] = X2|X1           3                 0.11199     TRUE\n## [c]                   0                 0.23308    FALSE\n## [d] = Residuals                         0.41362    FALSE\n## ---\n## Use function 'rda' to test significance of fractions of interest\n\n## Plot of the partitioning results\npar(mfrow = c(1, 1))\nplot(spe.part.all, digits = 2, bg = c(\"red\", \"blue\"),\n     Xnames = c(\"Chemistry\", \"Physiography\"), \n     id.size = 0.7)\n\n\n\n\n\n\n\n\n\n\nBorcard, Daniel, François Gillet, Pierre Legendre, u. a. 2011. Numerical ecology with R. Bd. 2. Springer."
  },
  {
    "objectID": "stat5-8/Statistik7_Uebung.html#übung-7.1-rda-naturwissenschaftlich",
    "href": "stat5-8/Statistik7_Uebung.html#übung-7.1-rda-naturwissenschaftlich",
    "title": "Stat7: Übung",
    "section": "Übung 7.1: RDA (naturwissenschaftlich)",
    "text": "Übung 7.1: RDA (naturwissenschaftlich)\nMoordatensatz in library(dave) :\n\nsveg (Vegetationsdaten)\nssit (Umweltdaten)\n\nFührt eine RDA mit allen in der Vorlesung gezeigten Schritten durch und interpretiert die Ergebnisse.\nVon den Umweltvariablen entfallen x.axis & y.axis\nFür die partielle RDA und die Varianzpartitionierung bildet zwei Gruppen:\n\nPhysiographie (Waterlev.max, Waterlev.av, Waterlev.min, log.peat.lev, log slope.deg)\nChemie (alle übrigen)"
  },
  {
    "objectID": "stat5-8/Statistik7_Loesung.html#musterlösung-aufgabe-7.1-rda",
    "href": "stat5-8/Statistik7_Loesung.html#musterlösung-aufgabe-7.1-rda",
    "title": "Stat7: Lösung",
    "section": "Musterlösung Aufgabe 7.1: RDA",
    "text": "Musterlösung Aufgabe 7.1: RDA\nÜbungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\n\nLadet die library dave, welche den Moordatensatz enthält. sveg beinhaltet presenceabsence-Daten aller untersuchten Arten in den Plots; ssit beinhaltet 18 metrische Umweltdaten sowie Koordinaten der Plots\nFührt eine RDA und eine Varianzpartizionierung in die Variablengruppen Physiographie (Waterlev.max, Waterlev.av, Waterlev.min, log.peat.lev, log slope.deg) und Chemie (alle übrigen) durch.\nFormuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nWährend im Text normalerweise die Variablen ausgeschrieben werden solltet, genügt es hier (da ihr die entsprechenden Infos nicht bekommen habt und nur raten könntet), wenn ihr die Abkürzungen aus dem dataframe nehmt.\n\nMoordatensatz laden\n\nif (!require(dave)) {\n    install.packages(\"dave\")\n}\nlibrary(dave)\ndata(sveg)\ndata(ssit)\n\n\nsummary(sveg)\nsummary(ssit)\nstr(ssit)\n\n\n# x.axis and y.axis vom data frame data frame ssit entfernen\nenv2 &lt;- ssit[, -c(19, 20)]\n\nBetrachtung der Daten zeigt, dass die Koordinaten in Spalten 19 und 20 sind, die daraufhin entfernt werden.\n\n# Generiere zwei subset der erklärenden Variablen Physiografie\n# (upstream-downstream-Gradient)\nenvtopo &lt;- env2[, c(11:15)]\nnames(envtopo)\n## [1] \"Waterlev.max\"  \"Waterlev.av\"   \"Waterlev.min\"  \"log.peat.lev\" \n## [5] \"log.slope.deg\"\n# Chemie\nenvchem &lt;- env2[, c(1:10, 16:18)]\nnames(envchem)\n##  [1] \"pH.peat\"        \"log.ash.perc\"   \"Ca_peat\"        \"Mg_peat\"       \n##  [5] \"Na_peat\"        \"K_peat\"         \"Acidity.peat\"   \"CEC.peat\"      \n##  [9] \"Base.sat.perc\"  \"P.peat\"         \"pH.water\"       \"log.cond.water\"\n## [13] \"log.Ca.water\"\n\n# Hellinger-transform the species dataset\nlibrary(vegan)\nspe.hel &lt;- decostand(sveg, \"hellinger\")\n\nVorstehend wurden die Variablen in die zwei Gruppen Chemistry und Physiography aufgteilt. Die Hellilnger-Transformation wird gemeinhin empfohlen (wobei dahingestellt sei, ob sie auch bei presence-absence-Daten nötig ist). Die weiteren Analysen führen wir mit der default-Einstellung „Scaling 2“ durch. (Je nach Bedarf bzw. persönlichen Vorlieben könnte auch Scaling 1 genommen werden).\nRedundancy analysis (RDA)\nRDA of the Hellinger-transformed mire species data, constrained by all the environmental variables contained in env2\n\n## RDA der Hellinger-transformireten Moorarten-Daten, constrained mit allen\n## Umweltvarialben die in env2 enthalten sind\n(spe.rda &lt;- rda(spe.hel ~ ., env2))  # Observe the shortcut formula\n## Call: rda(formula = spe.hel ~ pH.peat + log.ash.perc + Ca_peat +\n## Mg_peat + Na_peat + K_peat + Acidity.peat + CEC.peat + Base.sat.perc +\n## P.peat + Waterlev.max + Waterlev.av + Waterlev.min + log.peat.lev +\n## log.slope.deg + pH.water + log.cond.water + log.Ca.water, data = env2)\n## \n##               Inertia Proportion Rank\n## Total          0.4979     1.0000     \n## Constrained    0.2773     0.5569   18\n## Unconstrained  0.2206     0.4431   44\n## Inertia is variance \n## \n## Eigenvalues for constrained axes:\n##    RDA1    RDA2    RDA3    RDA4    RDA5    RDA6    RDA7    RDA8    RDA9   RDA10 \n## 0.13693 0.03784 0.01860 0.01110 0.00950 0.00882 0.00735 0.00670 0.00603 0.00582 \n##   RDA11   RDA12   RDA13   RDA14   RDA15   RDA16   RDA17   RDA18 \n## 0.00498 0.00478 0.00411 0.00394 0.00310 0.00294 0.00271 0.00205 \n## \n## Eigenvalues for unconstrained axes:\n##      PC1      PC2      PC3      PC4      PC5      PC6      PC7      PC8 \n## 0.018768 0.015312 0.013231 0.012074 0.011420 0.009385 0.008904 0.008639 \n## (Showing 8 of 44 unconstrained eigenvalues)\n\n\nsummary(spe.rda)  # Skalierung 2 (default)\n\n\n# Canonical coefficients from the rda object\ncoef(spe.rda)\n##                         RDA1          RDA2         RDA3         RDA4\n## pH.peat         4.531329e-02  0.0828973389  0.010965288 -0.155085533\n## log.ash.perc   -1.949986e-02 -0.0359844539 -0.701312936  0.034317746\n## Ca_peat        -1.337537e-03 -0.0288554798  0.027228621  0.068880266\n## Mg_peat        -3.936852e-02  0.0901067458 -0.141325150 -0.070256979\n## Na_peat        -9.247087e-02  0.4384377252  0.088851211 -0.248295480\n## K_peat          6.296120e-02 -0.0699716499  0.015460922  0.314006668\n## Acidity.peat    4.708024e-05 -0.0277547066  0.005448542  0.043747599\n## CEC.peat        3.744129e-03  0.0297812515 -0.010532610 -0.040816301\n## Base.sat.perc   1.129368e-03 -0.0041436746 -0.013166371 -0.005538586\n## P.peat         -1.097201e-02 -0.0352965867 -0.063285184 -0.019908698\n## Waterlev.max   -3.179331e-03 -0.0006509661 -0.015533249  0.038929542\n## Waterlev.av     8.236051e-04 -0.0049269233  0.027915286 -0.018711058\n## Waterlev.min    3.830259e-04 -0.0009284990  0.002283015  0.002325086\n## log.peat.lev   -1.168763e-01  0.1415162776  0.002413566 -0.271470076\n## log.slope.deg  -3.383155e-02  0.0016520826 -0.236646952  0.076539930\n## pH.water        6.367244e-02  0.0538977579  0.079825940  0.179498186\n## log.cond.water  6.110612e-03 -0.2600161375 -0.162560478  0.093945551\n## log.Ca.water    1.317034e-02 -0.0622061756 -0.023932814 -0.597437862\n##                         RDA5          RDA6         RDA7         RDA8\n## pH.peat        -0.1892255621  0.2118068056  0.071909611 -0.027538260\n## log.ash.perc    0.1802037523  0.2669540104  0.724618282 -0.633888225\n## Ca_peat        -0.0007103952 -0.0253977768  0.029045044 -0.021223056\n## Mg_peat         0.0994092273  0.1176642458  0.100095923  0.381181043\n## Na_peat         2.2386515374 -0.7455515343  0.664234477 -0.718624356\n## K_peat          0.1287939324  0.2046202165 -0.441055492 -0.417918725\n## Acidity.peat    0.0519802264 -0.0270485440  0.071957387 -0.032035431\n## CEC.peat       -0.0472321525  0.0329326428 -0.053476411  0.032372963\n## Base.sat.perc   0.0276382747 -0.0003004136  0.012777988  0.002256490\n## P.peat         -0.0401765601 -0.0112976551  0.068766100  0.003460905\n## Waterlev.max   -0.0456458872  0.1345249731  0.097859198 -0.022480057\n## Waterlev.av     0.0355444546 -0.0722625146 -0.090790927  0.036951069\n## Waterlev.min   -0.0105108162  0.0136267137  0.017986436 -0.006832735\n## log.peat.lev   -0.5267908795  0.2162043424 -0.274818126 -0.338891940\n## log.slope.deg  -0.2184244227 -0.1500482029 -0.105292557  0.095365402\n## pH.water       -0.0425049325 -0.1782004196 -0.059654515 -0.114988275\n## log.cond.water  0.0168002458 -0.1815625052 -0.007997259 -0.464849052\n## log.Ca.water   -0.1033058867  0.3899181259  0.078901455  0.315891985\n##                        RDA9        RDA10       RDA11        RDA12        RDA13\n## pH.peat         0.012382751 -0.017468114  0.06519173 -0.027412762 -0.104975398\n## log.ash.perc    0.965668438 -0.028793591 -0.44461671  0.129536979  0.377398995\n## Ca_peat         0.012619772 -0.107438326  0.04460841 -0.103406142  0.114429353\n## Mg_peat         0.158294076 -0.280166029 -0.05634171 -0.070201766  0.268732885\n## Na_peat        -1.104751868  1.544769984  1.39741894 -0.039845605  1.780098125\n## K_peat          0.534198927 -0.076296838  0.22435445 -0.163742387 -0.123997869\n## Acidity.peat    0.068974997 -0.087677482  0.06530877 -0.046256867  0.033805349\n## CEC.peat       -0.047560024  0.083210188 -0.06665024  0.077515801 -0.068579244\n## Base.sat.perc   0.020829815  0.009830788 -0.00910173  0.022643234 -0.034283054\n## P.peat         -0.012695344 -0.003138906 -0.03352924 -0.008483557  0.043199360\n## Waterlev.max   -0.012606335 -0.048317756 -0.09623737 -0.013544370 -0.032439161\n## Waterlev.av    -0.004208039  0.047229561  0.08062624 -0.005265601  0.023482438\n## Waterlev.min    0.001054475 -0.005577274 -0.01298239  0.007069788  0.002347459\n## log.peat.lev   -0.003928806 -0.238217788 -0.30310954  0.563613618  0.160866568\n## log.slope.deg   0.057527637  0.291653459  0.22134679  0.391384381  0.236877948\n## pH.water       -0.212751287  0.111459569  0.01131132 -0.090296584  0.120070232\n## log.cond.water -0.668937274 -0.008842423 -0.03594579 -0.809188635  0.224126901\n## log.Ca.water    1.090714519 -0.887533762  0.45040221  0.927233456 -0.402746243\n##                       RDA14        RDA15         RDA16       RDA17\n## pH.peat         0.067972158  0.018701068  0.1485174317  0.13349865\n## log.ash.perc    0.194930772  0.919823359 -0.0460514789 -1.35532652\n## Ca_peat        -0.021190482 -0.045772935 -0.0586146947  0.14093897\n## Mg_peat         0.350138790  0.113874369  0.0939318308  0.04980116\n## Na_peat        -0.278280645 -0.213777421 -0.8329460427 -1.09568980\n## K_peat          0.105394480 -0.062226421 -0.0750622607 -0.05144200\n## Acidity.peat    0.096815006  0.023817495 -0.0702792017  0.09633424\n## CEC.peat       -0.054680669 -0.005738998  0.0663748635 -0.11471933\n## Base.sat.perc   0.039890903  0.034786341 -0.0054775951 -0.00801054\n## P.peat         -0.071393270  0.052202136  0.0478902318  0.02965954\n## Waterlev.max   -0.061094560 -0.047338261  0.0032500492  0.07021013\n## Waterlev.av     0.037716665  0.061946296 -0.0002491098 -0.05976381\n## Waterlev.min   -0.007026893 -0.017849338  0.0015009609  0.01493811\n## log.peat.lev    0.263156404  0.415682834 -0.5691321204  0.16806725\n## log.slope.deg   0.219914297 -0.271490962 -0.4115760623 -0.06629741\n## pH.water        0.141903633 -0.018249431 -0.0414434431 -0.04361964\n## log.cond.water  0.817975382 -0.634193576  0.1319661957 -0.28691611\n## log.Ca.water   -1.358028534  0.192611956 -0.2582206194 -0.02520368\n##                        RDA18\n## pH.peat        -0.1701622100\n## log.ash.perc   -0.4171590020\n## Ca_peat        -0.1568432425\n## Mg_peat         0.2684650074\n## Na_peat         0.3478060087\n## K_peat         -0.1752018988\n## Acidity.peat   -0.0918662915\n## CEC.peat        0.0870338687\n## Base.sat.perc   0.0140300438\n## P.peat         -0.0140083088\n## Waterlev.max    0.0017944793\n## Waterlev.av     0.0001508213\n## Waterlev.min   -0.0034494643\n## log.peat.lev    0.1448115821\n## log.slope.deg   0.0655661508\n## pH.water        0.1695726179\n## log.cond.water -0.3588429780\n## log.Ca.water    0.2342020703\n# Unadjusted R^2 retrieved from the rda object\n(R2 &lt;- RsquareAdj(spe.rda)$r.squared)\n## [1] 0.5569395\n# Adjusted R^2 retrieved from the rda object\n(R2adj &lt;- RsquareAdj(spe.rda)$adj.r.squared)\n## [1] 0.3756874\n\nMan erhält R²adj. = 0.376 Jetzt kann man den Triplot erstellen\n\n## Triplots of the rda results (lc scores) Site scores as linear combinations\n## of the environmental variables dev.new(title = 'RDA scaling 1 and 2 + lc',\n## width = 15, height = 6, noRStudioGD = TRUE) par(mfrow = c(1, 2))\n\n# 1 und 2 Achse\nplot(spe.rda, display = c(\"sp\", \"lc\", \"cn\"), main = \"Triplot RDA spe.hel ~ env2 - scaling 2 - lc scores\")\nspe.sc2 &lt;- scores(spe.rda, choices = 1:2, display = \"sp\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\ntext(-0.82, 0.55, \"b\", cex = 1.5)\n\n\n\n\n# 1 und 3 Achse\nplot(spe.rda, display = c(\"sp\", \"lc\", \"cn\"), choices = c(1, 3), main = \"Triplot RDA spe.hel ~ env2 - scaling 2 - lc scores\")\nspe.sc2 &lt;- scores(spe.rda, choices = c(1, 3), display = \"sp\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\ntext(-0.82, 0.55, \"b\", cex = 1.5)\n\n\n\n\n## Triplots of the rda results (wa scores) Site scores as weighted averages\n## (vegan's default) Scaling 1 : distance triplot dev.new(title = 'RDA scaling\n## 2 + wa',width = 7, height = 6, noRStudioGD = TRUE)\n\n# Scaling 2 (default) : correlation triplot\nplot(spe.rda, main = \"Triplot RDA spe.hel ~ env3 - scaling 2 - wa scores\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\n\n\n\n\nAuswahl der höchstkorrelierten Arten (Grenzwert kann subjektiv nach Bedarf gesetzt werden, hier 0.5).\n\n# Select species with goodness-of-fit at least 0.6 in the ordination plane\n# formed by axes 1 and 2\nspe.good &lt;- goodness(spe.rda)\nsel.sp &lt;- which(spe.good[, 2] &gt;= 0.6)\nsel.sp\n\n\n# Global test of the RDA result\nanova(spe.rda, permutations = how(nperm = 999))\n\n# Tests of all canonical axes\nanova(spe.rda, permutations = how(nperm = 999))\nanova(spe.rda, by = \"axis\", permutations = how(nperm = 999))\n\nDie ersten drei RDA-Achsen sind also signifikant. Man könnte also auch noch eine Visualisierung von RDA 3 vs. RDA 1 machen.\n\nPartielle RDA\nSimple syntax; X and W may be in separate tables of quantitative\nvariables\n\n\nspechem.physio &lt;- rda(spe.hel, envchem, envtopo)\nsummary(spechem.physio)\n\n# Formula interface; X and W variables must be in the same data frame\n(spechem.physio2 &lt;- rda(spe.hel ~ pH.peat + log.ash.perc + Ca_peat + Mg_peat + Na_peat +\n    K_peat + Acidity.peat + CEC.peat + Base.sat.perc + P.peat + pH.water + log.cond.water +\n    log.Ca.water + Condition(Waterlev.max + Waterlev.av + Waterlev.min + log.peat.lev +\n    log.slope.deg), data = env2))\n\n# Test of the partial RDA, using the results with the formula interface to\n# allow the tests of the axes to be run\nanova(spechem.physio2, permutations = how(nperm = 999))\nanova(spechem.physio2, permutations = how(nperm = 999), by = \"axis\")\n\n# Partial RDA triplots (with fitted site scores) with function triplot.rda\n# dev.new(title = 'Partial RDA', width = 7, height = 6, noRStudioGD = TRUE)\n\ntriplot.rda(spechem.physio, site.sc = \"lc\", scaling = 2, cex.char2 = 0.8, pos.env = 3,\n    mult.spe = 1.1, mar.percent = 0.04)\ntext(-3.34, 3.64, \"b\", cex = 2)\n\nVarianzpartitionierung\n\n## 1. Variation partitioning with all explanatory variables\n(spe.part.all &lt;- varpart(spe.hel, envchem, envtopo))\n## \n## Partition of variance in RDA \n## \n## Call: varpart(Y = spe.hel, X = envchem, envtopo)\n## \n## Explanatory tables:\n## X1:  envchem\n## X2:  envtopo \n## \n## No. of explanatory tables: 2 \n## Total variation (SS): 30.873 \n##             Variance: 0.49795 \n## No. of observations: 63 \n## \n## Partition table:\n##                      Df R.squared Adj.R.squared Testable\n## [a+c] = X1           13   0.47805       0.33958     TRUE\n## [b+c] = X2            5   0.24779       0.18181     TRUE\n## [a+b+c] = X1+X2      18   0.55694       0.37569     TRUE\n## Individual fractions                                    \n## [a] = X1|X2          13                 0.19388     TRUE\n## [b] = X2|X1           5                 0.03611     TRUE\n## [c]                   0                 0.14570    FALSE\n## [d] = Residuals                         0.62431    FALSE\n## ---\n## Use function 'rda' to test significance of fractions of interest\n\n# Plot of the partitioning results dev.new(title = 'Variation partitioning',\n# width = 7, height = 7, noRStudioGD = TRUE)\n\nplot(spe.part.all, digits = 2, bg = c(\"red\", \"blue\"), Xnames = c(\"Chemistry\", \"Physiography\"),\n    id.size = 0.7)\n\n\n\n\nDie durch die erhobenen Umweltvariablen insgesamt erklärte Varianz (37.6%, s.o.) entfällt zu 19.4% auf chemische Variablen, 3.6% auf physiographische Variablen und zu 14.6% auf gemeinsame Erklärung."
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#k-means-clustering",
    "href": "stat5-8/Statistik8_Demo.html#k-means-clustering",
    "title": "Stat8: Demo",
    "section": "k-means clustering",
    "text": "k-means clustering\n\n# Das Moordatenset sveg laden\nif(!require(dave)){install.packages(\"dave\")}\nlibrary(dave)\n\n# PCA und CA rechnen\npca &lt;- rda(sveg^0.25, scale = TRUE)\nca &lt;- cca(sveg^0.5)\n\n# k-means-Clustering mit 4 Gruppen durchführen\nkmeans.1 &lt;- kmeans(sveg, 4)\n\n\n# Clustering-Resultat in Ordinationsplots darstellen\nplot(ca, type = \"n\")\npoints(ca, display = \"sites\", pch=19, col = kmeans.1[[1]])\n\n\n\n\n# k-means Clustering mit 3 Clusters\nkmeans.2 &lt;- kmeans(sveg, 3)\nplot(pca, type = \"n\")\npoints(pca, display = \"sites\", pch=19, col = kmeans.2[[1]])\n\n\n\n\n# mit 3. Achse darstellen\nplot(pca, choices = c(1, 3), type = \"n\")\npoints(pca, choices = c(1, 3), display = \"sites\", pch = 19, col=kmeans.2[[1]])\n\n\n\n\n# k-means partitioning, 2 to 10 groups\nKM.cascade &lt;- cascadeKM(sveg,  inf.gr = 2, sup.gr = 10, iter = 100, criterion = \"ssi\")\nsummary(KM.cascade)\n##           Length Class  Mode     \n## partition 567    -none- numeric  \n## results    18    -none- numeric  \n## criterion   1    -none- character\n## size       90    -none- numeric\nKM.cascade$results\n##       2 groups     3 groups     4 groups     5 groups     6 groups     7 groups\n## SSE 1840.13571 1629.4399038 1488.2961538 1378.3369048 1286.5005411 1214.3219697\n## ssi    0.26103    0.2706397    0.3446644    0.3261505    0.3895142    0.4532609\n##         8 groups     9 groups    10 groups\n## SSE 1156.7314935 1101.5523810 1053.1476190\n## ssi    0.3487447    0.4841782    0.5144259\nKM.cascade$partition\n##     2 groups 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups\n## 501        2        3        1        4        6        3        8        6\n## 502        2        3        3        2        1        5        6        9\n## 503        2        3        1        4        6        3        8        6\n## 504        2        3        1        4        6        3        8        6\n## 505        2        3        3        2        1        5        6        9\n## 506        2        3        1        4        1        5        6        9\n## 507        2        3        1        4        6        3        8        6\n## 508        2        3        1        4        6        3        8        6\n## 509        2        3        1        4        6        3        8        6\n## 510        2        3        3        2        2        4        7        3\n## 511        2        3        1        4        6        3        8        6\n## 512        2        3        3        2        1        5        6        9\n## 513        2        3        3        2        2        4        7        8\n## 514        2        3        3        2        1        5        6        9\n## 515        2        3        3        2        1        5        6        9\n## 516        2        3        3        2        1        5        6        9\n## 517        2        3        3        2        2        4        7        3\n## 518        1        1        2        1        4        7        4        5\n## 519        2        3        3        2        1        5        6        9\n## 520        2        3        3        2        2        4        7        3\n## 521        2        3        1        4        6        3        8        3\n## 522        2        3        3        2        1        5        6        9\n## 523        1        1        2        1        4        7        4        5\n## 524        2        3        1        4        6        3        8        3\n## 525        2        3        1        4        6        3        8        3\n## 526        1        2        4        1        4        7        4        5\n## 527        2        3        3        2        2        4        7        8\n## 528        2        3        3        2        2        4        7        8\n## 529        2        3        3        2        2        4        7        8\n## 530        2        3        3        2        2        4        7        3\n## 531        1        1        2        5        3        1        2        2\n## 532        2        3        3        2        2        4        7        8\n## 533        2        3        3        2        2        4        7        3\n## 534        2        1        2        5        3        1        2        2\n## 535        1        1        2        5        3        1        2        2\n## 536        2        3        3        2        2        4        7        8\n## 537        2        3        3        2        2        4        7        8\n## 538        1        1        2        1        4        7        4        5\n## 539        1        2        4        3        5        2        1        4\n## 540        2        3        1        4        6        3        8        6\n## 541        1        2        4        3        5        2        5        1\n## 542        1        2        4        3        5        2        1        4\n## 543        1        2        4        3        5        2        5        1\n## 544        2        1        2        5        3        1        2        2\n## 545        1        1        2        1        4        7        4        5\n## 546        1        1        2        5        3        1        2        2\n## 547        2        1        2        5        3        1        2        8\n## 548        1        2        4        3        5        2        5        1\n## 549        1        1        2        5        3        1        2        2\n## 550        1        1        2        5        3        1        2        2\n## 551        1        2        4        1        4        6        3        7\n## 552        1        2        4        3        5        2        5        1\n## 553        1        2        4        3        5        2        3        7\n## 554        1        2        4        1        4        6        3        7\n## 555        1        2        4        1        4        6        3        7\n## 556        1        2        4        3        5        2        5        1\n## 557        1        2        4        1        4        6        3        7\n## 558        1        2        4        1        4        6        3        7\n## 559        1        2        4        1        4        7        4        5\n## 560        1        2        4        1        4        7        4        5\n## 561        1        1        2        1        4        7        4        5\n## 562        1        2        4        3        5        2        1        4\n## 563        1        2        4        1        4        6        3        7\n##     10 groups\n## 501         4\n## 502         1\n## 503         4\n## 504         4\n## 505         1\n## 506         1\n## 507         4\n## 508         4\n## 509         4\n## 510         3\n## 511         2\n## 512         1\n## 513         9\n## 514         1\n## 515         1\n## 516         1\n## 517         3\n## 518         2\n## 519         1\n## 520         3\n## 521         3\n## 522         1\n## 523         8\n## 524         3\n## 525         3\n## 526         8\n## 527         9\n## 528         9\n## 529         9\n## 530         3\n## 531        10\n## 532         9\n## 533         3\n## 534        10\n## 535        10\n## 536         9\n## 537         9\n## 538         2\n## 539         7\n## 540         4\n## 541         6\n## 542         7\n## 543         6\n## 544        10\n## 545         8\n## 546        10\n## 547         9\n## 548         6\n## 549        10\n## 550        10\n## 551         5\n## 552         6\n## 553         5\n## 554         5\n## 555         5\n## 556         6\n## 557         5\n## 558         5\n## 559         8\n## 560         8\n## 561         8\n## 562         7\n## 563         5\n\n# k-means visualisation\nplot(KM.cascade, sortg = TRUE)"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#agglomarative-clusteranalyse",
    "href": "stat5-8/Statistik8_Demo.html#agglomarative-clusteranalyse",
    "title": "Stat8: Demo",
    "section": "Agglomarative Clusteranalyse",
    "text": "Agglomarative Clusteranalyse\nmit Daten und Skripten aus Borcard u. a. (2011)\n\n# Daten laden\nload(\"datasets/statistik/Doubs.RData\")  \n\n\n# Remove empty site 8\nspe &lt;- spe[-8, ]\nenv &lt;- env[-8, ]\nspa &lt;- spa[-8, ]\nlatlong &lt;- latlong[-8, ]"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#dendogramme-berechnen-und-ploten",
    "href": "stat5-8/Statistik8_Demo.html#dendogramme-berechnen-und-ploten",
    "title": "Stat8: Demo",
    "section": "Dendogramme berechnen und ploten",
    "text": "Dendogramme berechnen und ploten\n\n## Hierarchical agglomerative clustering of the species abundance \n\n# Compute matrix of chord distance among sites\nspe.norm &lt;- decostand(spe, \"normalize\")\nspe.ch &lt;- vegdist(spe.norm, \"euc\")\n\n# Attach site names to object of class 'dist'\nattr(spe.ch, \"Labels\") &lt;- rownames(spe)\n\npar(mfrow = c(1, 1))\n\n# Compute single linkage agglomerative clustering\nspe.ch.single &lt;- hclust(spe.ch, method = \"single\")\n# Plot a dendrogram using the default options\nplot(spe.ch.single, labels = rownames(spe), main = \"Chord - Single linkage\")\n\n\n\n\n# Compute complete-linkage agglomerative clustering\nspe.ch.complete &lt;- hclust(spe.ch, method = \"complete\")\nplot(spe.ch.complete, labels = rownames(spe), main = \"Chord - Complete linkage\")\n\n\n\n\n# Compute UPGMA agglomerative clustering\nspe.ch.UPGMA &lt;- hclust(spe.ch, method = \"average\")\nplot(spe.ch.UPGMA, labels = rownames(spe), main = \"Chord - UPGMA\")\n\n\n\n\n# Compute centroid clustering\nspe.ch.centroid &lt;- hclust(spe.ch, method = \"centroid\")\nplot(spe.ch.centroid, labels = rownames(spe),  main = \"Chord - Centroid\")\n\n\n\n\n# Compute Ward's minimum variance clustering\nspe.ch.ward &lt;-hclust(spe.ch, method = \"ward.D2\")\nplot(spe.ch.ward, labels = rownames(spe),  main = \"Chord - Ward\")\n\n\n\n\n# Compute beta-flexible clustering using cluster::agnes()\n# beta = -0.1\nspe.ch.beta1 &lt;- agnes(spe.ch, method = \"flexible\", par.method = 0.55)\n# beta = -0.25\nspe.ch.beta2 &lt;- agnes(spe.ch, method = \"flexible\", par.method = 0.625)\n# beta = -0.5\nspe.ch.beta3 &lt;- agnes(spe.ch, method = \"flexible\", par.method = 0.75)\n# Change the class of agnes objects\nclass(spe.ch.beta1)\n## [1] \"agnes\" \"twins\"\nspe.ch.beta1 &lt;- as.hclust(spe.ch.beta1)\nclass(spe.ch.beta1)\n## [1] \"hclust\"\nspe.ch.beta2 &lt;- as.hclust(spe.ch.beta2)\nspe.ch.beta3 &lt;- as.hclust(spe.ch.beta3)\n\npar(mfrow = c(2, 2))\nplot(spe.ch.beta1, labels = rownames(spe), main = \"Chord - Beta-flexible (beta=-0.1)\")\nplot(spe.ch.beta2, labels = rownames(spe), main = \"Chord - Beta-flexible (beta=-0.25)\")\nplot(spe.ch.beta3,  labels = rownames(spe),  main = \"Chord - Beta-flexible (beta=-0.5)\")\n\n# Compute Ward's minimum variance clustering\nspe.ch.ward &lt;- hclust(spe.ch, method = \"ward.D2\")\nplot(spe.ch.ward, labels = rownames(spe), main = \"Chord - Ward\")"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#cophenetic-correlations",
    "href": "stat5-8/Statistik8_Demo.html#cophenetic-correlations",
    "title": "Stat8: Demo",
    "section": "Cophenetic correlations",
    "text": "Cophenetic correlations\n\n# Single linkage clustering\nspe.ch.single.coph &lt;- cophenetic(spe.ch.single)\ncor(spe.ch, spe.ch.single.coph)\n## [1] 0.599193\n\n# Complete linkage clustering\nspe.ch.comp.coph &lt;- cophenetic(spe.ch.complete)\ncor(spe.ch, spe.ch.comp.coph)\n## [1] 0.7655628\n\n# Average clustering\nspe.ch.UPGMA.coph &lt;- cophenetic(spe.ch.UPGMA)\ncor(spe.ch, spe.ch.UPGMA.coph)\n## [1] 0.8608326\n\n# Ward clustering\nspe.ch.ward.coph &lt;- cophenetic(spe.ch.ward)\ncor(spe.ch, spe.ch.ward.coph)\n## [1] 0.7998516\n\n# Shepard-like diagrams\npar(mfrow = c(2, 2))\nplot(spe.ch, spe.ch.single.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)),\n  main = c(\"Single linkage\", paste(\"Cophenetic correlation =\",\n                                   round(cor(spe.ch, spe.ch.single.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.single.coph), col = \"red\")\n\nplot(spe.ch, spe.ch.comp.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)),\n  main = c(\"Complete linkage\", paste(\"Cophenetic correlation =\",\n                                     round(cor(spe.ch, spe.ch.comp.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.comp.coph), col = \"red\")\n\nplot(spe.ch, spe.ch.UPGMA.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)),\n  main = c(\"UPGMA\", paste(\"Cophenetic correlation =\",\n                          round( cor(spe.ch, spe.ch.UPGMA.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.UPGMA.coph), col = \"red\")\n\nplot(spe.ch, spe.ch.ward.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, max(spe.ch.ward$height)),\n  main = c(\"Ward\", paste(\"Cophenetic correlation =\", \n                         round(cor(spe.ch, spe.ch.ward.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.ward.coph), col = \"red\")"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#optimale-anzahl-cluster",
    "href": "stat5-8/Statistik8_Demo.html#optimale-anzahl-cluster",
    "title": "Stat8: Demo",
    "section": "Optimale Anzahl Cluster",
    "text": "Optimale Anzahl Cluster\n\n## Select a dendrogram (Ward/chord) and apply three criteria\n## to choose the optimal number of clusters\n\n# Choose and rename the dendrogram (\"hclust\" object)\nhc &lt;- spe.ch.ward\n# hc &lt;- spe.ch.beta2\n# hc &lt;- spe.ch.complete\n\npar(mfrow = c(1, 2))\n\n# Average silhouette widths (Rousseeuw quality index)\nSi &lt;- numeric(nrow(spe))\nfor (k in 2:(nrow(spe) - 1))\n{\n  sil &lt;- silhouette(cutree(hc, k = k), spe.ch)\n  Si[k] &lt;- summary(sil)$avg.width\n}\n\nk.best &lt;- which.max(Si)\nplot(1:nrow(spe), Si, type = \"h\",\n  main = \"Silhouette-optimal number of clusters\",\n  xlab = \"k (number of clusters)\", ylab = \"Average silhouette width\")\naxis(1, k.best,paste(\"optimum\", k.best, sep = \"\\n\"), col = \"red\", \n     font = 2, col.axis = \"red\")\npoints(k.best,max(Si), pch = 16, col = \"red\",cex = 1.5)\n\n# Optimal number of clusters according to matrix correlation \n# statistic (Pearson)\n\n# Homemade function grpdist from Borcard et al. (2018)\ngrpdist &lt;- function(X)\n{\n  require(cluster)\n  veg &lt;- as.data.frame(as.factor(X))\n  distgr &lt;- daisy(veg, \"gower\")\n  distgr\n} \n\nkt &lt;- data.frame(k = 1:nrow(spe), r = 0)\nfor (i in 2:(nrow(spe) - 1)) \n{\n  gr &lt;- cutree(hc, i)\n  distgr &lt;- grpdist(gr)\n  mt &lt;- cor(spe.ch, distgr, method = \"pearson\")\n  kt[i, 2] &lt;- mt\n}\n\nk.best &lt;- which.max(kt$r)\nplot(kt$k,kt$r, type = \"h\",\n  main = \"Matrix correlation-optimal number of clusters\",\n  xlab = \"k (number of clusters)\", ylab = \"Pearson's correlation\")\naxis(1, k.best, paste(\"optimum\", k.best, sep = \"\\n\"),\n  col = \"red\", font = 2, col.axis = \"red\")\npoints(k.best, max(kt$r), pch = 16, col = \"red\", cex = 1.5)\n\n\n\n\n# Optimal number of clusters according as per indicator species\n# analysis (IndVal, Dufrene-Legendre; package: labdsv)\nIndVal &lt;- numeric(nrow(spe))\nng &lt;- numeric(nrow(spe))\nfor (k in 2:(nrow(spe) - 1))\n{\n  iva &lt;- indval(spe, cutree(hc, k = k), numitr = 1000)\n  gr &lt;- factor(iva$maxcls[iva$pval &lt;= 0.05])\n  ng[k] &lt;- length(levels(gr)) / k\n  iv &lt;- iva$indcls[iva$pval &lt;= 0.05]\n  IndVal[k] &lt;- sum(iv)\n}\n\nk.best &lt;- which.max(IndVal[ng == 1]) + 1\ncol3 &lt;- rep(1, nrow(spe))\ncol3[ng == 1] &lt;- 3\n\npar(mfrow = c(1, 2))\nplot(1:nrow(spe), IndVal, type = \"h\",\n  main = \"IndVal-optimal number of clusters\",\n  xlab = \"k (number of clusters)\", ylab = \"IndVal sum\", col = col3)\naxis(1,k.best,paste(\"optimum\", k.best, sep = \"\\n\"),\n  col = \"red\", font = 2, col.axis = \"red\")\n\npoints(which.max(IndVal),max(IndVal),pch = 16,col = \"red\",cex = 1.5)\ntext(28, 15.7, \"a\", cex = 1.8)\n\nplot(1:nrow(spe),ng,\n  type = \"h\",\n  xlab = \"k (number of clusters)\",\n  ylab = \"Ratio\",\n  main = \"Proportion of clusters with significant indicator species\",\n  col = col3)\naxis(1,k.best,paste(\"optimum\", k.best, sep = \"\\n\"),\n     col = \"red\", font = 2, col.axis = \"red\")\npoints(k.best,max(ng), pch = 16, col = \"red\", cex = 1.5)\ntext(28, 0.98, \"b\", cex = 1.8)"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#final-dendrogram-with-the-selected-clusters",
    "href": "stat5-8/Statistik8_Demo.html#final-dendrogram-with-the-selected-clusters",
    "title": "Stat8: Demo",
    "section": "Final dendrogram with the selected clusters",
    "text": "Final dendrogram with the selected clusters\n\n# Choose the number of clusters\nk &lt;- 4\n# Silhouette plot of the final partition\nspech.ward.g &lt;- cutree(spe.ch.ward, k = k)\nsil &lt;- silhouette(spech.ward.g, spe.ch)\nrownames(sil) &lt;- row.names(spe)\n\nplot(sil, main = \"Silhouette plot - Chord - Ward\", cex.names = 0.8, col = 2:(k + 1), nmax = 100)\n\n\n\n\n# Reorder clusters\nif(!require(gclus)){install.packages(\"gclus\")}\nlibrary(\"gclus\")\nspe.chwo &lt;- reorder.hclust(spe.ch.ward, spe.ch)\n\n# Plot reordered dendrogram with group labels\npar(mfrow = c(1, 1))\nplot(spe.chwo, hang = -1, xlab = \"4 groups\", ylab = \"Height\", sub = \"\",\n  main = \"Chord - Ward (reordered)\", labels = cutree(spe.chwo, k = k))\nrect.hclust(spe.chwo, k = k)\n\n\n\n\n# Plot the final dendrogram with group colors (RGBCMY...)\n# Fast method using the additional hcoplot() function:\n# Usage:\n# hcoplot(tree = hclust.object,\n#   diss = dissimilarity.matrix,\n#   lab = object labels (default NULL),\n#   k = nb.clusters,\n#   title = paste(\"Reordered dendrogram from\",deparse(tree$call),\n#   sep=\"\\n\"))\nsource(\"stat5-8/hcoplot.R\")\nhcoplot(spe.ch.ward, spe.ch, lab = rownames(spe), k = 4)\n\n\n\n\n# Plot the Ward clusters on a map of the Doubs River\n# (see Chapter 2)\nsource(\"stat5-8/drawmap.R\")\ndrawmap(xy = spa, clusters = spech.ward.g, main = \"Four Ward clusters along the Doubs River\")"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#miscellaneous-graphical-outputs",
    "href": "stat5-8/Statistik8_Demo.html#miscellaneous-graphical-outputs",
    "title": "Stat8: Demo",
    "section": "Miscellaneous graphical outputs",
    "text": "Miscellaneous graphical outputs\n\n# konvertieren von \"hclust\" Objekt in ein Dendogram Objekt\ndend &lt;- as.dendrogram(spe.ch.ward)\n\n# Heat map of the dissimilarity matrix ordered with the dendrogram\nheatmap(as.matrix(spe.ch), Rowv = dend, symm = TRUE, margin = c(3, 3))\n\n\n\n\n# Ordered community table\n# Species are ordered by their weighted averages on site scores.\n# Dots represent absences.\nlibrary(vegan)\nor &lt;- vegemite(spe, spe.chwo)\n##                                    \n##       32222222222  111111     1111 \n##       09876210543959876506473221341\n##  Icme 5432121......................\n##  Abbr 54332431.....1...............\n##  Blbj 54542432.1...1...............\n##  Anan 54432222.....111.............\n##  Gyce 5555443212...11..............\n##  Scer 522112221...21...............\n##  Cyca 53421321.....1111............\n##  Rham 55432333.....221.............\n##  Legi 35432322.1...1111............\n##  Alal 55555555352..322.............\n##  Chna 12111322.1...211.............\n##  Titi 53453444...1321111.21........\n##  Ruru 55554555121455221..1.........\n##  Albi 53111123.....2341............\n##  Baba 35342544.....23322.........1.\n##  Eslu 453423321...41111..12.1....1.\n##  Gogo 5544355421..242122111......1.\n##  Pefl 54211432....41321..12........\n##  Pato 2211.222.....3344............\n##  Sqce 3443242312152132232211..11.1.\n##  Lele 332213221...52235321.1.......\n##  Babl .1111112...32534554555534124.\n##  Teso .1...........11254........23.\n##  Phph .1....11...13334344454544455.\n##  Cogo ..............1123......2123.\n##  Satr .1..........2.123413455553553\n##  Thth .1............11.2......2134.\n## 29 sites, 27 species\n\n\n\n\n\n\n\nBorcard, Daniel, François Gillet, Pierre Legendre, u. a. 2011. Numerical ecology with R. Bd. 2. Springer."
  },
  {
    "objectID": "stat5-8/Statistik8_Uebung.html#uebung-8.1-clusteranalyse-sozioökonomisch",
    "href": "stat5-8/Statistik8_Uebung.html#uebung-8.1-clusteranalyse-sozioökonomisch",
    "title": "Stat8: Übung",
    "section": "Uebung 8.1: Clusteranalyse (sozioökonomisch)",
    "text": "Uebung 8.1: Clusteranalyse (sozioökonomisch)\n\nDatensatz crime2.csv\n\nRaten von 7 Kriminalitätsformen pro 100000 Einwohner und Jahr für die Bundesstaaten der USA\n\n(a) Führt eine k-means- und eine agglomerative Clusteranalyse eurer Wahl durch.\n(b) Überlegt in beiden Fällen, wie viele Cluster sinnvoll sind (k-means z. B.visuelle Betrachtung einer PCA, agglomerative Clusteranalyse z. B. SilhouettePlot).\n(c) Abschliessend entscheidet euch für eine Clusterung und vergleicht die erhaltenen Cluster bezüglich der Kriminalitätsformen mittels ANOVA und interpretiert die Cluster entsprechend.\n\nHinweis: Wegen der sehr ungleichen Varianzen muss auf jeden Fall eine Standardisierung stattfinden, damit Distanzen zwischen den verschiedenen Kriminalitätsraten sinnvoll berechnet werden können"
  },
  {
    "objectID": "stat5-8/Statistik8_Loesung.html#musterlösung-aufgabe-8.1-clusteranalysen",
    "href": "stat5-8/Statistik8_Loesung.html#musterlösung-aufgabe-8.1-clusteranalysen",
    "title": "Stat8: Lösung",
    "section": "Musterlösung Aufgabe 8.1: Clusteranalysen",
    "text": "Musterlösung Aufgabe 8.1: Clusteranalysen\n\nÜbungsaufgabe\n(hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\n\nLadet den Datensatz crime2.csv. Dieser enthält Raten von 7 Kriminatlitätsformen pro 100000 Einwohner und Jahr für die Bundesstaaten der USA.\nFührt eine k-means- und eine agglomerative Clusteranalyse eurer Wahl durch. Bitte beachet, dass wegen der sehr ungleichen Varianzen in jedem Fall eine Standardisierung stattfinden muss, damit die Distanzen zwischen den verschiedenen Kriminalitätsraten sinnvoll berechnet werden können.\nÜberlegt in beiden Fällen, wie viele Cluster sinnvoll sind (k-means: z. B. visuelle Betrachtung einer PCA, agglomertive Clusteranalyse: z. B. Silhoutte-Plot).\nEntscheidet euch dann für eine der beiden Clusterungen und vergleicht dann die erhaltenen Cluster bezüglich der Kriminalitätsformen und interpretiert die Cluster entsprechend.\nBitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu diesem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in das ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nFormuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).\n\n\n\nLösung\n\ncrime &lt;- read.csv(\"datasets/statistik/crime2.csv\", sep = \";\")\n\n\ncrime\n##     X Murder Rape Robbery Assault Burglary Theft Vehicle\n## 1  ME    2.0 14.8      28     102      803  2347     164\n## 2  NH    2.2 21.5      24      92      755  2208     228\n## 3  VT    2.0 21.8      22     103      949  2697     181\n## 4  MA    3.6 29.7     193     331     1071  2189     906\n## 5  RI    3.5 21.4     119     192     1294  2568     705\n## 6  CT    4.6 23.8     192     205     1198  2758     447\n## 7  NY   10.7 30.5     514     431     1221  2924     637\n## 8  NJ    5.2 33.2     269     265     1071  2822     776\n## 9  PA    5.5 25.1     152     176      735  1654     354\n## 10 OH    5.5 38.6     142     235      988  2574     376\n## 11 IN    6.0 25.9      90     186      887  2333     328\n## 12 IL    8.9 32.4     325     434     1180  2938     628\n## 13 MI   11.3 67.4     301     424     1509  3378     800\n## 14 WI    3.1 20.1      73     162      783  2802     254\n## 15 MN    2.5 31.8     102     148     1004  2785     288\n## 16 IA    1.8 12.5      42     179      956  2801     158\n## 17 MO    9.2 29.2     170     370     1136  2500     439\n## 18 ND    1.0 11.6       7      32      385  2049     120\n## 19 SD    4.0 17.7      16      87      554  1939      99\n## 20 NE    3.1 24.6      51     184      748  2677     168\n## 21 KS    4.4 32.9      80     252     1188  3008     258\n## 22 DE    4.9 56.9     124     241     1042  3090     272\n## 23 MD    9.0 43.6     304     476     1296  2978     545\n## 24 VA    7.1 26.5     106     167      813  2522     219\n## 25 WV    5.9 18.9      41      99      625  1358     169\n## 26 NC    8.1 26.4      88     354     1225  2423     208\n## 27 SC    8.6 41.3      99     525     1340  2846     277\n## 28 GA   11.2 43.9     214     319     1453  2984     430\n## 29 FL   11.7 52.7     367     605     2221  4373     598\n## 30 KY    6.7 23.1      83     222      824  1740     193\n## 31 TN   10.4 47.0     208     274     1325  2126     544\n## 32 AL   10.1 28.4     112     408     1159  2304     267\n## 33 MS   11.2 25.8      65     172     1076  1845     150\n## 34 AR    8.1 28.9      80     278     1030  2305     195\n## 35 LA   12.8 40.1     224     482     1461  3417     442\n## 36 OK    8.1 36.4     107     285     1787  3142     649\n## 37 TX   13.5 51.6     240     354     2049  3987     714\n## 38 MT    2.9 17.3      20     118      783  3314     215\n## 39 ID    3.2 20.0      21     178     1003  2800     181\n## 40 WY    5.3 21.9      22     243      817  3078     169\n## 41 CO    7.0 42.3     145     329     1792  4231     486\n## 42 NM   11.5 46.9     130     538     1845  3712     343\n## 43 AZ    9.3 43.0     169     437     1908  4337     419\n## 44 UT    3.2 25.3      59     180      915  4074     223\n## 45 NV   12.6 64.9     287     354     1604  3489     478\n## 46 WA    5.0 53.4     135     244     1861  4267     315\n## 47 OR    6.6 51.1     206     286     1967  4163     402\n## 48 CA   11.3 44.9     343     521     1696  3384     762\n## 49 AK    8.6 72.7      88     401     1162  3910     604\n## 50 HI    4.8 31.0     106     103     1339  3759     328\n\nIm mitgelieferten R-Skript habe ich die folgenden Analysen zunächst mit untransformierten, dann mit standardisierten Kriminalitätsraten berechnet. Ihr könnt die Ergebnisse vergleichen und seht, dass sie sehr unterschiedlich ausfallen.\n\ncrimez &lt;- crime\ncrimez[, c(2:8)] &lt;- lapply(crime[, c(2:8)], scale)\ncrimez\n##     X      Murder        Rape       Robbery     Assault     Burglary\n## 1  ME -1.38246619 -1.31559240 -1.0464120463 -1.24765744 -0.938630226\n## 2  NH -1.32457397 -0.85298774 -1.0830960795 -1.31950227 -1.053079880\n## 3  VT -1.38246619 -0.83227410 -1.1014380961 -1.24047295 -0.590512528\n## 4  MA -0.91932843 -0.28681488  0.4668043222  0.39758933 -0.299619657\n## 5  RI -0.94827454 -0.85989229 -0.2118502916 -0.60105391  0.232094361\n## 6  CT -0.62986734 -0.69418316  0.4576333139 -0.50765562  0.003195053\n## 7  NY  1.13584533 -0.23157851  3.4106979845  1.11603770  0.058035512\n## 8  NJ -0.45619068 -0.04515574  1.1638009525 -0.07658660 -0.299619657\n## 9  PA -0.36935236 -0.60442405  0.0907929821 -0.71600564 -1.100767236\n## 10 OH -0.36935236  0.32768980 -0.0009171008 -0.29212111 -0.497522184\n## 11 IN -0.22462181 -0.54918767 -0.4778095321 -0.64416081 -0.738343331\n## 12 IL  0.61481536 -0.10039211  1.6773774169  1.13759115 -0.039723567\n## 13 MI  1.30952199  2.31619936  1.4572732179  1.06574631  0.744733437\n## 14 WI -1.06405898 -0.94965140 -0.6337166731 -0.81658842 -0.986317582\n## 15 MN -1.23773564 -0.14181940 -0.3677574326 -0.91717119 -0.459372299\n## 16 IA -1.44035840 -1.47439698 -0.9180179302 -0.69445219 -0.573821954\n## 17 MO  0.70165369 -0.32133762  0.2558711314  0.67778419 -0.144635750\n## 18 ND -1.67192728 -1.53653790 -1.2390032205 -1.75057130 -1.935295964\n## 19 SD -0.80354400 -1.11536053 -1.1564641459 -1.35542469 -1.532337807\n## 20 NE -1.06405898 -0.63894678 -0.8354788556 -0.65852977 -1.069770455\n## 21 KS -0.68775956 -0.06586938 -0.5695196150 -0.16998488 -0.020648625\n## 22 DE -0.54302901  1.59122191 -0.1659952501 -0.24901420 -0.368766323\n## 23 MD  0.64376147  0.67291716  1.4847862428  1.43933946  0.236863097\n## 24 VA  0.09378539 -0.50776039 -0.3310733994 -0.78066600 -0.914786548\n## 25 WV -0.25356792 -1.03250597 -0.9271889385 -1.26921089 -1.363047694\n## 26 NC  0.38324649 -0.51466494 -0.4961515487  0.56283245  0.067572983\n## 27 SC  0.52797704  0.51411257 -0.3952704575  1.79137916  0.341775280\n## 28 GA  1.28057588  0.69363080  0.6593954963  0.31137552  0.611208841\n## 29 FL  1.42530643  1.30123094  2.0625597653  2.36613786  2.442403307\n## 30 KY -0.02199904 -0.74251499 -0.5420065902 -0.38551939 -0.888558502\n## 31 TN  1.04900701  0.90767176  0.6043694466 -0.01192624  0.306009763\n## 32 AL  0.96216868 -0.37657400 -0.2760473496  0.95079457 -0.089795291\n## 33 MS  1.28057588 -0.55609222 -0.7070847395 -0.74474358 -0.287697818\n## 34 AR  0.38324649 -0.34205126 -0.5695196150  0.01681169 -0.397378737\n## 35 LA  1.74371363  0.43125801  0.7511055793  1.48244636  0.630283783\n## 36 OK  0.38324649  0.17578977 -0.3219023911  0.06710308  1.407587684\n## 37 TX  1.94633640  1.22528092  0.8978417120  0.56283245  2.032292046\n## 38 MT -1.12195120 -1.14297872 -1.1197801127 -1.13270570 -0.986317582\n## 39 ID -1.03511287 -0.95655595 -1.1106091044 -0.70163668 -0.461756667\n## 40 WY -0.42724457 -0.82536956 -1.1014380961 -0.23464524 -0.905249077\n## 41 CO  0.06483929  0.58315804  0.0265959241  0.38322036  1.419509523\n## 42 NM  1.36741421  0.90076721 -0.1109692004  1.88477745  1.545881016\n## 43 AZ  0.73059980  0.63148987  0.2467001231  1.15914460  1.696096187\n## 44 UT -1.03511287 -0.59061496 -0.7621107892 -0.68726771 -0.671581033\n## 45 NV  1.68582141  2.14358568  1.3288791018  0.56283245  0.971248378\n## 46 WA -0.51408290  1.34956277 -0.0651141589 -0.22746075  1.584030901\n## 47 OR -0.05094515  1.19075819  0.5860274300  0.07428756  1.836773887\n## 48 CA  1.30952199  0.76267627  1.8424555662  1.76264123  1.190610215\n## 49 AK  0.52797704  2.68214035 -0.4961515487  0.90050319 -0.082642188\n## 50 HI -0.57197512 -0.19705577 -0.3310733994 -1.24047295  0.339390912\n##           Theft     Vehicle\n## 1  -0.759700660 -1.04035426\n## 2  -0.944578268 -0.73523707\n## 3  -0.294181505 -0.95930750\n## 4  -0.969849308  2.49709812\n## 5  -0.465758565  1.53883946\n## 6  -0.213048167  0.30883580\n## 7   0.007740919  1.21465245\n## 8  -0.127924664  1.87732884\n## 9  -1.681428588 -0.13453761\n## 10 -0.457778237 -0.02965358\n## 11 -0.778321427 -0.25849147\n## 12  0.026361685  1.17174535\n## 13  0.611585766  1.99174778\n## 14 -0.154525758 -0.61128321\n## 15 -0.177136689 -0.44918971\n## 16 -0.155855813 -1.06895899\n## 17 -0.556202287  0.27069615\n## 18 -1.156056970 -1.25012232\n## 19 -1.302362990 -1.35023890\n## 20 -0.320782600 -1.02128443\n## 21  0.119465516 -0.59221339\n## 22  0.228530004 -0.52546900\n## 23  0.079563874  0.77604649\n## 24 -0.526941083 -0.77814417\n## 25 -2.075124788 -1.01651698\n## 26 -0.658616501 -0.83058619\n## 27 -0.096003350 -0.50163172\n## 28  0.087544202  0.22778905\n## 29  1.934990222  1.02872166\n## 30 -1.567043881 -0.90209803\n## 31 -1.053642756  0.77127904\n## 32 -0.816893014 -0.54930628\n## 33 -1.427388135 -1.10709864\n## 34 -0.815562959 -0.89256312\n## 35  0.663457900  0.28499852\n## 36  0.297692850  1.27186192\n## 37  1.421589096  1.58174656\n## 38  0.526462263 -0.79721400\n## 39 -0.157185868 -0.95930750\n## 40  0.212569347 -1.01651698\n## 41  1.746122450  0.49476659\n## 42  1.055824046 -0.18697963\n## 43  1.887108252  0.17534703\n## 44  1.537303858 -0.75907435\n## 45  0.759221841  0.45662694\n## 46  1.794004420 -0.32046839\n## 47  1.655678729  0.09430028\n## 48  0.619566094  1.81058445\n## 49  1.319174882  1.05732640\n## 50  1.118336618 -0.25849147\n\n„scale“ führt eine Standardisierung (z-Transformation) durch, so dass alle Variablen anschiessen einen Mittelwert von 0 und eine SD von 1 haben, ausgenommen natürlich die 1. Spalte mit den Kürzeln der Bundesstaaten. Anschliessend wird das SSI-Kriterium getestet und zwar für Partitionierungen von 2 bis 6 Gruppen (wie viele Gruppen man maximal haben will, muss man pragmatisch nach der jeweiligen Fragestelltung entscheiden).\n\nlibrary(vegan)\ncrimez.KM.cascade &lt;- cascadeKM(crimez[, c(2:8)], inf.gr = 2, sup.gr = 6, iter = 100,\n    criterion = \"ssi\")\nsummary(crimez.KM.cascade)\n##           Length Class  Mode     \n## partition 250    -none- numeric  \n## results    10    -none- numeric  \n## criterion   1    -none- character\n## size       30    -none- numeric\n\ncrimez.KM.cascade$results\n##       2 groups   3 groups   4 groups   5 groups  6 groups\n## SSE 174.959159 144.699605 124.437221 108.119280 95.316398\n## ssi   1.226057   1.304674   1.555594   1.539051  1.351146\ncrimez.KM.cascade$partition\n##    2 groups 3 groups 4 groups 5 groups 6 groups\n## 1         1        3        1        3        6\n## 2         1        3        1        3        6\n## 3         1        3        1        3        6\n## 4         2        1        4        1        4\n## 5         1        1        2        1        4\n## 6         1        1        2        1        5\n## 7         2        1        4        5        3\n## 8         2        1        4        1        4\n## 9         1        3        2        2        5\n## 10        1        1        2        2        5\n## 11        1        3        2        2        5\n## 12        2        1        4        5        3\n## 13        2        2        3        5        1\n## 14        1        3        1        3        6\n## 15        1        3        1        3        6\n## 16        1        3        1        3        6\n## 17        2        1        2        2        3\n## 18        1        3        1        3        6\n## 19        1        3        1        3        6\n## 20        1        3        1        3        6\n## 21        1        3        2        2        5\n## 22        1        1        2        2        5\n## 23        2        1        4        5        3\n## 24        1        3        2        2        5\n## 25        1        3        1        3        6\n## 26        1        1        2        2        5\n## 27        2        1        2        2        3\n## 28        2        1        3        5        3\n## 29        2        2        3        5        1\n## 30        1        3        2        2        5\n## 31        2        1        4        2        3\n## 32        1        1        2        2        5\n## 33        1        3        2        2        5\n## 34        1        3        2        2        5\n## 35        2        2        3        5        3\n## 36        2        1        3        4        2\n## 37        2        2        3        5        1\n## 38        1        3        1        3        6\n## 39        1        3        1        3        6\n## 40        1        3        1        3        6\n## 41        2        2        3        4        2\n## 42        2        2        3        4        2\n## 43        2        2        3        4        2\n## 44        1        3        1        3        6\n## 45        2        2        3        5        1\n## 46        2        2        3        4        2\n## 47        2        2        3        4        2\n## 48        2        2        3        5        1\n## 49        2        2        3        4        2\n## 50        1        3        2        3        5\n\n# k-means visualisation\nlibrary(cclust)\nplot(crimez.KM.cascade, sortg = TRUE)\n\n\n\n\nNach SSI ist die 4-Gruppenlösung die beste, mit dieser wird also weitergerechnet.\n\n# 4 Kategorien sind nach SSI offensichtlich besonders gut\nmodelz &lt;- kmeans(crimez[, c(2:8)], 4)\nmodelz\n## K-means clustering with 4 clusters of sizes 6, 14, 16, 14\n## \n## Cluster means:\n##         Murder       Rape    Robbery    Assault     Burglary      Theft\n## 1  0.344651676  0.1527746  1.4679727  0.6670075 -0.006342418 -0.3396250\n## 2 -1.088869933 -0.9575423 -0.9573223 -1.0018455 -0.966220768 -0.3729397\n## 3 -0.002098593 -0.2436615 -0.2668763 -0.1457373 -0.279054485 -0.5371659\n## 4  0.943560464  1.1705377  0.6331926  0.8825420  1.287858358  1.1323972\n##      Vehicle\n## 1  1.3846917\n## 2 -0.9310433\n## 3 -0.3276196\n## 4  0.7120264\n## \n## Clustering vector:\n##  [1] 2 2 2 1 3 3 1 1 3 3 3 1 4 2 2 2 3 2 2 2 3 3 1 3 2 3 3 4 4 3 1 3 3 3 4 4 4 2\n## [39] 2 2 4 4 4 2 4 4 4 4 4 3\n## \n## Within cluster sum of squares by cluster:\n## [1] 16.59050 19.37307 39.51883 48.95481\n##  (between_SS / total_SS =  63.7 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n\n# File für ANOVA (Originaldaten der Vorfälle, nicht die ztransformierten)\ncrime.KM4 &lt;- data.frame(crime, modelz[1])\ncrime.KM4$cluster &lt;- as.factor(crime.KM4$cluster)\ncrime.KM4\n##     X Murder Rape Robbery Assault Burglary Theft Vehicle cluster\n## 1  ME    2.0 14.8      28     102      803  2347     164       2\n## 2  NH    2.2 21.5      24      92      755  2208     228       2\n## 3  VT    2.0 21.8      22     103      949  2697     181       2\n## 4  MA    3.6 29.7     193     331     1071  2189     906       1\n## 5  RI    3.5 21.4     119     192     1294  2568     705       3\n## 6  CT    4.6 23.8     192     205     1198  2758     447       3\n## 7  NY   10.7 30.5     514     431     1221  2924     637       1\n## 8  NJ    5.2 33.2     269     265     1071  2822     776       1\n## 9  PA    5.5 25.1     152     176      735  1654     354       3\n## 10 OH    5.5 38.6     142     235      988  2574     376       3\n## 11 IN    6.0 25.9      90     186      887  2333     328       3\n## 12 IL    8.9 32.4     325     434     1180  2938     628       1\n## 13 MI   11.3 67.4     301     424     1509  3378     800       4\n## 14 WI    3.1 20.1      73     162      783  2802     254       2\n## 15 MN    2.5 31.8     102     148     1004  2785     288       2\n## 16 IA    1.8 12.5      42     179      956  2801     158       2\n## 17 MO    9.2 29.2     170     370     1136  2500     439       3\n## 18 ND    1.0 11.6       7      32      385  2049     120       2\n## 19 SD    4.0 17.7      16      87      554  1939      99       2\n## 20 NE    3.1 24.6      51     184      748  2677     168       2\n## 21 KS    4.4 32.9      80     252     1188  3008     258       3\n## 22 DE    4.9 56.9     124     241     1042  3090     272       3\n## 23 MD    9.0 43.6     304     476     1296  2978     545       1\n## 24 VA    7.1 26.5     106     167      813  2522     219       3\n## 25 WV    5.9 18.9      41      99      625  1358     169       2\n## 26 NC    8.1 26.4      88     354     1225  2423     208       3\n## 27 SC    8.6 41.3      99     525     1340  2846     277       3\n## 28 GA   11.2 43.9     214     319     1453  2984     430       4\n## 29 FL   11.7 52.7     367     605     2221  4373     598       4\n## 30 KY    6.7 23.1      83     222      824  1740     193       3\n## 31 TN   10.4 47.0     208     274     1325  2126     544       1\n## 32 AL   10.1 28.4     112     408     1159  2304     267       3\n## 33 MS   11.2 25.8      65     172     1076  1845     150       3\n## 34 AR    8.1 28.9      80     278     1030  2305     195       3\n## 35 LA   12.8 40.1     224     482     1461  3417     442       4\n## 36 OK    8.1 36.4     107     285     1787  3142     649       4\n## 37 TX   13.5 51.6     240     354     2049  3987     714       4\n## 38 MT    2.9 17.3      20     118      783  3314     215       2\n## 39 ID    3.2 20.0      21     178     1003  2800     181       2\n## 40 WY    5.3 21.9      22     243      817  3078     169       2\n## 41 CO    7.0 42.3     145     329     1792  4231     486       4\n## 42 NM   11.5 46.9     130     538     1845  3712     343       4\n## 43 AZ    9.3 43.0     169     437     1908  4337     419       4\n## 44 UT    3.2 25.3      59     180      915  4074     223       2\n## 45 NV   12.6 64.9     287     354     1604  3489     478       4\n## 46 WA    5.0 53.4     135     244     1861  4267     315       4\n## 47 OR    6.6 51.1     206     286     1967  4163     402       4\n## 48 CA   11.3 44.9     343     521     1696  3384     762       4\n## 49 AK    8.6 72.7      88     401     1162  3910     604       4\n## 50 HI    4.8 31.0     106     103     1339  3759     328       3\nstr(crime.KM4)\n## 'data.frame':    50 obs. of  9 variables:\n##  $ X       : chr  \"ME\" \"NH\" \"VT\" \"MA\" ...\n##  $ Murder  : num  2 2.2 2 3.6 3.5 4.6 10.7 5.2 5.5 5.5 ...\n##  $ Rape    : num  14.8 21.5 21.8 29.7 21.4 23.8 30.5 33.2 25.1 38.6 ...\n##  $ Robbery : int  28 24 22 193 119 192 514 269 152 142 ...\n##  $ Assault : int  102 92 103 331 192 205 431 265 176 235 ...\n##  $ Burglary: int  803 755 949 1071 1294 1198 1221 1071 735 988 ...\n##  $ Theft   : int  2347 2208 2697 2189 2568 2758 2924 2822 1654 2574 ...\n##  $ Vehicle : int  164 228 181 906 705 447 637 776 354 376 ...\n##  $ cluster : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 2 2 2 1 3 3 1 1 3 3 ...\n\nVon den agglomerativen Clusterverfahren habe ich mich für Ward’s minimum variance clustering entschieden, da dieses allgemein als besonders geeignet gilt.\nVor der Berechnung von crime.norm und crime.ch muss man die Spalte mit den Bundesstaatenkürzeln entfern.\n\n# Agglomerative Clusteranalyse\ncrime2 &lt;- crime[, -1]\ncrime.norm &lt;- decostand(crime2, \"normalize\")\ncrime.ch &lt;- vegdist(crime.norm, \"euc\")\n# Attach site names to object of class 'dist'\nattr(crime.ch, \"Labels\") &lt;- crime[, 1]\n\n# Ward's minimum variance clustering\ncrime.ch.ward &lt;- hclust(crime.ch, method = \"ward.D2\")\npar(mfrow = c(1, 1))\nplot(crime.ch.ward, labels = crime[, 1], main = \"Chord - Ward\")\n\n\n\n\n# Choose and rename the dendrogram ('hclust' object)\nhc &lt;- crime.ch.ward\n# hc &lt;- spe.ch.beta2 hc &lt;- spe.ch.complete\ndev.new(title = \"Optimal number of clusters\", width = 12, height = 8, noRStudioGD = TRUE)\ndev.off()\n## png \n##   2\npar(mfrow = c(1, 2))\n\n\n# Average silhouette widths (Rousseeuw quality index)\nlibrary(cluster)\nSi &lt;- numeric(nrow(crime))\nfor (k in 2:(nrow(crime) - 1)) {\n    sil &lt;- silhouette(cutree(hc, k = k), crime.ch)\n    Si[k] &lt;- summary(sil)$avg.width\n}\nk.best &lt;- which.max(Si)\nplot(1:nrow(crime), Si, type = \"h\", main = \"Silhouette-optimal number of clusters\",\n    xlab = \"k (number of clusters)\", ylab = \"Average silhouette width\")\n\naxis(1, k.best, paste(\"optimum\", k.best, sep = \"\\n\"), col = \"red\", font = 2, col.axis = \"red\")\npoints(k.best, max(Si), pch = 16, col = \"red\", cex = 1.5)\n\n\n\n\nDemnach wären beim Ward’s-Clustering nur zwei Gruppen die optimale Lösung.\nFür die Vergleiche der Bundesstaatengruppen habe ich mich im Folgenden für die k-means Clusterung mit 4 Gruppen entschieden.\nDamit die Boxplots und die ANOVA direkt interpretierbar sind, werden für diese, anders als für die Clusterung, die untransformierten Incidenz-Werte verwendet (also crime statt crimez). Die Spalte mit der Clusterzugehörigkeit im Fall von k-means mit 4 Clustern hängt man als Spalte an (Achtung: muss als Faktor definiert werden!).\nAnschliessend kann man die 7 ANOVAs rechnen, die Posthoc-Vergleiche durchführen und die zugehörigen Boxplots mit Buchstaben für die homogenen Gruppen erzeugen. Sinnvollerweise gruppiert man die Abbildungen gleich, z. B. je 2 x 2. Das Skript ist hier simple für jede Verbrechensart wiederholt. Erfahrenere R-Nutzer können das Ganze hier natürlich durch eine Schleife abkürzen.\n\nlibrary(multcomp)\nif (!require(multcomp)) {\n    install.packages(\"multcomp\")\n}\nlibrary(multcomp)\npar(mfrow = c(3, 3))\n\nANOVA.Murder &lt;- aov(Murder ~ cluster, data = crime.KM4)\nsummary(ANOVA.Murder)\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cluster      3  355.4  118.46   23.75 1.96e-09 ***\n## Residuals   46  229.4    4.99                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters &lt;- cld(glht(ANOVA.Murder, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Murder ~ cluster, xlab = \"Cluster\", ylab = \"Murder\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Rape &lt;- aov(Rape ~ cluster, data = crime.KM4)\nsummary(ANOVA.Rape)\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cluster      3   6945  2315.0   31.95 2.58e-11 ***\n## Residuals   46   3333    72.5                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters &lt;- cld(glht(ANOVA.Rape, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Rape ~ cluster, xlab = \"Cluster\", ylab = \"Rape\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Robbery &lt;- aov(Robbery ~ cluster, data = crime.KM4)\nsummary(ANOVA.Robbery)\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cluster      3 386563  128854   30.24 5.96e-11 ***\n## Residuals   46 196025    4261                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters &lt;- cld(glht(ANOVA.Robbery, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Robbery ~ cluster, xlab = \"Cluster\", ylab = \"Robbery\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Assault &lt;- aov(Assault ~ cluster, data = crime.KM4)\nsummary(ANOVA.Assault)\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cluster      3 541786  180595   20.39 1.51e-08 ***\n## Residuals   46 407517    8859                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters &lt;- cld(glht(ANOVA.Assault, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Assault ~ cluster, xlab = \"Cluster\", ylab = \"Assault\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Burglary &lt;- aov(Burglary ~ cluster, data = crime.KM4)\nsummary(ANOVA.Burglary)\n##             Df  Sum Sq Mean Sq F value  Pr(&gt;F)    \n## cluster      3 6602474 2200825   50.21 1.5e-14 ***\n## Residuals   46 2016382   43834                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters &lt;- cld(glht(ANOVA.Burglary, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Burglary ~ cluster, data = crime.KM4, xlab = \"Cluster\", ylab = \"Burglary\")\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Theft &lt;- aov(Theft ~ cluster, data = crime.KM4)\nsummary(ANOVA.Theft)\n##             Df   Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cluster      3 14249791 4749930   16.25 2.44e-07 ***\n## Residuals   46 13448760  292364                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters &lt;- cld(glht(ANOVA.Theft, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Theft ~ cluster, xlab = \"Cluster\", ylab = \"Theft\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Vehicle &lt;- aov(Vehicle ~ cluster, data = crime.KM4)\nsummary(ANOVA.Vehicle)\n##             Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cluster      3 1427939  475980   30.08 6.46e-11 ***\n## Residuals   46  727932   15825                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nletters &lt;- cld(glht(ANOVA.Vehicle, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Vehicle ~ cluster, data = crime.KM4, xlab = \"Cluster\", ylab = \"Vehicle\")\nmtext(letters$mcletters$Letters, at = 1:6)\n\n\n\n\nDie Boxplots erlauben jetzt auch eine Beurteilung der Modelldiagnostik: sind die Residuen hinreichen normalverteilt (symmetrisch) und sind die Varianzen zwischen den Kategorien einigermassen ähnlich. Mit der Symmetrie/Normalverteilung sieht es OK aus. Die Varianzhomogenität ist nicht optimal – meist deutlich grössere Varianz bei höheren Mittelwerten. Eine log-Transformation hätte das verbessert und könnte hier gut begründet werden. Da die p-Werte sehr niedrig waren und die Varianzheterogenität noch nicht extrem war, habe ich aber von einer Transformation abgesehen, da jede Transformation die Interpretation der Ergebnisse erschwert. Jetzt muss man nur noch herausfinden, welche Bundesstaaten überhaupt zu welchem der vier Cluster gehören, sonst ist das ganze schöne Ergebnis nutzlos. Z. B. kann man in R auf den Dataframe clicken und ihn nach cluster sortieren."
  },
  {
    "objectID": "StatKons.html",
    "href": "StatKons.html",
    "title": "Statistik Konsolidierung",
    "section": "",
    "text": "In den vier Blöcken “Konsolidierung Statistik” repetieren die Studierenden die wichtigen Verfahren der Inferenz-Statistik. Beginnend mit den beiden Fällen, dem \\(Chi ^{2}/\\)-Test für die Assoziation zwischen zwei kategorialen Variablen und dem t-Test auf Unterschiede in Mittelwerten zwischen zwei Gruppen. Die Studierene wiederholen auch die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Weiter geht es mit der Repetition von komplexere Versionen linearer Regressionen und generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Abschliessend bekommen die Studierenden eine Einführung in die Welt der Ordinationen z.B. PCA.\n\nStatstik Konsolidierung 1\nIn diesem Block beschäftigen wir uns mit folgenden Inhalten:\nWarum Statistik? Warum mit R? Genereller Ablauf einer statistischen Analyse \\(Chi ^{2}/\\)-Test- bzw. Fishers Test (für kategoriale Daten) t-Test (für metrische Daten)\n\n\nStatistik Konsolidierung 2\nIn Statistik Konsolidierung 2 bekommen die Studierenden eine Einführung in das Thema der Ordinationen, eine Technik der deskriptiven Statistik. Diese Methoden visualisiert die Strukturen in multivariaten Datensätzen via Dimensionsreduktion. Das Prinzip und die praktische Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse (PCA) erklärt. Danach folgen kurze Einführungen in weitere Ordinationstechniken für besondere Fälle, welche bestimmte Limitierungen der PCA überwinden, namentlich NMDS.\n\n\nStatistik Konsolidierung 3\nIn Statistik Konsolidierung 3 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R. Im Fokus steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind.\n\n\nStatistik Konsolidierung 4\nIn Statistik Konsolidierung 4 kennen die Studierende alles Rund um das Thema der lineare Regressionen (inkl. nicht-lineare Regressionen). Die Studierenden bekommen eine Einführung in die generalized linear models (GLMs), eine Methode die einige wesentliche Limitierungen von linearen Modellen überwindenwerden können. Spezifisch werden wir uns die Poisson-Regressionen für Zähldaten und logistische Regression für ja/nein-Daten anschauen.\n\n\n\n\n\n\n\n\n\n\n\nTitel\n\n\nDatum\n\n\nLesson\n\n\nThema\n\n\n\n\n\n\nStatKons1: Demo\n\n\n2022-11-14\n\n\nStatKons1\n\n\nStatistik Grundlagen\n\n\n\n\nStatKons1: Open Datasets\n\n\n2022-11-14\n\n\nStatKons1\n\n\nStatistik Grundlagen\n\n\n\n\nStatKons2: Demo\n\n\n2022-11-15\n\n\nStatKons2\n\n\nPCA\n\n\n\n\nStatKons3: Demo\n\n\n2022-11-21\n\n\nStatKons3\n\n\nLM\n\n\n\n\nStatKons4: Demo\n\n\n2022-11-22\n\n\nStatKons4\n\n\nGLM\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "statKons/StatKons1_Demo_assoziationen.html#grundlagen",
    "href": "statKons/StatKons1_Demo_assoziationen.html#grundlagen",
    "title": "StatKons1: Demo",
    "section": "Grundlagen",
    "text": "Grundlagen\n\n## Error in library(ggplot2): there is no package called 'ggplot2'\n## Error in theme_classic(): could not find function \"theme_classic\"\n\n\n#lade Daten\n# mehr Info darüber: https://cran.r-project.org/web/packages/explore/vignettes/explore_mtcars.html\ncars &lt;- mtcars\n\n#neue kategoriale variable\ncars &lt;-  \n  as_tibble(cars) |&gt; # da \"nur\" data frame kann glimplse nichts damit anfangen \n  mutate(vs_cat = if_else(vs == 0, \"normal\", \"v-type\")) |&gt; \n  mutate(am_cat = if_else(am == 0, \"automatic\", \"manual\"))\n\n# betrachte die Daten\nsummary(cars)\n##       mpg             cyl             disp             hp       \n##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n##       drat             wt             qsec             vs        \n##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n##        am              gear            carb          vs_cat         \n##  Min.   :0.0000   Min.   :3.000   Min.   :1.000   Length:32         \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000   Class :character  \n##  Median :0.0000   Median :4.000   Median :2.000   Mode  :character  \n##  Mean   :0.4062   Mean   :3.688   Mean   :2.812                     \n##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000                     \n##  Max.   :1.0000   Max.   :5.000   Max.   :8.000                     \n##     am_cat         \n##  Length:32         \n##  Class :character  \n##  Mode  :character  \n##                    \n##                    \n## \nglimpse(cars)\n## Rows: 32\n## Columns: 13\n## $ mpg    &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.…\n## $ cyl    &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, …\n## $ disp   &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, …\n## $ hp     &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 1…\n## $ drat   &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.9…\n## $ wt     &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, …\n## $ qsec   &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, …\n## $ vs     &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, …\n## $ am     &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, …\n## $ gear   &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, …\n## $ carb   &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, …\n## $ vs_cat &lt;chr&gt; \"normal\", \"normal\", \"v-type\", \"v-type\", \"normal\", \"v-type\", \"no…\n## $ am_cat &lt;chr&gt; \"manual\", \"manual\", \"manual\", \"automatic\", \"automatic\", \"automa…\n\n#Assoziation zwischen Anzahl Zylinder und Motorentyp ()\ntable(cars$vs_cat, cars$am_cat) # Achtung: sieht aus, als gäbe es weniger V-Motoren bei den handgeschalteten Autos\n##         \n##          automatic manual\n##   normal        12      6\n##   v-type         7      7\n\n#lass und das überprüfen\n#achtung: bei chi-square test kommt es sehr auf das format drauf an (er erwartet entweder vektoren oder eine matrix!)\n\n#exkurs um in es in ein matrix form zu bringen\nchi_sq_matrix &lt;- xtabs(~ vs_cat + am_cat, data = as.data.frame(cars)) # in diesem Spezialfall haben wir keine Kriteriumsvariable\n\n#1.version\nchi_sq &lt;-chisq.test(chi_sq_matrix)\n\n#2. version\nchi_sq &lt;- chisq.test(cars$am_cat, cars$vs_cat)\n\n#resp. fisher exacter test verwenden, da 2x2 table\nfisher.test(chi_sq_matrix)\n## \n##  Fisher's Exact Test for Count Data\n## \n## data:  chi_sq_matrix\n## p-value = 0.4727\n## alternative hypothesis: true odds ratio is not equal to 1\n## 95 percent confidence interval:\n##   0.3825342 10.5916087\n## sample estimates:\n## odds ratio \n##   1.956055\n\n#fisher exakter test\nfisher.test(cars$am_cat, cars$vs_cat)\n## \n##  Fisher's Exact Test for Count Data\n## \n## data:  cars$am_cat and cars$vs_cat\n## p-value = 0.4727\n## alternative hypothesis: true odds ratio is not equal to 1\n## 95 percent confidence interval:\n##   0.3825342 10.5916087\n## sample estimates:\n## odds ratio \n##   1.956055\n\n#visualisieren: kudos goes to https://mgimond.github.io/Stats-in-R/ChiSquare_test.html#3_two_factor_classification\nOP &lt;- par(mfrow=c(1,2), \"mar\"=c(1,1,3,1))\nmosaicplot(chi_sq$observed, cex.axis =1 , main = \"Observed counts\")\nmosaicplot(chi_sq$expected, cex.axis =1 , main = \"Expected counts\\n(if class had no influence)\")\n\n\n\npar(OP)"
  },
  {
    "objectID": "statKons/StatKons1_Demo_assoziationen.html#möglicher-text-für-ergebnisse",
    "href": "statKons/StatKons1_Demo_assoziationen.html#möglicher-text-für-ergebnisse",
    "title": "StatKons1: Demo",
    "section": "Möglicher Text für Ergebnisse",
    "text": "Möglicher Text für Ergebnisse\nDer \\(\\chi^2\\)-Test sagt uns, dass das Art des Motors und Art des Fahrwerks statistisch nicht zusammenhängen. Es gibt keine signifikante Unterscheide zwischen den Variablen “VS” und “AM - Transmission” (p = .555). Der Fisher exacter Test bestätigt diesen Befund. Die Odds Ratio (OR) sagt uns hingegen - unter der Prämisse, dass “normale” Motoren eher mit automatischen und V-Motoren eher mit handgeschalteten Fahrwerken ausgestattet sind - dass die Chance doppelt so hoch ist, dass ein Auto mit “normalem” Motor automatisch geschaltet ist, als dies bei einem Auto mit V-Motor der Fall wäre\n\n#define dataset\ncars &lt;- mtcars\n\n#neue kategoriale variable\ncars &lt;- \n  as_tibble(cars) |&gt; # da \"nur\" data frame kann glimplse nichts damit anfangen \n  mutate(vs_cat = if_else(vs == 0, \"normal\", \"v-type\")) |&gt; \n  mutate(am_cat = if_else(am == 0, \"automatic\", \"manual\"))\n\n# bei t-Test immer zuerst visualisieren: in diesem Fall Boxplot mit Variablen Getriebe (v- vs. s-motor) und Anzahl Pferdestärke\nggplot2::ggplot(cars, aes(y = hp, x = vs_cat)) +\n  stat_boxplot(geom ='errorbar', width = .25) +\n  geom_boxplot() +\n  # geom_violin()+\n  labs(x = \"\\nBauform Motor\", y = \"Pferdestärke (PS)\\n\") +\n  mytheme\n## Error in loadNamespace(x): there is no package called 'ggplot2'\n  \n#alternativ     \nboxplot(cars$hp ~ cars$vs_cat) # sieht ganz ok aus, jedoch weist die variable \"normale Motoren\" deutlich eine grössere Streuung aus -&gt; siehe aov.1 und deren Modelgüte-Plots\n\n\n\n\n# Definiere Model: t-Test, wobei die AV metrisch (in unserem Fall eine Zählvariable) sein muss\nttest &lt;- t.test(cars$hp ~ cars$vs_cat)\naov.1 &lt;- aov(cars$hp ~ cars$vs_cat)\n\n#schaue Modellgüte an\npar(mfrow = c(2,2))\nplot(aov.1)\n\n\n\n\n#zeige resultate\nttest\n## \n##  Welch Two Sample t-test\n## \n## data:  cars$hp by cars$vs_cat\n## t = 6.2908, df = 23.561, p-value = 1.82e-06\n## alternative hypothesis: true difference in means between group normal and group v-type is not equal to 0\n## 95 percent confidence interval:\n##   66.06161 130.66854\n## sample estimates:\n## mean in group normal mean in group v-type \n##            189.72222             91.35714\nsummary.lm(aov.1)\n## \n## Call:\n## aov(formula = cars$hp ~ cars$vs_cat)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -98.72 -25.61  -4.04  22.55 145.28 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)         189.72      11.35  16.720  &lt; 2e-16 ***\n## cars$vs_catv-type   -98.37      17.16  -5.734 2.94e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 48.14 on 30 degrees of freedom\n## Multiple R-squared:  0.5229, Adjusted R-squared:  0.507 \n## F-statistic: 32.88 on 1 and 30 DF,  p-value: 2.941e-06\n\n#wie würdet ihr nun die Ergebnisse darstellen?\n\n\n# für mehr infos here: https://cran.r-project.org/web/packages/datasauRus/vignettes/Datasaurus.html\n\nlibrary(datasauRus)\nif(requireNamespace(\"dplyr\")){\n  suppressPackageStartupMessages(library(dplyr))\n  dt &lt;- datasaurus_dozen |&gt; \n    group_by(dataset) |&gt; \n    summarize(\n      mean_x    = mean(x),\n      mean_y    = mean(y),\n      std_dev_x = sd(x),\n      std_dev_y = sd(y),\n      corr_x_y  = cor(x, y)\n    )\n}\n\n# check data structure\nglimpse(dt)\n## Rows: 13\n## Columns: 6\n## $ dataset   &lt;chr&gt; \"away\", \"bullseye\", \"circle\", \"dino\", \"dots\", \"h_lines\", \"hi…\n## $ mean_x    &lt;dbl&gt; 54.26610, 54.26873, 54.26732, 54.26327, 54.26030, 54.26144, …\n## $ mean_y    &lt;dbl&gt; 47.83472, 47.83082, 47.83772, 47.83225, 47.83983, 47.83025, …\n## $ std_dev_x &lt;dbl&gt; 16.76982, 16.76924, 16.76001, 16.76514, 16.76774, 16.76590, …\n## $ std_dev_y &lt;dbl&gt; 26.93974, 26.93573, 26.93004, 26.93540, 26.93019, 26.93988, …\n## $ corr_x_y  &lt;dbl&gt; -0.06412835, -0.06858639, -0.06834336, -0.06447185, -0.06034…\n\n# plot two examples  \nif(requireNamespace(\"ggplot2\")){\n  library(ggplot2)\n  \n  dt = filter(datasaurus_dozen, dataset == \"dino\" | dataset == \"slant_up\")\n  \n  ggplot(dt, aes(x=x, y=y, colour=dataset))+\n    geom_point()+\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    facet_wrap(~dataset) +\n    geom_smooth(method = \"lm\", se = FALSE)\n  \n}"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#in-r",
    "href": "statKons/StatKons1_Demo_open_datasets.html#in-r",
    "title": "StatKons1: Open Datasets",
    "section": "in R",
    "text": "in R\nIn R gibt es vordefinierte Datensätze, welche gut abrufbar sind. Beispiele sind:\n\nsleep\nUSAccDeaths\nUSArrests\nTitanic\n\n\ndata()  # erzeugt eine Liste mit den Datensätzen, welche in R verfügbaren sind\nhead(chickwts)\n##   weight      feed\n## 1    179 horsebean\n## 2    160 horsebean\n## 3    136 horsebean\n## 4    227 horsebean\n## 5    217 horsebean\n## 6    168 horsebean\nstr(chickwts)\n## 'data.frame':    71 obs. of  2 variables:\n##  $ weight: num  179 160 136 227 217 168 108 124 143 140 ...\n##  $ feed  : Factor w/ 6 levels \"casein\",\"horsebean\",..: 2 2 2 2 2 2 2 2 2 2 ..."
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#kaggle",
    "href": "statKons/StatKons1_Demo_open_datasets.html#kaggle",
    "title": "StatKons1: Open Datasets",
    "section": "Kaggle",
    "text": "Kaggle\nAuf Kaggle findet ihr öffentlich zugängliche Datensätze. Einzig was ihr tun müsst, ist euch registrieren. Beispiele sind:\n\n911\nfoodPreferences\nS.F. salaries\nTitanic\n…\n\n\n# Load packages and data\ndata_911 &lt;- read_delim(\"datasets/statistik/911.csv\", delim = \",\")\nstr(data_911)\n## spc_tbl_ [99,492 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ lat      : num [1:99492] 40.3 40.3 40.1 40.1 40.3 ...\n##  $ lng      : num [1:99492] -75.6 -75.3 -75.4 -75.3 -75.6 ...\n##  $ desc     : chr [1:99492] \"REINDEER CT & DEAD END;  NEW HANOVER; Station 332; 2015-12-10 @ 17:10:52;\" \"BRIAR PATH & WHITEMARSH LN;  HATFIELD TOWNSHIP; Station 345; 2015-12-10 @ 17:29:21;\" \"HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-Station:STA27;\" \"AIRY ST & SWEDE ST;  NORRISTOWN; Station 308A; 2015-12-10 @ 16:47:36;\" ...\n##  $ zip      : num [1:99492] 19525 19446 19401 19401 NA ...\n##  $ title    : chr [1:99492] \"EMS: BACK PAINS/INJURY\" \"EMS: DIABETIC EMERGENCY\" \"Fire: GAS-ODOR/LEAK\" \"EMS: CARDIAC EMERGENCY\" ...\n##  $ timeStamp: POSIXct[1:99492], format: \"2015-12-10 17:40:00\" \"2015-12-10 17:40:00\" ...\n##  $ twp      : chr [1:99492] \"NEW HANOVER\" \"HATFIELD TOWNSHIP\" \"NORRISTOWN\" \"NORRISTOWN\" ...\n##  $ addr     : chr [1:99492] \"REINDEER CT & DEAD END\" \"BRIAR PATH & WHITEMARSH LN\" \"HAWS AVE\" \"AIRY ST & SWEDE ST\" ...\n##  $ e        : num [1:99492] 1 1 1 1 1 1 1 1 1 1 ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   lat = col_double(),\n##   ..   lng = col_double(),\n##   ..   desc = col_character(),\n##   ..   zip = col_double(),\n##   ..   title = col_character(),\n##   ..   timeStamp = col_datetime(format = \"\"),\n##   ..   twp = col_character(),\n##   ..   addr = col_character(),\n##   ..   e = col_double()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#tidytuesday",
    "href": "statKons/StatKons1_Demo_open_datasets.html#tidytuesday",
    "title": "StatKons1: Open Datasets",
    "section": "Tidytuesday",
    "text": "Tidytuesday\nTidytuesday ist eine Plattform, in der wöchentlich - jeden Dienstag - einen öffentlich zugänglichen Datensatz publiziert. Dieses Projekt ist aus der R4DS Online Learning Community und dem R for Data Science Lehrbuch hervorgegangen. Beispiele sind:\n\nWomen in the Workplace\nDairy production & Consumption\nStar Wars Survey\nGlobal Coffee Chains\nMalaria Deaths\n…\n\nDownload via Github - 1. Möglichkeit\n\nGeht zum File, welches ihr herunterladen wollt\nKlickt auf das File (.csv, .xlsx etc.), um den Inhalt innerhalb der GitHub Benutzeroberfläche anzuzeigen\n\n\n\nKlickt mit der rechten Maustaste auf den Knopf “raw”\n\n\n\n(Ziel) Speichern unter…\n\nDownload via Github - 2. Möglichkeit\n\n# Beachtet dabei, dass ihr die URL zum originalen (raw) Datensatz habt\nstar_wars &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-05-14/week7_starwars.csv\",\n    locale = readr::locale(encoding = \"latin1\"))  #not working yet"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#opendata.swiss",
    "href": "statKons/StatKons1_Demo_open_datasets.html#opendata.swiss",
    "title": "StatKons1: Open Datasets",
    "section": "opendata.swiss",
    "text": "opendata.swiss\nAuf opendata.swiss sind offene, frei verfügbare Daten der Schweizerischen Behörden zu finden. opendata.swiss ist ein gemeinsames Projekt von Bund, Kantonen, Gemeinden und weiteren Organisationen mit einem staatlichen Auftrag. Beispiele sind:\n\nStatistik der Schweizer Städte\nBevölkerung nach Stadtquartier, Herkunft, Geschlecht und Alter\nAltpapiermengen\n…"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#open-data-katalog-stadt-zürich",
    "href": "statKons/StatKons1_Demo_open_datasets.html#open-data-katalog-stadt-zürich",
    "title": "StatKons1: Open Datasets",
    "section": "Open Data Katalog Stadt Zürich",
    "text": "Open Data Katalog Stadt Zürich\nAuf der Seite der Stadt Zürich Open Data findet ihr verschiedene Datensätze der Stadt Zürich. Spannend daran ist, dass die veröffentlichten Daten kostenlos und zur freien - auch kommerziellen - Weiterverwendung zur Verfügung. Beispiele sind:\n\nBevölkerung nach Bildungsstand, Jahr, Alter und Geschlecht seit 1970\nLuftqualitätsmessungen\nHäufigste Hauptsprachen\n…\n\n\n# lade die Datei 'Häufigste Sprachen'\nurlfile = \"https://data.stadt-zuerich.ch/dataset/bfs_ste_bev_hauptsprachen_top50_od3011/download/BEV301OD3011.csv\"\n\ndat_lang &lt;- read_delim(url(urlfile), delim = \",\", col_names = T)\nhead(dat_lang)\n## # A tibble: 6 × 7\n##   Sprache        AntBev AnzBev untAntBevKI obAntBevKI untAnzBevKI obAnzBevKI\n##   &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Deutsch          74.5 259890        74         75        258160     261610\n## 2 Englisch         13.9  48660        13.5       14.3       47290      50030\n## 3 Italienisch       6.1  21240         5.8        6.4       20310      22170\n## 4 Französisch       4.8  16920         4.6        5.1       16090      17760\n## 5 Spanisch          4.4  15300         4.2        4.6       14490      16110\n## 6 Serbokroatisch    2.9  10130         2.7        3.1        9470      10800"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#pca-mit-sveg",
    "href": "statKons/StatKons2_Demo_PCA.html#pca-mit-sveg",
    "title": "StatKons2: Demo",
    "section": "PCA mit sveg",
    "text": "PCA mit sveg\n\n# Mit Beispieldaten aus Wildi (2013, 2017)\nlibrary(labdsv)\nlibrary(dave)  # lade package für Daten sveg\nhead(sveg)\nstr(sveg)\n# View(sveg)\n\n# PCA----------- Deckungen Wurzeltransformiert, cor=T erzwingt Nutzung der\n# Korrelationsmatrix\no.pca &lt;- labdsv::pca(sveg^0.25, cor = T)\no.pca2 &lt;- stats::prcomp(sveg^0.25)\n\n# Koordinaten im Ordinationsraum =&gt; Y\nhead(o.pca$scores)\nhead(o.pca2$x)\n\n# Korrelationen der Variablen mit den Ordinationsachsen\nhead(o.pca$loadings)\nhead(o.pca2$rotation)\n\n# Erklaerte Varianz der Achsen (sdev ist die Wurzel daraus) früher gabs den\n# Befehl summary() jetzt von hand: standardabweichung im quadrat/totale varianz\n# * 100 (um prozentwerte zu bekommen)\nE &lt;- o.pca$sdev^2/o.pca$totdev * 100\nE[1:5]  # erste fünf PCA\n\n# package stats funktioniert summary()\nsummary(o.pca2)\n\n# PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nplot(o.pca$scores[, 1], o.pca$scores[, 2], type = \"n\", asp = 1, xlab = \"PC1\", ylab = \"PC2\")\npoints(o.pca$scores[, 1], o.pca$scores[, 2], pch = 18)\n\n\n\n\nplot(o.pca$scores[, 1], o.pca$scores[, 3], type = \"n\", asp = 1, xlab = \"PC1\", ylab = \"PC3\")\npoints(o.pca$scores[, 1], o.pca$scores[, 3], pch = 18)\n\n\n\n\n# Subjektive Auswahl von Arten zur Darstellung\nsel.sp &lt;- c(3, 11, 23, 39, 46, 72, 77, 96, 101, 119)\nsnames &lt;- names(sveg[, sel.sp])\nsnames\n\n# PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (hier\n# reduction der observationen)\nx &lt;- o.pca$loadings[, 1]\ny &lt;- o.pca$loadings[, 2]\nplot(x, y, type = \"n\", asp = 1)\narrows(0, 0, x[sel.sp], y[sel.sp], length = 0.08)\ntext(x[sel.sp], y[sel.sp], snames, pos = 1, cex = 0.6)\n\n\n\n\n# hier gehts noch zu weiteren Beispielen zu PCA's:\n# https://stats.stackexchange.com/questions/102882/steps-done-in-factor-analysis-compared-to-steps-done-in-pca/102999#102999\n# https://stats.stackexchange.com/questions/222/what-are-principal-component-scores\n# https://stats.stackexchange.com/questions/102882/steps-done-in-factor-analysis-compared-to-steps-done-in-pca/102999#102999"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#pca-mit-beispiel-aus-skript",
    "href": "statKons/StatKons2_Demo_PCA.html#pca-mit-beispiel-aus-skript",
    "title": "StatKons2: Demo",
    "section": "PCA mit Beispiel aus Skript",
    "text": "PCA mit Beispiel aus Skript\n\n# Idee von Ordinationen aus Wildi p. 73-74\n\n# Für Ordinationen benötigen wir Matrizen, nicht Data.frames Generieren von\n# Daten\nraw &lt;- matrix(c(1, 2, 2.5, 2.5, 1, 0.5, 0, 1, 2, 4, 3, 1), nrow = 6)\ncolnames(raw) &lt;- c(\"spec.1\", \"spec.2\")\nrownames(raw) &lt;- c(\"r1\", \"r2\", \"r3\", \"r4\", \"r5\", \"r6\")\nraw\n\n# originale Daten im zweidimensionalen Raum\nx1 &lt;- raw[, 1]\ny1 &lt;- raw[, 2]\nz &lt;- c(rep(1:6))\n\n# Plot Abhängigkeit der Arten vom Umweltgradienten\nplot(c(x1, y1) ~ c(z, z), type = \"n\", axes = T, bty = \"l\", las = 1, xlim = c(1, 6),\n    ylim = c(0, 5), xlab = \"Umweltgradient\", ylab = \"Deckung der Arten\")\npoints(x1 ~ z, pch = 21, type = \"b\")\npoints(y1 ~ z, pch = 16, type = \"b\")\n\n\n\n\n# zentrierte Daten\ncent &lt;- scale(raw, scale = F)\nx2 &lt;- cent[, 1]\ny2 &lt;- cent[, 2]\n\n# rotierte Daten\no.pca &lt;- pca(raw)\nx3 &lt;- o.pca$scores[, 1]\ny3 &lt;- o.pca$scores[, 2]\n\n# Visualisierung der Schritte im Ordinationsraum\nplot(c(y1, y2, y3) ~ c(x1, x2, x3), type = \"n\", axes = T, bty = \"l\", las = 1, xlim = c(-4,\n    4), ylim = c(-4, 4), xlab = \"Art 1\", ylab = \"Art 2\")\npoints(y1 ~ x1, pch = 21, type = \"b\", col = \"green\", lwd = 2)\npoints(y2 ~ x2, pch = 16, type = \"b\", col = \"red\", lwd = 2)\npoints(y3 ~ x3, pch = 17, type = \"b\", col = \"blue\", lwd = 2)\n\n\n\n\n# zusammengefasst:-------\n\n# Durchführung der PCA\npca &lt;- pca(raw)\n\n# Koordinaten im Ordinationsraum\npca$scores\n\n# Korrelationen der Variablen mit den Ordinationsachsen\npca$loadings\n\n# Erklärte Varianz der Achsen in Prozent\nE &lt;- pca$sdev^2/pca$totdev * 100\nE\n\n### excurs für weitere r-packages####\n\n# mit prcomp, ein weiteres Package für Ordinationen\npca.2 &lt;- stats::prcomp(raw, scale = F)\nsummary(pca.2)\nplot(pca.2)\n\n\n\nbiplot(pca.2)\n\n\n\n\n# mit vegan, ein anderes Package für Ordinationen\npca.3 &lt;- vegan::rda(raw, scale = FALSE)  #Die Funktion rda führt ein PCA aus an wenn nicht Umwelt- und Artdaten definiert werden\n# scores(pca.3,display=c('sites')) scores(pca.3,display=c('species'))\nsummary(pca.3, axes = 0)\nbiplot(pca.3, scaling = 2)\nbiplot(pca.3, scaling = \"species\")  #scaling=species macht das selbe wie scaling=2"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#ca-mit-sveg",
    "href": "statKons/StatKons2_Demo_PCA.html#ca-mit-sveg",
    "title": "StatKons2: Demo",
    "section": "CA mit sveg",
    "text": "CA mit sveg\n\n\nlibrary(vegan)\nlibrary(dave)  #for the dataset sveg\nlibrary(FactoMineR)  # siehe Beispiel hier: https://www.youtube.com/watch?v=vP4korRby0Q\n\n# ebenfalls mit transformierten daten\no.ca &lt;- cca(sveg^0.5)  #package vegan\no.ca1 &lt;- CA(sveg^0.5)  #package FactoMineR\n\n\n\n\n# Arten (o) und Communities (+) plotten\nplot(o.ca)\n\n\n\nsummary(o.ca1)\n\n# Nur Arten plotten\nx &lt;- o.ca$CA$u[, 1]\ny &lt;- o.ca$CA$u[, 2]\nplot(x, y)\n\n\n\n\n# Anteilige Varianz, die durch die ersten beiden Achsen erklaert wird\no.ca$CA$eig[1:63]/sum(o.ca$CA$eig)"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#nmds-mit-sveg",
    "href": "statKons/StatKons2_Demo_PCA.html#nmds-mit-sveg",
    "title": "StatKons2: Demo",
    "section": "NMDS mit sveg",
    "text": "NMDS mit sveg\n\n# NMDS----------\n\n# Distanzmatrix als Start erzeugen\nlibrary(MASS)\nlibrary(vegan)\n\nmde &lt;- vegdist(sveg, method = \"euclidean\")\nmdm &lt;- vegdist(sveg, method = \"manhattan\")\n\n# Zwei verschiedene NMDS-Methoden\nset.seed(1)  #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\no.imds &lt;- isoMDS(mde, k = 2)  # mit K = Dimensionen\nset.seed(1)\no.mmds &lt;- metaMDS(mde, k = 3)  # scheint nicht mit 2 Dimensionen zu konvergieren\n\nplot(o.imds$points)\nplot(o.mmds$points)\n\n# Stress = Abweichung der zweidimensionalen NMDS-Loesung von der originalen\n# Distanzmatrix\nstressplot(o.imds, mde)\nstressplot(o.mmds, mde)"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#pca-mit-mtcars",
    "href": "statKons/StatKons2_Demo_PCA.html#pca-mit-mtcars",
    "title": "StatKons2: Demo",
    "section": "PCA mit mtcars",
    "text": "PCA mit mtcars\n\n# Beispiel inspiriert von Luke Hayden:\n# https://www.datacamp.com/community/tutorials/pca-analysis-r\n\n# Ausgangslage: viel zusammenhängende Variablen Ziel: Reduktion der\n# Variablenkomplexität WICHTIG hier: Datenformat muss Wide sein! Damit die\n# Matrixmultiplikation gemacht werden kann\n\n# lade Datei\ncars &lt;- mtcars\n\n# Korrelationen\ncor &lt;- cor(cars[, c(1:7, 10, 11)])\ncor[abs(cor) &lt; 0.7] &lt;- 0\ncor\n\n# definiere Datei für PCA\ncars &lt;- mtcars[, c(1:7, 10, 11)]\n\n# pca achtung unterschiedliche messeinheiten, wichtig es muss noch einheitlich\n# transfomiert werden\nlibrary(FactoMineR)  # siehe Beispiel hier: https://www.youtube.com/watch?v=vP4korRby0Q\no.pca &lt;- PCA(cars, scale.unit = TRUE)  # entweder korrelations oder covarianzmatrix\n\n# schaue output an\nsummary(o.pca)  # generiert auch automatische plots"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#ca-mit-mtcars",
    "href": "statKons/StatKons2_Demo_PCA.html#ca-mit-mtcars",
    "title": "StatKons2: Demo",
    "section": "CA mit mtcars",
    "text": "CA mit mtcars\n\nlibrary(vegan)\n\n# ebenfalls mit transformierten daten\no.ca &lt;- vegan::cca(cars)\no.ca1 &lt;- FactoMineR::CA(cars)  #blau: auots, rot: variablen\n\n\n\n\n# plotten (schwarz: autos, rot: variablen)\nplot(o.ca)\n\n\n\nsummary(o.ca)\nsummary(o.ca1)\n\n# Nur autos plotten; wieso?\nx &lt;- o.ca$CA$u[, 1]\ny &lt;- o.ca$CA$u[, 2]\nplot(x, y)\n\n\n\n\n# Anteilige Varianz, die durch die ersten beiden Achsen erklaert wird\no.ca$CA$eig[1:8]/sum(o.ca$CA$eig)"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#nmds-mit-mtcars",
    "href": "statKons/StatKons2_Demo_PCA.html#nmds-mit-mtcars",
    "title": "StatKons2: Demo",
    "section": "NMDS mit mtcars",
    "text": "NMDS mit mtcars\n\n# Distanzmatrix als Start erzeugen\nlibrary(MASS)\n\nmde &lt;- vegan::vegdist(cars, method = \"euclidean\")\nmdm &lt;- vegan::vegdist(cars, method = \"manhattan\")\n\n# Zwei verschiedene NMDS-Methoden\nset.seed(1)  #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\no.mde.mass &lt;- MASS::isoMDS(mde, k = 2)  # mit K = Dimensionen\no.mdm.mass &lt;- MASS::isoMDS(mdm)\n\nset.seed(1)\no.mde.vegan &lt;- vegan::metaMDS(mde, k = 1)  # scheint nicht mit 2 Dimensionen zu konvergieren\no.mdm.vegan &lt;- vegan::metaMDS(mdm, k = 2)\n\n# plot euclidean distance\nplot(o.mde.mass$points)\n\n\n\nplot(o.mde.vegan$points)\n\n\n\n\n# plot manhattan distance\nplot(o.mdm.mass$points)\n\n\n\nplot(o.mdm.vegan$points)\n\n\n\n\n# Stress = Abweichung der zweidimensionalen NMDS-Loesung von der originalen\n# Distanzmatrix\nvegan::stressplot(o.mde.vegan, mde)\n\n\n\nvegan::stressplot(o.mde.mass, mde)"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#einfaktorielle-anova",
    "href": "statKons/StatKons3_Demo_LM.html#einfaktorielle-anova",
    "title": "StatKons3: Demo",
    "section": "Einfaktorielle ANOVA",
    "text": "Einfaktorielle ANOVA\n\n# für mehr infos\n# https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html\n\ncars &lt;- mtcars |&gt;\n    mutate(cyl = as.factor(cyl)) |&gt;\n    slice(-31)  # lösch die 31ste Zeile\n\n# Alternativ ginge auch das\ncars[-31, ]\n\n# schaue daten zuerst mal an 1. Responsevariable\nhist(cars$hp)  # nur sinnvoll bei grossem n\nboxplot(cars$hp)\n\n\n# 2. Responsevariable ~ Prediktorvariable\ntable(cars$cyl)  # mögliches probel, da n's unterschiedlich gross\n\nboxplot(cars$hp ~ cars$cyl)  # varianzheterogentität weniger das problem, \n# aber normalverteilung der residuen problematisch\n\n# definiere das modell für eine ein-faktorielle anova\naov.1 &lt;- aov(log10(hp) ~ cyl, data = cars)\n\n# 3. Schaue Modelgüte an\npar(mfrow = c(2, 2))\nplot(aov.1)\n\n# 4. Schaue output an und ordne es ein\nsummary.lm(aov.1)\n\n# 5. bei meheren Kategorien wende einen post-hoc Vergleichstest an\nTukeyHSD(aov.1)\n\n# 6. Ergebnisse passend darstellen\nlibrary(multcomp)\n\n# erstens die signifikanten Unterschiede mit Buchstaben versehen\nletters &lt;- multcomp::cld(multcomp::glht(aov.1, linfct = multcomp::mcp(cyl = \"Tukey\")))  # Achtung die kategoriale\n# Variable (unsere unabhängige Variable 'cyl') muss als Faktor definiert sein\n# z.B. as.factor()\n\n# einfachere Variante\nboxplot(hp ~ cyl, data = cars)\nmtext(letters$mcletters$Letters, at = 1:3)\n\n# schönere Variante :)\nggplot(cars, aes(x = cyl, y = hp)) + stat_boxplot(geom = \"errorbar\", width = 0.5) +\n    geom_boxplot(size = 1) + annotate(\"text\", x = 1, y = 350, label = \"a\", size = 7) +\n    annotate(\"text\", x = 2, y = 350, label = \"b\", size = 7) + annotate(\"text\", x = 3,\n    y = 350, label = \"c\", size = 7)\nlabs(x = \"\\nAnzahl Zylinder\", y = \"Pferdestärke\") + mytheme\n\n# Plot exportieren\nggsave(filename = \"statKons/distill-preview.png\", device = \"png\")  # hier kann man festlegen, was für ein Bildformat\n# exportiert werden möchte\n\n# Sind die Voraussetzungen für eine Anova verletzt, überprüfe alternative\n# nicht-parametische Tests z.B. oneway-Test mit Welch-korrektur für ungleiche\n# Varianzen (Achtung auch dieser Test hat Voraussetzungen -&gt; siehe Skript XY)\nlibrary(rosetta)\nwelch1 &lt;- oneway.test(hp ~ cyl, data = cars, var.equal = FALSE)\nrosetta::posthocTGH(cars$hp, cars$cyl, method = \"games-howell\")"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#mehrfaktorielle-anova",
    "href": "statKons/StatKons3_Demo_LM.html#mehrfaktorielle-anova",
    "title": "StatKons3: Demo",
    "section": "Mehrfaktorielle ANOVA",
    "text": "Mehrfaktorielle ANOVA\n\n\n\n\n## \n## Call:\n## aov(formula = hp ~ cyl * am + wt, data = cars)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -33.834 -14.280  -7.418   7.120  60.282 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   32.743     31.636   1.035 0.310980    \n## cyl6          22.556     20.859   1.081 0.290274    \n## cyl8          88.818     20.463   4.340 0.000222 ***\n## am            13.002     19.952   0.652 0.520811    \n## wt            17.691      9.409   1.880 0.072272 .  \n## cyl6:am       14.626     27.392   0.534 0.598276    \n## cyl8:am       73.356     33.194   2.210 0.036894 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 26.73 on 24 degrees of freedom\n## Multiple R-squared:  0.8428, Adjusted R-squared:  0.8035 \n## F-statistic: 21.45 on 6 and 24 DF,  p-value: 1.511e-08"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#einfache-regression",
    "href": "statKons/StatKons3_Demo_LM.html#einfache-regression",
    "title": "StatKons3: Demo",
    "section": "Einfache Regression",
    "text": "Einfache Regression\n\n# inspiriert von Simon Jackson: http s://drsimonj.svbtle.com/visualising-residuals\ncars &lt;- mtcars |&gt; \n  #ändere die unabhängige Variable mpg in 100Km/L\n  mutate(kml = (235.214583/mpg)) # mehr Infos hier: https://www.asknumbers.com/mpg-to-L100km.aspx\n  # |&gt;  # klone data set\n  # slice(-31) # # lösche Maserrati und schaue nochmals Modelfit an\n\n#############\n##1.Daten anschauen\n############\n\n# Zusammenhang mal anschauen\n# Achtung kml = 100km pro Liter \nplot(hp ~ kml, data = cars)\n\n\n\n\n# Responsevariable anschauen\nboxplot(cars$hp)\n\n\n\n\n# Korrelationen uv + av anschauen\n# Reihenfolge spielt hier keine Rolle, wieso?\ncor(cars$kml, cars$hp) # hängen stark zusammen\n## [1] 0.7629477\n\n###################\n#2. Modell definieren: einfache regression\n##################\nmodel &lt;- lm(hp ~ kml, data = cars)\nsummary.lm(model)\n## \n## Call:\n## lm(formula = hp ~ kml, data = cars)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -75.22 -25.52 -13.31  30.92 148.69 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -26.021     27.880  -0.933    0.358    \n## kml           13.540      2.095   6.464 3.84e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 45.06 on 30 degrees of freedom\n## Multiple R-squared:  0.5821, Adjusted R-squared:  0.5682 \n## F-statistic: 41.79 on 1 and 30 DF,  p-value: 3.839e-07\n\n###############\n#3.Modeldiagnostik und ggf. Anpassungen ans Modell oder ähnliches\n###############\n\n# semi schöne Ergebnisse\nlibrary(ggfortify)\nggplot2::autoplot(model) + mytheme # gitb einige Extremwerte =&gt; was tun? (Eingabe/Einlesen \n\n\n\n#überprüfen, Transformation, Extremwerte nur ausschliessen mit guter Begründung)\n\n# erzeuge vorhergesagte Werte und Residualwerte\ncars$predicted &lt;- predict(model)   # bilde neue Variable mit geschätzten y-Werten\ncars$residuals &lt;- residuals(model)\n\n# schaue es dir an, sieht man gut was die Residuen sind\nd &lt;- cars |&gt;  \n    dplyr::select(hp, kml, predicted, residuals)\n\n# schauen wir es uns an\nhead(d, 4)\n##                 hp      kml predicted residuals\n## Mazda RX4      110 11.20069  125.6411 -15.64107\n## Mazda RX4 Wag  110 11.20069  125.6411 -15.64107\n## Datsun 710      93 10.31643  113.6678 -20.66776\n## Hornet 4 Drive 110 10.99134  122.8063 -12.80626\n\n#visualisiere residuen\nggplot(d, aes(x = kml, y = hp)) +\n  # verbinde beobachtete werte mit vorausgesagte werte\n  geom_segment(aes(xend = kml, yend = predicted)) + \n  geom_point() + # Plot the actual points\n  geom_point(aes(y = predicted), shape = 4) + # plot geschätzten y-Werten\n  # geom_line(aes(y = predicted), color = \"lightgrey\") # alternativ code\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n  # Farbe wird hier zu den redisuen gemapped, abs(residuals) wegen negativen zahlen  \n  geom_point(aes(color = abs(residuals))) + \n  # Colors to use here (für mehrere farben verwende color_gradient2)\n  scale_color_continuous(low = \"blue\", high = \"red\") +  \n  scale_x_continuous(limits = c(0, 40)) +\n  scale_y_continuous(limits = c(0, 300)) +\n  guides(color = \"none\") +  # Color legende entfernen\n  labs(x = \"\\nVerbraucht in Liter pro 100km\", y = \"Motorleistung in PS\\n\") +\n  mytheme\n\n\n\n\n##########\n#4. plotte Ergebnis\n##########\nggplot(d, aes(x = kml, y = hp)) +\n    geom_point(size = 4) +\n    # geom_point(aes(y = predicted), shape = 1, size = 4) +\n    # plot regression line\n    geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n    #intercept\n    geom_line(aes(y = mean(hp)), color = \"blue\") +\n    mytheme"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#multiple-regression",
    "href": "statKons/StatKons3_Demo_LM.html#multiple-regression",
    "title": "StatKons3: Demo",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\n# Select data\ncars &lt;- mtcars |&gt; \n    slice(-31) |&gt;\n    mutate(kml = (235.214583/mpg)) |&gt; \n    dplyr::select(kml, hp, wt, disp)\n\n################\n# 1. Multikollinearitüt überprüfen\n# Korrelation zwischen Prädiktoren kleiner .7\ncor &lt;- cor(cars[, -2])\ncor[abs(cor)&lt;0.7] &lt;- 0  \ncor # \n##            kml        wt      disp\n## kml  1.0000000 0.8912658 0.8786238\n## wt   0.8912658 1.0000000 0.8878515\n## disp 0.8786238 0.8878515 1.0000000\n\n##### info zu Variablen\n#wt = gewicht\n#disp = hubraum\n\n###############\n#2. Responsevariable + Kriteriumsvariable anschauen\n##############\n# was würdet ihr tun?\n\n############\n#3. Definiere das Model\n############\nmodel1 &lt;- lm(hp ~ kml + wt + disp, data = cars) \nmodel2 &lt;- lm(hp ~ kml + wt, data = cars)\nmodel3 &lt;- lm(log10(hp) ~ kml + wt, data = cars)\n\n#############\n#4. Modeldiagnostik\n############\n\nlibrary(ggfortify)\nggplot2::autoplot(model1)\n\n\n\nggplot2::autoplot(model2) # besser, immernoch nicht ok =&gt; transformation? vgl. model3\n\n\n\nggplot2::autoplot(model3)\n\n\n\n\n############\n#5. Modellfit vorhersagen: wie gut sagt mein Modell meine Daten vorher\n############\n\n#es gibt 3 Mögliche Wege\n\n# gebe dir predicted values aus für model2 (für vorzeigebeispiel einfacher :)\n# gibts unterschidliche varianten die predicted values zu berechnen\n# 1. default funktion predict(model) verwenden\ncars$predicted &lt;- predict(model2)\n\n# 2. datensatz selber zusammenstellen (nicht empfohlen): wichtig, die \n# prädiktoren müssen denselben\n# namen haben wie im Model\n# besser mit Traindata von Beginn an mehr Infos hier: https://www.r-bloggers.com/using-linear-regression-to-predict-energy-output-of-a-power-plant/\n\nnew.data &lt;- tibble(kml = sample(seq(6.9384, 22.61, .3), 31),\n                   wt = sample(seq(1.513, 5.424, 0.01), 31),\n                   disp = sample(seq(71.1, 472.0, .1), 31)) \ncars$predicted_own &lt;- predict(model2, newdata = new.data)\n\n# 3. train_test_split durchführen (empfohlen) muss jedoch von beginn an bereits \n# gemacht werden - Logik findet ihr hier: https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6 oder https://towardsdatascience.com/6-amateur-mistakes-ive-made-working-with-train-test-splits-916fabb421bb\n# beispiel hier: https://ijlyttle.github.io/model_cv_selection.html\ncars &lt;- mtcars |&gt; \n  mutate(id = row_number()) |&gt;  # für das mergen der Datensätze\n  mutate(kml = (235.214583/mpg)) |&gt; \n  dplyr::select(kml, hp, wt, disp, id)\n  \ntrain_data &lt;- cars |&gt; \n  dplyr::sample_frac(.75) # für das Modellfitting\n\ntest_data  &lt;- dplyr::anti_join(cars, train_data, by = 'id') # für den Test mit predict\n\n# erstelle das Modell und \"trainiere\" es auf den train Datensatz\nmodel2_train &lt;- lm(hp ~ kml + wt, data = train_data)\n\n# mit dem \"neuen\" Datensatz wird das Model überprüft ob guter Modelfit\ntrain_data$predicted_test &lt;- predict(model2_train, newdata = test_data)\n\n# Residuen\ntrain_data$residuals &lt;- residuals(model2_train)\nhead(train_data)\n##                      kml  hp    wt  disp id predicted_test residuals\n## Mazda RX4 Wag  11.200694 110 2.875 160.0  2      130.95950 -16.93706\n## Fiat X1-9       8.615919  66 1.935  79.0 26      119.16903 -27.46760\n## Ford Pantera L 14.886999 264 3.170 351.0 29      143.76592  72.83667\n## Lotus Europa    7.737322 113 1.513  95.1 28      158.21111  29.29249\n## Duster 360     16.448572 245 3.570 360.0  7      300.04143  30.96793\n## Valiant        12.995281 105 3.460 225.0  6       82.09853 -46.24147\n\n#weiterführende Infos zu \"machine learning\" Idee hier: https://stat-ata-asu.github.io/MachineLearningToolbox/regression-models-fitting-them-and-evaluating-their-performance.html\n#wichtigstes Packet in dieser Hinsicht ist \"caret\": https://topepo.github.io/caret/\n#beste Philosophie ist tidymodels: https://www.tidymodels.org\n\n#----------------\n# Schnelle variante mit broom\nd &lt;- lm(hp ~ kml + wt + disp, data = cars) |&gt; \n    broom::augment()\n\nhead(d)\n## # A tibble: 6 × 11\n##   .rownames            hp   kml    wt  disp .fitted .resid   .hat .sigma .cooksd\n##   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n## 1 Mazda RX4           110  11.2  2.62   160    123. -12.7  0.0478   41.4 1.29e-3\n## 2 Mazda RX4 Wag       110  11.2  2.88   160    114.  -4.21 0.0456   41.4 1.34e-4\n## 3 Datsun 710           93  10.3  2.32   108    103.  -9.87 0.0758   41.4 1.31e-3\n## 4 Hornet 4 Drive      110  11.0  3.22   258    142. -31.6  0.0958   41.0 1.77e-2\n## 5 Hornet Sportabout   175  12.6  3.44   360    191. -16.3  0.210    41.3 1.35e-2\n## 6 Valiant             105  13.0  3.46   225    138. -33.5  0.0445   40.9 8.22e-3\n## # ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\nggplot(d, aes(x = kml, y = hp)) +\n    geom_segment(aes(xend = kml, yend = .fitted), alpha = .2) +\n    geom_point(aes(color = .resid)) +\n    scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n    guides(color = \"none\") +\n    geom_point(aes(y = .fitted), shape = 4) +\n    scale_y_continuous(limits = c(0,350)) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n    mytheme\n\n\n\n\n############\n# 6. Modellvereinfachung\n############\n\n# Varianzpartitionierung\nlibrary(hier.part)\n## Error in library(hier.part): there is no package called 'hier.part'\ncars &lt;- mtcars |&gt; \n  mutate(kml = (235.214583/mpg)) |&gt; \n  select(-mpg)\n\nnames(cars) # finde \"position\" deiner Responsevariable\n##  [1] \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\" \"carb\"\n## [11] \"kml\"\n\nX = cars[, -3] # definiere all die Prädiktorvariablen im Model (minus Responsevar)\n\n# dauert ein paar sekunden\nhier.part(cars$hp, X, gof = \"Rsqu\")\n## Error in hier.part(cars$hp, X, gof = \"Rsqu\"): could not find function \"hier.part\"\n\n# alle Modelle miteinander vergleichen mit dredge Befehl: geht nur bis \n# maximal 15 Variablen\nmodel2 &lt;- lm(hp ~ ., data = cars)\nlibrary(MuMIn)\noptions(na.action = \"na.fail\")\nallmodels &lt;- dredge(model2)\nhead(allmodels)\n## Global model call: lm(formula = hp ~ ., data = cars)\n## ---\n## Model selection table \n##     (Intrc)  carb   cyl   disp   drat   kml   qsec    vs     wt df   logLik\n## 523   53.17 23.58       0.5166                           -28.59  5 -145.394\n## 779   42.55 25.02       0.5653                     11.94 -31.67  6 -145.015\n## 527   38.82 22.53 4.368 0.4560                           -27.27  6 -145.126\n## 539   90.56 24.30       0.5030 -8.159                    -30.75  6 -145.162\n## 139  176.50 16.79       0.2999              -8.193               5 -146.709\n## 587   48.91 22.98       0.4979        1.462              -31.20  6 -145.254\n##      AICc delta weight\n## 523 303.1  0.00  0.418\n## 779 305.4  2.29  0.133\n## 527 305.6  2.52  0.119\n## 539 305.7  2.59  0.115\n## 139 305.7  2.63  0.112\n## 587 305.9  2.77  0.104\n## Models ranked by AICc(x)\n\n# Wichtigkeit der Prädiktoren\nMuMIn::importance(allmodels)\n## Error: 'importance' is defunct.\n## Use 'sw' instead.\n## See help(\"Defunct\")\n\n# mittleres Model\navgmodel&lt;- MuMIn::model.avg(get.models(allmodels, subset=TRUE))\nsummary(avgmodel)\n## \n## Call:\n## model.avg(object = get.models(allmodels, subset = TRUE))\n## \n## Component model call: \n## lm(formula = hp ~ &lt;1024 unique rhs&gt;, data = cars)\n## \n## Component models: \n##                      df  logLik   AICc delta weight\n## 2+4+10                5 -145.39 303.10  0.00   0.15\n## 2+4+9+10              6 -145.02 305.39  2.29   0.05\n## 2+3+4+10              6 -145.13 305.61  2.52   0.04\n## 2+4+5+10              6 -145.16 305.68  2.59   0.04\n## 2+4+8                 5 -146.71 305.73  2.63   0.04\n## 2+4+7+10              6 -145.25 305.87  2.77   0.04\n## 2+4+8+10              6 -145.33 306.02  2.92   0.04\n## 1+2+4+10              6 -145.38 306.12  3.03   0.03\n## 2+4+6+10              6 -145.39 306.14  3.04   0.03\n## 2+4+8+9               6 -145.60 306.56  3.46   0.03\n## 2+3+4+9+10            7 -144.25 307.17  4.08   0.02\n## 2+4+8+9+10            7 -144.54 307.75  4.65   0.01\n## 2+4+5+9+10            7 -144.68 308.03  4.93   0.01\n## 2+4+7+9+10            7 -144.87 308.41  5.31   0.01\n## 2+4+6+8               6 -146.53 308.43  5.33   0.01\n## 2+3+4+6+10            7 -144.96 308.60  5.50   0.01\n## 2+3+4+8               6 -146.63 308.63  5.53   0.01\n## 2+3+4+7+10            7 -144.98 308.63  5.53   0.01\n## 2+4                   4 -149.59 308.65  5.56   0.01\n## 2+4+6+9+10            7 -145.01 308.70  5.60   0.01\n## 1+2+4+9+10            7 -145.01 308.70  5.60   0.01\n## 2+4+5+7+10            7 -145.03 308.73  5.63   0.01\n## 2+4+5+8               6 -146.69 308.74  5.64   0.01\n## 2+4+7+8               6 -146.69 308.75  5.65   0.01\n## 1+2+4+8               6 -146.70 308.76  5.66   0.01\n## 2+3+4+5+10            7 -145.05 308.77  5.67   0.01\n## 2+4+5+6+10            7 -145.10 308.86  5.76   0.01\n## 2+4+5+8+10            7 -145.11 308.88  5.79   0.01\n## 2+3+4+8+10            7 -145.12 308.90  5.81   0.01\n## 1+2+3+4+10            7 -145.12 308.91  5.81   0.01\n## 2+3+4+8+9             7 -145.15 308.96  5.87   0.01\n## 1+2+4+5+10            7 -145.16 308.98  5.89   0.01\n## 2+4+7+8+10            7 -145.20 309.08  5.98   0.01\n## 2+4+6+7+10            7 -145.24 309.14  6.05   0.01\n## 1+2+4+7+10            7 -145.24 309.15  6.05   0.01\n## 2+3+4+6               6 -146.89 309.15  6.05   0.01\n## 1+2+4+8+10            7 -145.29 309.25  6.16   0.01\n## 1+2+4                 5 -148.48 309.27  6.18   0.01\n## 2+4+6+8+10            7 -145.33 309.32  6.22   0.01\n## 1+2+4+6+10            7 -145.36 309.40  6.30   0.01\n## 2+4+6                 5 -148.66 309.62  6.52   0.01\n## 2+4+5+8+9             7 -145.49 309.64  6.55   0.01\n## 2+4+6+8+9             7 -145.56 309.78  6.69   0.01\n## 2+4+7+8+9             7 -145.59 309.84  6.74   0.01\n## 1+2+4+8+9             7 -145.59 309.85  6.76   0.01\n## 2+3+4+8+9+10          8 -143.92 310.10  7.00   0.00\n## 1+2+3+4               6 -147.37 310.10  7.01   0.00\n## 2+3+4                 5 -148.91 310.12  7.03   0.00\n## 2+3+4+6+9+10          8 -143.99 310.24  7.15   0.00\n## 1+2+3+4+9+10          8 -144.06 310.37  7.28   0.00\n## 2+3+4+7+9+10          8 -144.10 310.46  7.36   0.00\n## 2+4+5+8+9+10          8 -144.18 310.62  7.52   0.00\n## 2+3+4+5+9+10          8 -144.21 310.68  7.59   0.00\n## 2+4+7                 5 -149.20 310.71  7.62   0.00\n## 2+3+4+6+8             7 -146.13 310.92  7.83   0.00\n## 2+4+7+8+9+10          8 -144.45 311.15  8.06   0.00\n## 1+2+4+8+9+10          8 -144.51 311.28  8.18   0.00\n## 2+4+6+8+9+10          8 -144.53 311.32  8.23   0.00\n## 2+4+5+7+9+10          8 -144.55 311.36  8.27   0.00\n## 2+4+5                 5 -149.53 311.38  8.28   0.00\n## 2+4+9                 5 -149.55 311.41  8.31   0.00\n## 2+4+5+6+8             7 -146.41 311.49  8.39   0.00\n## 1+2+4+5+9+10          8 -144.63 311.53  8.43   0.00\n## 2+4+5+6+9+10          8 -144.65 311.55  8.46   0.00\n## 2+3+4+6+8+9           8 -144.67 311.61  8.51   0.00\n## 2+3+4+6+9             7 -146.49 311.65  8.56   0.00\n## 1+2+4+6+8             7 -146.51 311.69  8.59   0.00\n## 2+4+6+7+8             7 -146.53 311.73  8.64   0.00\n## 1+2+3+4+9             7 -146.54 311.75  8.65   0.00\n## 2+3+4+6+7+10          8 -144.75 311.75  8.66   0.00\n## 1+2+3+4+8             7 -146.56 311.78  8.68   0.00\n## 2+3+4+7+8             7 -146.62 311.90  8.80   0.00\n## 2+3+4+5+8             7 -146.63 311.93  8.84   0.00\n## 2+3+4+5+6+10          8 -144.85 311.96  8.87   0.00\n## 1+2+3+4+6             7 -146.65 311.97  8.88   0.00\n## 1+2+4+5+8             7 -146.66 311.98  8.88   0.00\n## 2+4+5+7+8             7 -146.67 312.00  8.90   0.00\n## 1+2+4+6               6 -148.32 312.00  8.90   0.00\n## 2+4+6+7+9+10          8 -144.87 312.00  8.91   0.00\n## 1+2+4+7+9+10          8 -144.87 312.00  8.91   0.00\n## 1+2+4+7+8             7 -146.68 312.03  8.94   0.00\n## 1+2+4+7               6 -148.34 312.04  8.94   0.00\n## 1+2+3+4+8+9           8 -144.90 312.06  8.97   0.00\n## 2+3+4+5+7+10          8 -144.91 312.09  8.99   0.00\n## 2+4+5+6+7+10          8 -144.93 312.12  9.03   0.00\n## 1+2+4+5               6 -148.40 312.16  9.07   0.00\n## 1+2+3+4+6+10          8 -144.96 312.18  9.08   0.00\n## 2+3+4+6+8+10          8 -144.96 312.19  9.09   0.00\n## 1+2+3+4+7+10          8 -144.98 312.21  9.12   0.00\n## 2+3+4+7+8+10          8 -144.98 312.22  9.12   0.00\n## 2+4+5+7+8+10          8 -144.99 312.24  9.15   0.00\n## 1+2+4+9               6 -148.46 312.27  9.18   0.00\n## 1+2+4+6+9+10          8 -145.01 312.29  9.19   0.00\n## 1+2+4+5+7+10          8 -145.03 312.32  9.22   0.00\n## 1+2+3+4+5+10          8 -145.03 312.32  9.23   0.00\n## 2+3+4+5+8+10          8 -145.04 312.33  9.24   0.00\n## 2+4+5+6+8+10          8 -145.05 312.35  9.26   0.00\n## 2+3+4+5+6             7 -146.88 312.44  9.34   0.00\n## 1+2+4+5+6+10          8 -145.09 312.45  9.35   0.00\n## 2+3+4+6+7             7 -146.89 312.45  9.36   0.00\n## 2+4+6+7               6 -148.56 312.47  9.38   0.00\n## 1+2+4+5+8+10          8 -145.11 312.48  9.38   0.00\n## 2+3+4+5               6 -148.56 312.49  9.39   0.00\n## 1+2+3+4+8+10          8 -145.12 312.49  9.40   0.00\n## 2+3+4+7+8+9           8 -145.13 312.53  9.43   0.00\n## 2+3+4+5+8+9           8 -145.15 312.56  9.46   0.00\n## 2+4+5+6               6 -148.60 312.56  9.46   0.00\n## 2+3+4+7               6 -148.60 312.56  9.46   0.00\n## 1+2+4+7+8+10          8 -145.17 312.60  9.50   0.00\n## 2+4+6+9               6 -148.63 312.63  9.53   0.00\n## 2+4+6+7+8+10          8 -145.19 312.64  9.55   0.00\n## 1+2+4+6+7+10          8 -145.19 312.65  9.55   0.00\n## 1+2+4+6+8+10          8 -145.26 312.78  9.69   0.00\n## 2+4+5+6+8+9           8 -145.36 312.98  9.88   0.00\n## 2+3+4+9               6 -148.86 313.09  9.99   0.00\n## 1+2+4+5+8+9           8 -145.43 313.13 10.03   0.00\n## 2+4+5+7+8+9           8 -145.46 313.19 10.09   0.00\n## 1+2+3+4+7             7 -147.31 313.29 10.20   0.00\n## 2+4+6+7+8+9           8 -145.55 313.37 10.27   0.00\n## 1+2+4+6+8+9           8 -145.56 313.37 10.28   0.00\n## 1+2+3+4+5             7 -147.37 313.40 10.30   0.00\n## 1+2+4+7+8+9           8 -145.58 313.42 10.33   0.00\n## 2+3+4+6+7+9+10        9 -143.74 313.65 10.56   0.00\n## 2+3+4+6+8+9+10        9 -143.76 313.69 10.60   0.00\n## 2+3                   4 -152.12 313.71 10.62   0.00\n## 2+4+5+7               6 -149.19 313.74 10.64   0.00\n## 2+4+7+9               6 -149.20 313.75 10.66   0.00\n## 2+3+4+7+8+9+10        9 -143.81 313.80 10.70   0.00\n## 2+3+4+5+8+9+10        9 -143.85 313.88 10.78   0.00\n## 1+2+3+4+8+9+10        9 -143.85 313.88 10.78   0.00\n## 1+2+3+4+6+9           8 -145.83 313.92 10.83   0.00\n## 2+3+4+5+6+9+10        9 -143.91 314.00 10.90   0.00\n## 1+2+3+4+7+9+10        9 -143.91 314.00 10.91   0.00\n## 1+2+3+4+6+9+10        9 -143.92 314.02 10.92   0.00\n## 3+6+7                 5 -150.88 314.06 10.96   0.00\n## 1+2+3+4+5+9+10        9 -143.96 314.11 11.01   0.00\n## 3+4+6                 5 -150.95 314.20 11.11   0.00\n## 2+3+4+5+7+9+10        9 -144.06 314.31 11.21   0.00\n## 2+4+5+9               6 -149.50 314.36 11.26   0.00\n## 2+4+5+7+8+9+10        9 -144.10 314.37 11.28   0.00\n## 2+3+4+5+6+8           8 -146.11 314.48 11.38   0.00\n## 2+3+4+6+7+8           8 -146.12 314.50 11.40   0.00\n## 2+4+5+6+8+9+10        9 -144.17 314.51 11.42   0.00\n## 1+2+3+4+6+8           8 -146.13 314.51 11.42   0.00\n## 1+2+4+5+8+9+10        9 -144.18 314.54 11.44   0.00\n## 2+3+6+7               6 -149.63 314.63 11.53   0.00\n## 2+3+8                 5 -151.22 314.75 11.66   0.00\n## 2+3+7+8               6 -149.78 314.92 11.82   0.00\n## 3+4+6+7               6 -149.78 314.92 11.82   0.00\n## 2+3+6                 5 -151.34 314.99 11.89   0.00\n## 1+2+4+5+6             7 -148.16 314.99 11.90   0.00\n## 1+2+4+7+8+9+10        9 -144.41 315.01 11.91   0.00\n## 2+4+6+7+8+9+10        9 -144.44 315.07 11.97   0.00\n## 1+2+4+5+6+8           8 -146.41 315.07 11.98   0.00\n## 2+4+5+6+7+8           8 -146.41 315.08 11.99   0.00\n## 1+2+4+6+7             7 -148.24 315.14 12.04   0.00\n## 1+2+4+5+7             7 -148.24 315.14 12.05   0.00\n## 2+4+5+6+7+9+10        9 -144.49 315.16 12.07   0.00\n## 2+3+4+5+6+9           8 -146.46 315.19 12.09   0.00\n## 1+2+4+6+8+9+10        9 -144.51 315.20 12.10   0.00\n## 1+2+4+5+7+9+10        9 -144.51 315.20 12.10   0.00\n## 1+2+3+4+7+9           8 -146.47 315.20 12.11   0.00\n## 2+3+4+6+7+9           8 -146.49 315.25 12.15   0.00\n## 1+2+4+7+9             7 -148.30 315.26 12.17   0.00\n## 1+2+4+6+7+8           8 -146.51 315.28 12.19   0.00\n## 1+2+4+6+9             7 -148.31 315.29 12.20   0.00\n## 1+2+3+4+5+9           8 -146.54 315.33 12.24   0.00\n## 1+2+3+4+7+8           8 -146.54 315.35 12.25   0.00\n## 1+2+3+4+5+8           8 -146.55 315.36 12.27   0.00\n## 1+2+4+5+9             7 -148.36 315.38 12.28   0.00\n## 1+2+3+4+6+8+9         9 -144.61 315.39 12.30   0.00\n## 2+3+4+5+7             7 -148.38 315.42 12.32   0.00\n## 1+2+4+5+6+9+10        9 -144.62 315.43 12.33   0.00\n## 2+3+7                 5 -151.57 315.44 12.35   0.00\n## 2+3+4+5+6+7+10        9 -144.63 315.45 12.36   0.00\n## 2+3+4+5+6+8+9         9 -144.65 315.49 12.39   0.00\n## 2+3+4+5+7+8           8 -146.62 315.49 12.40   0.00\n## 2+3+4+6+7+8+9         9 -144.67 315.51 12.42   0.00\n## 1+2+4+5+7+8           8 -146.64 315.53 12.44   0.00\n## 1+2+3+4+5+6           8 -146.65 315.56 12.47   0.00\n## 2+3+4+5+9             7 -148.45 315.56 12.47   0.00\n## 1+2+3+4+6+7           8 -146.65 315.56 12.47   0.00\n## 1+2+3+4+6+7+10        9 -144.73 315.63 12.54   0.00\n## 2+4+5+6+7             7 -148.50 315.66 12.56   0.00\n## 2+3+4+6+7+8+10        9 -144.74 315.66 12.57   0.00\n## 2+3+4+7+9             7 -148.51 315.69 12.60   0.00\n## 2+4+6+7+9             7 -148.55 315.76 12.67   0.00\n## 1+2+3                 5 -151.73 315.77 12.68   0.00\n## 2+4+5+6+9             7 -148.58 315.83 12.73   0.00\n## 1+2+3+4+5+6+10        9 -144.85 315.88 12.79   0.00\n## 2+3+4+5+6+8+10        9 -144.85 315.88 12.79   0.00\n## 1+2+4+6+7+9+10        9 -144.87 315.92 12.82   0.00\n## 1+2+3+4+5+8+9         9 -144.88 315.95 12.85   0.00\n## 3+6+7+8               6 -150.30 315.96 12.87   0.00\n## 1+2+3+4+7+8+9         9 -144.89 315.97 12.88   0.00\n## 2+4+5+6+7+8+10        9 -144.90 315.98 12.88   0.00\n## 1+2+3+4+5+7+10        9 -144.90 315.98 12.88   0.00\n## 2+3+4+5+7+8+10        9 -144.91 316.00 12.90   0.00\n## 1+2+4+5+6+7+10        9 -144.92 316.03 12.93   0.00\n## 2+3+4+5+6+7           8 -146.88 316.03 12.93   0.00\n## 1+2+3+4+6+8+10        9 -144.96 316.09 13.00   0.00\n## 2+3+8+10              6 -150.38 316.12 13.02   0.00\n## 1+2+3+4+7+8+10        9 -144.98 316.13 13.04   0.00\n## 1+2+4+5+7+8+10        9 -144.99 316.16 13.07   0.00\n## 1+2+3+4+5+8+10        9 -145.03 316.24 13.14   0.00\n## 1+2+4+5+6+8+10        9 -145.03 316.24 13.14   0.00\n## 3+4+6+8               6 -150.45 316.26 13.17   0.00\n## 2+3+7+8+9             7 -148.83 316.33 13.24   0.00\n## 1+2+4+6+7+8+10        9 -145.10 316.39 13.30   0.00\n## 2+3+8+9               6 -150.52 316.40 13.30   0.00\n## 2+3+9                 5 -152.07 316.44 13.34   0.00\n## 2+3+4+5+7+8+9         9 -145.13 316.45 13.35   0.00\n## 2+3+5                 5 -152.07 316.45 13.35   0.00\n## 2+3+10                5 -152.12 316.54 13.44   0.00\n## 1+2+3+7               6 -150.60 316.55 13.45   0.00\n## 3+6+7+9               6 -150.67 316.70 13.60   0.00\n## 3+4+6+9               6 -150.68 316.72 13.62   0.00\n## 2+3+6+7+8             7 -149.07 316.80 13.71   0.00\n## 3+4+6+7+8             7 -149.09 316.84 13.74   0.00\n## 1+2+3+4+5+7           8 -147.31 316.88 13.79   0.00\n## 1+2+4+5+6+8+9         9 -145.35 316.89 13.79   0.00\n## 2+4+5+6+7+8+9         9 -145.35 316.89 13.79   0.00\n## 1+2+4+5+7+8+9         9 -145.41 317.01 13.91   0.00\n## 2+4+5+7+9             7 -149.18 317.03 13.93   0.00\n## 3+5+6+7               6 -150.84 317.03 13.94   0.00\n## 1+3+6+7               6 -150.84 317.04 13.95   0.00\n## 3+6+7+10              6 -150.88 317.11 14.02   0.00\n## 2+8+10                5 -152.41 317.13 14.03   0.00\n## 3+4+5+6               6 -150.89 317.15 14.05   0.00\n## 3+4+6+10              6 -150.92 317.19 14.10   0.00\n## 1+3+4+6               6 -150.92 317.20 14.10   0.00\n## 2+3+8+9+10            7 -149.30 317.26 14.16   0.00\n## 2+3+6+10              6 -150.96 317.28 14.18   0.00\n## 1+2+4+6+7+8+9         9 -145.55 317.29 14.19   0.00\n## 2+3+7+10              6 -151.02 317.40 14.31   0.00\n## 2+3+6+7+9             7 -149.39 317.44 14.34   0.00\n## 2+3+6+8               6 -151.06 317.47 14.38   0.00\n## 2+3+6+9               6 -151.11 317.57 14.48   0.00\n## 3+6+7+8+9             7 -149.46 317.58 14.49   0.00\n## 2+3+4+6+7+8+9+10     10 -143.57 317.61 14.51   0.00\n## 3+4+6+7+9             7 -149.51 317.70 14.60   0.00\n## 3+4+6+8+9             7 -149.52 317.70 14.60   0.00\n## 3+6+8+10              6 -151.17 317.70 14.60   0.00\n## 2+3+5+8               6 -151.18 317.72 14.62   0.00\n## 3+4+6+7+10            7 -149.56 317.78 14.68   0.00\n## 2+3+4+5+6+8+9+10     10 -143.65 317.78 14.68   0.00\n## 1+2+3+8               6 -151.22 317.79 14.70   0.00\n## 2+3+4+5+6+7+9+10     10 -143.66 317.79 14.70   0.00\n## 1+2+3+6+7             7 -149.56 317.79 14.70   0.00\n## 2+3+6+7+10            7 -149.58 317.82 14.72   0.00\n## 1+2+3+4+5+6+9         9 -145.83 317.84 14.74   0.00\n## 1+2+3+4+6+7+9         9 -145.83 317.84 14.75   0.00\n## 1+2+3+4+6+7+9+10     10 -143.69 317.85 14.76   0.00\n## 3+6+10                5 -152.78 317.87 14.77   0.00\n## 2+3+5+6+7             7 -149.63 317.93 14.83   0.00\n## 1+2+3+4+7+8+9+10     10 -143.74 317.95 14.85   0.00\n## 1+2+3+4+6+8+9+10     10 -143.74 317.95 14.85   0.00\n## 1+2+3+4+5+8+9+10     10 -143.74 317.96 14.87   0.00\n## 2+3+4+5+7+8+9+10     10 -143.74 317.97 14.87   0.00\n## 2+3+5+6               6 -151.32 318.00 14.90   0.00\n## 1+2+3+6               6 -151.33 318.03 14.93   0.00\n## 4+6+7+8               6 -151.35 318.05 14.96   0.00\n## 1+3+4+6+7             7 -149.70 318.06 14.96   0.00\n## 3+4+5+6+7             7 -149.70 318.06 14.97   0.00\n## 2+3+5+7               6 -151.36 318.07 14.98   0.00\n## 1+2+3+4+5+6+9+10     10 -143.80 318.09 14.99   0.00\n## 1+2+3+4+5+7+9+10     10 -143.83 318.13 15.04   0.00\n## 2+3+7+8+10            7 -149.75 318.17 15.08   0.00\n## 1+2+3+9               6 -151.41 318.18 15.08   0.00\n## 1+2+3+7+8             7 -149.76 318.19 15.09   0.00\n## 2+3+5+7+8             7 -149.78 318.22 15.13   0.00\n## 4+6+8                 5 -152.98 318.28 15.18   0.00\n## 3+4+6+7+8+9           8 -148.01 318.29 15.19   0.00\n## 2+7+8                 5 -152.99 318.29 15.19   0.00\n## 1+2+3+10              6 -151.48 318.32 15.22   0.00\n## 3+6                   4 -154.45 318.38 15.29   0.00\n## 2+3+4+5+6+7+8         9 -146.10 318.38 15.29   0.00\n## 1+2+3+4+5+6+8         9 -146.10 318.39 15.30   0.00\n## 1+2+4+5+6+7           8 -148.07 318.41 15.31   0.00\n## 1+2+3+4+6+7+8         9 -146.12 318.42 15.32   0.00\n## 2+3+7+9               6 -151.55 318.45 15.36   0.00\n## 2+3+6+8+10            7 -149.91 318.48 15.38   0.00\n## 2+3+6+7+8+9           8 -148.14 318.54 15.45   0.00\n## 1+2+4+5+6+9           8 -148.15 318.55 15.46   0.00\n## 1+2+4+5+7+9           8 -148.17 318.60 15.50   0.00\n## 2+4+5+6+7+8+9+10     10 -144.07 318.61 15.51   0.00\n## 1+2+4+5+7+8+9+10     10 -144.09 318.66 15.57   0.00\n## 3+4+6+8+10            7 -150.00 318.67 15.58   0.00\n## 1+2+4+6+7+9           8 -148.22 318.70 15.61   0.00\n## 2+3+4+5+7+9           8 -148.23 318.72 15.62   0.00\n## 3+6+7+8+10            7 -150.04 318.74 15.64   0.00\n## 3+6+8+9+10            7 -150.04 318.75 15.66   0.00\n## 2+7+8+10              6 -151.71 318.78 15.69   0.00\n## 1+2+4+5+6+8+9+10     10 -144.17 318.81 15.71   0.00\n## 1+2+3+5               6 -151.73 318.82 15.72   0.00\n## 8+10                  4 -154.69 318.86 15.76   0.00\n## 1+2+4+5+6+7+8         9 -146.41 318.99 15.90   0.00\n## 1+2+3+7+9             7 -150.18 319.02 15.92   0.00\n## 2+5+7+8               6 -151.84 319.03 15.94   0.00\n## 4+6+8+10              6 -151.85 319.05 15.96   0.00\n## 2+3+4+5+6+7+9         9 -146.46 319.11 16.01   0.00\n## 1+2+3+4+5+7+9         9 -146.47 319.12 16.02   0.00\n## 1+3+6+7+8             7 -150.23 319.13 16.04   0.00\n## 2+5+8+10              6 -151.93 319.22 16.12   0.00\n## 1+2+7+8               6 -151.93 319.23 16.13   0.00\n## 2+4+5+6+7+9           8 -148.49 319.24 16.14   0.00\n## 1+2+3+4+5+7+8         9 -146.54 319.25 16.16   0.00\n## 3+5+6+7+8             7 -150.30 319.27 16.17   0.00\n## 1+2+4+6+7+8+9+10     10 -144.41 319.29 16.20   0.00\n## 2+3+5+9               6 -151.99 319.35 16.25   0.00\n## 1+2+3+8+10            7 -150.35 319.38 16.28   0.00\n## 2+3+5+8+10            7 -150.38 319.42 16.32   0.00\n## 2+3+6+8+9             7 -150.38 319.43 16.34   0.00\n## 1+2+4+5+6+7+9+10     10 -144.48 319.44 16.34   0.00\n## 1+3+4+6+8             7 -150.39 319.45 16.35   0.00\n## 2+3+5+10              6 -152.06 319.47 16.38   0.00\n## 1+2+3+4+5+6+7         9 -146.65 319.48 16.39   0.00\n## 2+3+9+10              6 -152.06 319.49 16.39   0.00\n## 1+2+8+10              6 -152.07 319.50 16.40   0.00\n## 3+4+5+6+8             7 -150.45 319.57 16.47   0.00\n## 2+3+5+8+9             7 -150.47 319.60 16.50   0.00\n## 1+2+3+4+5+6+8+9      10 -144.57 319.61 16.52   0.00\n## 1+2+3+4+6+7+8+9      10 -144.60 319.67 16.58   0.00\n## 1+3+4+6+9             7 -150.51 319.69 16.60   0.00\n## 1+3+6+7+9             7 -150.52 319.70 16.60   0.00\n## 1+2+3+8+9             7 -150.52 319.70 16.61   0.00\n## 2+3+7+8+9+10          8 -148.73 319.71 16.62   0.00\n## 2+8+9+10              6 -152.18 319.72 16.63   0.00\n## 1+2+3+7+10            7 -150.53 319.73 16.63   0.00\n## 1+2+3+4+5+6+7+10     10 -144.63 319.73 16.64   0.00\n## 2+3+4+5+6+7+8+10     10 -144.63 319.74 16.65   0.00\n## 1+2+3+7+8+9           8 -148.75 319.76 16.66   0.00\n## 4+7+8                 5 -153.73 319.76 16.67   0.00\n## 2+3+4+5+6+7+8+9      10 -144.65 319.77 16.67   0.00\n## 1+2+3+5+7             7 -150.59 319.84 16.75   0.00\n## 3+4+5+6+9             7 -150.59 319.84 16.75   0.00\n## 3+5+6+7+9             7 -150.60 319.87 16.78   0.00\n## 2+3+6+8+9+10          8 -148.81 319.87 16.78   0.00\n## 3+4+6+8+9+10          8 -148.83 319.92 16.83   0.00\n## 2+3+5+7+8+9           8 -148.83 319.93 16.83   0.00\n## 1+2+3+4+6+7+8+10     10 -144.73 319.93 16.83   0.00\n## 4+8                   4 -155.23 319.93 16.84   0.00\n## 3+6+7+9+10            7 -150.67 320.00 16.90   0.00\n## 3+4+6+9+10            7 -150.67 320.00 16.91   0.00\n## 3+7+8                 5 -153.85 320.02 16.92   0.00\n## 2+6+8+10              6 -152.35 320.06 16.97   0.00\n## 4+8+10                5 -153.91 320.13 17.03   0.00\n## 1+4+6+8               6 -152.38 320.13 17.03   0.00\n## 1+2+3+4+5+6+8+10     10 -144.85 320.17 17.08   0.00\n## 1+2+4+5+6+7+8+10     10 -144.87 320.21 17.12   0.00\n## 1+2+3+4+5+7+8+9      10 -144.87 320.22 17.13   0.00\n## 2+3+6+9+10            7 -150.78 320.23 17.13   0.00\n## 3+6+7+8+9+10          8 -148.99 320.23 17.14   0.00\n## 1+2+3+4+5+7+8+10     10 -144.90 320.27 17.18   0.00\n## 3+8+10                5 -153.99 320.29 17.19   0.00\n## 2+3+6+7+8+10          8 -149.02 320.30 17.20   0.00\n## 1+3+5+6+7             7 -150.82 320.30 17.20   0.00\n## 1+2+3+6+10            7 -150.82 320.31 17.22   0.00\n## 1+3+6+7+10            7 -150.83 320.33 17.23   0.00\n## 3+5+6+7+10            7 -150.83 320.33 17.24   0.00\n## 7+8+10                5 -154.02 320.34 17.24   0.00\n## 1+2+3+6+7+8           8 -149.04 320.34 17.24   0.00\n## 1+3+4+6+10            7 -150.84 320.35 17.26   0.00\n## 3+4+5+6+10            7 -150.84 320.35 17.26   0.00\n## 2+3+5+6+7+8           8 -149.05 320.36 17.26   0.00\n## 1+3+4+6+7+8           8 -149.05 320.36 17.27   0.00\n## 3+4+5+6+7+8           8 -149.08 320.42 17.32   0.00\n## 6+8+10                5 -154.06 320.42 17.33   0.00\n## 1+3+4+5+6             7 -150.88 320.43 17.33   0.00\n## 1+4+6+7+8             7 -150.88 320.43 17.34   0.00\n## 3+4+6+7+8+10          8 -149.09 320.43 17.34   0.00\n## 2+3+7+9+10            7 -150.93 320.52 17.42   0.00\n## 1+3+6+10              6 -152.58 320.53 17.43   0.00\n## 1+2+3+6+7+9           8 -149.14 320.55 17.45   0.00\n## 2+3+5+6+10            7 -150.96 320.58 17.48   0.00\n## 1+2+3+8+9+10          8 -149.16 320.58 17.49   0.00\n## 2+3+5+7+10            7 -150.97 320.60 17.51   0.00\n## 2+3+5+6+8             7 -150.97 320.61 17.51   0.00\n## 3+4+6+7+9+10          8 -149.19 320.64 17.55   0.00\n## 1+2+3+6+8             7 -150.99 320.65 17.56   0.00\n## 2+6+7+8               6 -152.67 320.70 17.61   0.00\n## 1+2+3+6+9             7 -151.03 320.72 17.62   0.00\n## 1+3+4+6+7+9           8 -149.23 320.73 17.63   0.00\n## 3+6+9+10              6 -152.69 320.74 17.64   0.00\n## 3+5+6+10              6 -152.69 320.74 17.65   0.00\n## 8+9+10                5 -154.23 320.76 17.67   0.00\n## 2+3+6+7+9+10          8 -149.29 320.84 17.74   0.00\n## 3+6+8                 5 -154.27 320.84 17.75   0.00\n## 2+3+5+8+9+10          8 -149.30 320.85 17.76   0.00\n## 2+3+5+6+9             7 -151.10 320.86 17.77   0.00\n## 4+5+6+8               6 -152.76 320.87 17.78   0.00\n## 4+6+7+8+9             7 -151.10 320.88 17.78   0.00\n## 3+6+9                 5 -154.29 320.88 17.78   0.00\n## 1+2+3+9+10            7 -151.11 320.90 17.80   0.00\n## 7+8                   4 -155.72 320.92 17.82   0.00\n## 1+3+6+8+10            7 -151.16 320.98 17.88   0.00\n## 3+5+6+8+10            7 -151.16 321.00 17.90   0.00\n## 4+5+6+7+8             7 -151.17 321.00 17.91   0.00\n## 2+3+5+6+7+9           8 -149.37 321.01 17.91   0.00\n## 4+6+8+9               6 -152.83 321.01 17.92   0.00\n## 4+6+7+8+10            7 -151.17 321.02 17.92   0.00\n## 1+2+3+5+8             7 -151.18 321.02 17.92   0.00\n## 3+4+5+6+7+9           8 -149.39 321.04 17.95   0.00\n## 3+8+9+10              6 -152.87 321.10 18.00   0.00\n## 1+3+6+7+8+9           8 -149.45 321.15 18.06   0.00\n## 1+3+6                 5 -154.43 321.17 18.07   0.00\n## 1+2+4+5+6+7+8+9      10 -145.35 321.17 18.07   0.00\n## 3+5+6+7+8+9           8 -149.46 321.18 18.08   0.00\n## 3+5+6                 5 -154.45 321.21 18.11   0.00\n## 2+5+7+8+10            7 -151.28 321.22 18.13   0.00\n## 1+2+3+5+6             7 -151.30 321.27 18.18   0.00\n## 1+3+4+6+8+9           8 -149.51 321.28 18.18   0.00\n## 2+3+5+7+9             7 -151.31 321.28 18.18   0.00\n## 3+4+5+6+8+9           8 -149.51 321.28 18.19   0.00\n## 3+4+5+6+7+10          8 -149.52 321.30 18.20   0.00\n## 2+7+8+9               6 -152.99 321.34 18.24   0.00\n## 1+2+3+6+7+10          8 -149.54 321.35 18.25   0.00\n## 1+3+4+6+7+10          8 -149.55 321.35 18.26   0.00\n## 1+2+3+5+6+7           8 -149.56 321.39 18.29   0.00\n## 2+3+5+6+7+10          8 -149.58 321.41 18.32   0.00\n## 1+2+7+8+10            7 -151.40 321.46 18.36   0.00\n## 3+7+8+9               6 -153.06 321.47 18.38   0.00\n## 1+2+3+5+9             7 -151.40 321.47 18.38   0.00\n## 4+6+8+9+10            7 -151.42 321.51 18.42   0.00\n## 1+3+4+5+6+7           8 -149.65 321.56 18.46   0.00\n## 1+2+3+5+10            7 -151.48 321.62 18.52   0.00\n## 2+7+8+9+10            7 -151.50 321.67 18.57   0.00\n## 1+8+10                5 -154.69 321.68 18.58   0.00\n## 5+8+10                5 -154.69 321.68 18.59   0.00\n## 4+8+9+10              6 -153.16 321.68 18.59   0.00\n## 6+7+8+10              6 -153.17 321.70 18.60   0.00\n## 1+2+5+7+8             7 -151.52 321.71 18.61   0.00\n## 1+2+3+7+8+10          8 -149.73 321.72 18.62   0.00\n## 2+3+5+7+8+10          8 -149.75 321.77 18.67   0.00\n## 1+2+3+5+7+8           8 -149.76 321.78 18.68   0.00\n## 1+4+6+8+10            7 -151.56 321.79 18.70   0.00\n## 3+4+8                 5 -154.75 321.81 18.72   0.00\n## 2+5+8+9+10            7 -151.58 321.83 18.74   0.00\n## 4+7+8+9               6 -153.24 321.84 18.74   0.00\n## 4+8+9                 5 -154.86 322.03 18.94   0.00\n## 2+3+5+6+8+10          8 -149.89 322.04 18.95   0.00\n## 1+2+3+6+8+10          8 -149.90 322.06 18.96   0.00\n## 2+6+7+8+10            7 -151.70 322.07 18.98   0.00\n## 3+7+8+10              6 -153.37 322.09 19.00   0.00\n## 3+4+7+8               6 -153.38 322.13 19.03   0.00\n## 1+2+3+4+5+6+7+9      10 -145.83 322.13 19.04   0.00\n## 2+3+4+5+6+7+8+9+10   11 -143.47 322.14 19.04   0.00\n## 4+5+6+8+10            7 -151.74 322.14 19.05   0.00\n## 2+3+6+7+8+9+10        9 -147.98 322.14 19.05   0.00\n## 3+4+6+7+8+9+10        9 -147.99 322.16 19.06   0.00\n## 2+5+7+8+9             7 -151.75 322.16 19.06   0.00\n## 3+4+5+6+7+8+9         9 -148.01 322.19 19.10   0.00\n## 4+7+8+10              6 -153.42 322.20 19.11   0.00\n## 1+3+4+6+7+8+9         9 -148.01 322.21 19.11   0.00\n## 1+3+4+6+8+10          8 -149.98 322.23 19.13   0.00\n## 3+4+5+6+8+10          8 -149.99 322.25 19.15   0.00\n## 1+2+4+5+6+7+9         9 -148.04 322.26 19.17   0.00\n## 1+3+6+7+8+10          8 -150.01 322.27 19.18   0.00\n## 1+2+5+8+10            7 -151.81 322.29 19.19   0.00\n## 1+2+3+4+6+7+8+9+10   11 -143.55 322.31 19.21   0.00\n## 1+3+6+8+9+10          8 -150.03 322.32 19.23   0.00\n## 3+5+6+7+8+10          8 -150.03 322.33 19.23   0.00\n## 1+2+8+9+10            7 -151.83 322.33 19.24   0.00\n## 3+5+6+8+9+10          8 -150.04 322.33 19.24   0.00\n## 2+5+6+7+8             7 -151.83 322.34 19.24   0.00\n## 1+2+3+4+5+6+7+9+10   11 -143.59 322.37 19.28   0.00\n## 2+3+5+6+7+8+9         9 -148.12 322.42 19.32   0.00\n## 1+2+3+4+5+6+8+9+10   11 -143.62 322.43 19.34   0.00\n## 1+2+3+6+7+8+9         9 -148.14 322.46 19.37   0.00\n## 1+2+7+8+9             7 -151.90 322.47 19.37   0.00\n## 1+2+3+4+5+7+8+9+10   11 -143.64 322.48 19.39   0.00\n## 1+2+3+7+9+10          8 -150.12 322.50 19.40   0.00\n## 3+5+7+8               6 -153.57 322.50 19.40   0.00\n## 7+8+9+10              6 -153.58 322.52 19.42   0.00\n## 1+2+6+7+8             7 -151.93 322.52 19.42   0.00\n## 2+5+6+8+10            7 -151.93 322.52 19.43   0.00\n## 1+2+3+5+7+9           8 -150.17 322.59 19.50   0.00\n## 2+3+5+9+10            7 -151.99 322.65 19.55   0.00\n## 1+3+7+8               6 -153.65 322.67 19.57   0.00\n## 4+5+7+8               6 -153.65 322.67 19.57   0.00\n## 1+2+3+4+5+6+7+8      10 -146.10 322.67 19.58   0.00\n## 4+5+8                 5 -155.18 322.68 19.58   0.00\n## 1+6+8+10              6 -153.68 322.71 19.62   0.00\n## 1+3+5+6+7+8           8 -150.23 322.72 19.62   0.00\n## 3+5+8+10              6 -153.69 322.74 19.64   0.00\n## 1+4+8                 5 -155.22 322.75 19.65   0.00\n## 1+4+7+8               6 -153.69 322.75 19.65   0.00\n## 3+4+8+10              6 -153.70 322.76 19.66   0.00\n## 1+3+8+10              6 -153.70 322.77 19.67   0.00\n## 1+2+6+8+10            7 -152.06 322.79 19.70   0.00\n## 2+6+8+9+10            7 -152.07 322.80 19.70   0.00\n## 2+3+5+6+8+9           8 -150.29 322.84 19.75   0.00\n## 6+8+9+10              6 -153.80 322.96 19.86   0.00\n## 4+5+8+10              6 -153.80 322.96 19.87   0.00\n## 1+2+3+5+8+10          8 -150.35 322.96 19.87   0.00\n## 3+6+8+9               6 -153.81 322.98 19.89   0.00\n## 5+6+8+10              6 -153.82 323.00 19.90   0.00\n## 1+2+3+6+8+9           8 -150.37 323.00 19.90   0.00\n## 1+4+8+10              6 -153.82 323.00 19.91   0.00\n## 1+3+4+5+6+8           8 -150.38 323.02 19.93   0.00\n## 4+6                   4 -156.81 323.11 20.01   0.00\n## 1+3+4+6+9+10          8 -150.43 323.12 20.03   0.00\n## 1+7+8                 5 -155.41 323.14 20.04   0.00\n## 1+4+6+8+9             7 -152.25 323.18 20.08   0.00\n## 1+2+3+5+8+9           8 -150.46 323.18 20.08   0.00\n## 1+2+3+6+9+10          8 -150.46 323.19 20.09   0.00\n## 6+7+8                 5 -155.44 323.19 20.09   0.00\n## 1+3+4+5+6+9           8 -150.47 323.20 20.10   0.00\n## 1+3+5+6+7+9           8 -150.49 323.24 20.14   0.00\n## 3+4+8+9               6 -153.94 323.25 20.15   0.00\n## 1+3+6+7+9+10          8 -150.50 323.26 20.17   0.00\n## 1+4+5+6+8             7 -152.30 323.26 20.17   0.00\n## 1+3+8+9+10            7 -152.30 323.27 20.18   0.00\n## 3+7+8+9+10            7 -152.31 323.28 20.18   0.00\n## 1+3+6+9+10            7 -152.33 323.32 20.22   0.00\n## 1+2+3+5+7+10          8 -150.53 323.32 20.22   0.00\n## 1+2+4+5+6+7+8+9+10   11 -144.06 323.33 20.23   0.00\n## 1+2+3+7+8+9+10        9 -148.59 323.36 20.27   0.00\n## 5+7+8                 5 -155.53 323.38 20.28   0.00\n## 5+7+8+10              6 -154.01 323.39 20.29   0.00\n## 3+4+5+6+9+10          8 -150.56 323.39 20.29   0.00\n## 1+7+8+10              6 -154.02 323.39 20.29   0.00\n## 1+3+7                 5 -155.55 323.41 20.32   0.00\n## 4+6+7                 5 -155.56 323.44 20.34   0.00\n## 3+5+6+7+9+10          8 -150.60 323.47 20.37   0.00\n## 1+3+6+8               6 -154.06 323.47 20.38   0.00\n## 3+8                   4 -157.03 323.55 20.45   0.00\n## 3+4+7+8+9             7 -152.46 323.59 20.49   0.00\n## 1+6+7+8               6 -154.12 323.60 20.50   0.00\n## 1+4+6+7+8+9           8 -150.67 323.61 20.51   0.00\n## 2+3+5+7+8+9+10        9 -148.73 323.63 20.54   0.00\n## 1+2+3+5+7+8+9         9 -148.74 323.66 20.57   0.00\n## 7+8+9                 5 -155.70 323.72 20.62   0.00\n## 1+3+5+6+10            7 -152.54 323.75 20.65   0.00\n## 1+2+3+6+8+9+10        9 -148.79 323.76 20.66   0.00\n## 3+5+8+9+10            7 -152.55 323.77 20.67   0.00\n## 2+3+5+6+8+9+10        9 -148.80 323.77 20.68   0.00\n## 4+5+6+8+9             7 -152.55 323.78 20.68   0.00\n## 5+8+9+10              6 -154.21 323.78 20.69   0.00\n## 3+4+8+9+10            7 -152.56 323.79 20.69   0.00\n## 1+8+9+10              6 -154.22 323.81 20.71   0.00\n## 2+3+5+6+9+10          8 -150.78 323.81 20.72   0.00\n## 3+4+5+6+8+9+10        9 -148.82 323.82 20.72   0.00\n## 1+2+8                 5 -155.75 323.82 20.72   0.00\n## 3+5+6+9+10            7 -152.58 323.82 20.73   0.00\n## 1+3+4+6+8+9+10        9 -148.82 323.83 20.73   0.00\n## 1+3+4+5+6+10          8 -150.80 323.85 20.76   0.00\n## 1+3+5+6+7+10          8 -150.80 323.87 20.77   0.00\n## 3+5+6+8               6 -154.26 323.87 20.78   0.00\n## 1+4+5+6+7+8           8 -150.81 323.89 20.79   0.00\n## 4+6+7+8+9+10          8 -150.82 323.89 20.80   0.00\n## 1+4+6+7+8+10          8 -150.82 323.89 20.80   0.00\n## 1+2+3+5+6+10          8 -150.82 323.90 20.81   0.00\n## 3+5+6+9               6 -154.28 323.93 20.83   0.00\n## 1+3+6+9               6 -154.29 323.93 20.84   0.00\n## 2+6+7+8+9             7 -152.64 323.95 20.86   0.00\n## 2+3+5+7+9+10          8 -150.85 323.97 20.87   0.00\n## 2+5+8                 5 -155.84 324.00 20.90   0.00\n## 4+5+6+7+8+9           8 -150.88 324.02 20.92   0.00\n## 1+2+5+8               6 -154.34 324.05 20.95   0.00\n## 1+3+7+8+9             7 -152.69 324.05 20.96   0.00\n## 1+6+7+8+10            7 -152.70 324.06 20.96   0.00\n## 4+6+9                 5 -155.90 324.11 21.01   0.00\n## 1+2+3+5+6+8           8 -150.93 324.12 21.02   0.00\n## 4+7+8+9+10            7 -152.73 324.13 21.04   0.00\n## 3+5+6+7+8+9+10        9 -148.98 324.14 21.05   0.00\n## 1+3+6+7+8+9+10        9 -148.98 324.15 21.06   0.00\n## 3+4+5+8               6 -154.40 324.15 21.06   0.00\n## 2+5+7+8+9+10          8 -150.96 324.18 21.08   0.00\n## 1+2+3+6+7+8+10        9 -149.00 324.18 21.08   0.00\n## 2+3+5+6+7+8+10        9 -149.00 324.19 21.09   0.00\n## 3+5+7+8+9             7 -152.77 324.20 21.11   0.00\n## 1+3+5+6               6 -154.43 324.21 21.12   0.00\n## 1+2+3+5+6+7+8         9 -149.03 324.23 21.14   0.00\n## 1+2+3+5+6+9           8 -151.00 324.25 21.16   0.00\n## 1+3+4+5+6+7+8         9 -149.04 324.26 21.16   0.00\n## 1+3+4+6+7+8+10        9 -149.05 324.28 21.19   0.00\n## 1+2+3+4+5+6+7+8+9    11 -144.56 324.33 21.23   0.00\n## 4+5+6+7+8+10          8 -151.04 324.34 21.24   0.00\n## 3+4+5+6+7+8+10        9 -149.08 324.34 21.25   0.00\n## 1+3+4+6+7+9+10        9 -149.08 324.34 21.25   0.00\n## 2+7+9                 5 -156.03 324.36 21.27   0.00\n## 1+3+4+8               6 -154.51 324.38 21.29   0.00\n## 3+4+5+6+7+9+10        9 -149.13 324.43 21.34   0.00\n## 1+2+3+6+7+9+10        9 -149.13 324.44 21.34   0.00\n## 1+2+3+4+5+6+7+8+10   11 -144.63 324.46 21.36   0.00\n## 5+6+7+8+10            7 -152.90 324.46 21.36   0.00\n## 1+2+3+5+6+7+9         9 -149.14 324.47 21.37   0.00\n## 1+2+3+5+9+10          8 -151.11 324.49 21.39   0.00\n## 1+2+3+5+8+9+10        9 -149.16 324.49 21.40   0.00\n## 1+3+4+5+6+7+9         9 -149.18 324.54 21.44   0.00\n## 1+3+5+6+8+10          8 -151.15 324.56 21.46   0.00\n## 6+7+8+9+10            7 -152.96 324.59 21.49   0.00\n## 1+2+5+7+8+10          8 -151.17 324.59 21.50   0.00\n## 1+2+7+8+9+10          8 -151.18 324.61 21.52   0.00\n## 5+6+7+8               6 -154.65 324.67 21.57   0.00\n## 3+4+5+7+8             7 -153.00 324.67 21.58   0.00\n## 1+4+6+8+9+10          8 -151.21 324.67 21.58   0.00\n## 4+6+7+9               6 -154.66 324.68 21.59   0.00\n## 1+5+8+10              6 -154.69 324.73 21.64   0.00\n## 3+5+7+8+10            7 -153.04 324.75 21.65   0.00\n## 2+3+5+6+7+9+10        9 -149.29 324.76 21.66   0.00\n## 2+5+6+7+8+10          8 -151.25 324.77 21.67   0.00\n## 1+4+8+9+10            7 -153.07 324.81 21.71   0.00\n## 1+3+7+8+10            7 -153.07 324.81 21.71   0.00\n## 4+5+6+8+9+10          8 -151.28 324.82 21.73   0.00\n## 1+3+4+7+8             7 -153.08 324.83 21.74   0.00\n## 4+5+8+9+10            7 -153.12 324.90 21.80   0.00\n## 1+2+6+7+8+10          8 -151.34 324.94 21.84   0.00\n## 2+5+6+8               6 -154.81 324.97 21.88   0.00\n## 1+3+4                 5 -156.36 325.03 21.93   0.00\n## 3+4+7+8+10            7 -153.18 325.03 21.94   0.00\n## 4+5+8+9               6 -154.85 325.06 21.96   0.00\n## 1+3+5+6+7+8+9         9 -149.45 325.07 21.98   0.00\n## 1+2+5+7+8+9           8 -151.41 325.08 21.98   0.00\n## 1+4+8+9               6 -154.86 325.08 21.98   0.00\n## 4+5+7+8+9             7 -153.21 325.09 21.99   0.00\n## 1+4+7+8+9             7 -153.21 325.09 21.99   0.00\n## 1+2+5+6+7+8           8 -151.45 325.17 22.07   0.00\n## 1+3+4+5+6+8+9         9 -149.50 325.19 22.09   0.00\n## 2+6+7+8+9+10          8 -151.46 325.19 22.10   0.00\n## 1+3+4+5+6+7+10        9 -149.51 325.21 22.11   0.00\n## 1+2+5+8+9+10          8 -151.48 325.23 22.13   0.00\n## 1+2+3+5+6+7+10        9 -149.54 325.27 22.17   0.00\n## 1+4+5+6+8+10          8 -151.52 325.30 22.20   0.00\n## 4+5+7+8+10            7 -153.32 325.30 22.20   0.00\n## 3+4+5+8+10            7 -153.33 325.33 22.23   0.00\n## 2+6+8                 5 -156.51 325.34 22.24   0.00\n## 4+6+7+10              6 -154.99 325.34 22.24   0.00\n## 1+4+7+8+10            7 -153.34 325.35 22.26   0.00\n## 1+3+4+8+10            7 -153.35 325.37 22.28   0.00\n## 2+5+6+8+9+10          8 -151.58 325.42 22.32   0.00\n## 3+8+9                 5 -156.56 325.44 22.34   0.00\n## 1+3+5+7               6 -155.06 325.48 22.38   0.00\n## 2+7                   4 -158.02 325.51 22.42   0.00\n## 1+3+5+7+8             7 -153.46 325.59 22.49   0.00\n## 1+3+4+7               6 -155.12 325.60 22.51   0.00\n## 1+6+8+9+10            7 -153.47 325.61 22.52   0.00\n## 1+2+3+5+7+8+10        9 -149.73 325.64 22.54   0.00\n## 1+3+4+8+9             7 -153.51 325.68 22.59   0.00\n## 4+5+6                 5 -156.69 325.68 22.59   0.00\n## 1+2+6+8               6 -155.16 325.69 22.59   0.00\n## 5+6+8+9+10            7 -153.52 325.70 22.60   0.00\n## 1+3+5+8+10            7 -153.52 325.70 22.60   0.00\n## 2+5+6+7+8+9           8 -151.73 325.72 22.63   0.00\n## 1+4+5+8               6 -155.18 325.73 22.63   0.00\n## 1+3+7+8+9+10          8 -151.74 325.74 22.64   0.00\n## 1+5+6+8+10            7 -153.55 325.77 22.67   0.00\n## 5+7+8+9+10            7 -153.57 325.81 22.71   0.00\n## 3+4+5+8+9             7 -153.57 325.81 22.72   0.00\n## 1+2+5+6+8+10          8 -151.78 325.81 22.72   0.00\n## 1+7+8+9+10            7 -153.58 325.82 22.73   0.00\n## 1+4+6                 5 -156.78 325.86 22.77   0.00\n## 1+2+6+8+9+10          8 -151.83 325.92 22.83   0.00\n## 4+6+10                5 -156.81 325.93 22.83   0.00\n## 1+3+10                5 -156.81 325.93 22.83   0.00\n## 1+2+3+5+6+8+10        9 -149.89 325.96 22.86   0.00\n## 1+4+5+7+8             7 -153.65 325.96 22.87   0.00\n## 1+3+6+8+9             7 -153.68 326.03 22.94   0.00\n## 1+3+7+9               6 -155.34 326.04 22.95   0.00\n## 1+2+6+7+8+9           8 -151.90 326.06 22.96   0.00\n## 1+3+4+8+9+10          8 -151.90 326.07 22.97   0.00\n## 1+7+8+9               6 -155.36 326.08 22.98   0.00\n## 3+5+8                 5 -156.89 326.10 23.00   0.00\n## 1+3+4+7+8+9           8 -151.92 326.11 23.01   0.00\n## 1+3+4+5+6+8+10        9 -149.97 326.12 23.02   0.00\n## 1+5+7+8               6 -155.39 326.14 23.05   0.00\n## 1+3+5+6+7+8+10        9 -150.00 326.18 23.09   0.00\n## 3+5+7+8+9+10          8 -151.96 326.19 23.09   0.00\n## 1+4+5+8+10            7 -153.77 326.21 23.11   0.00\n## 6+7+8+9               6 -155.44 326.23 23.14   0.00\n## 1+3+5+6+8+9+10        9 -150.03 326.24 23.14   0.00\n## 1+5+6+7+8             7 -153.79 326.24 23.14   0.00\n## 3+5+6+8+9             7 -153.80 326.27 23.17   0.00\n## 1+3+8                 5 -156.99 326.29 23.20   0.00\n## 1+4+6+7               6 -155.47 326.30 23.21   0.00\n## 1+3                   4 -158.41 326.30 23.21   0.00\n## 5+7+8+9               6 -155.47 326.30 23.21   0.00\n## 4+5+6+7               6 -155.48 326.31 23.22   0.00\n## 3+4+5+7+8+9           8 -152.06 326.38 23.29   0.00\n## 1+2+3+5+7+9+10        9 -150.12 326.41 23.32   0.00\n## 1+3+7+10              6 -155.53 326.42 23.32   0.00\n## 2+3+5+6+7+8+9+10     10 -147.97 326.42 23.32   0.00\n## 1+2+3+6+7+8+9+10     10 -147.98 326.43 23.33   0.00\n## 3+4+5+6+7+8+9+10     10 -147.98 326.43 23.33   0.00\n## 1+3+4+6+7+8+9+10     10 -147.99 326.45 23.35   0.00\n## 3+4+7+8+9+10          8 -152.10 326.47 23.37   0.00\n## 1+2+8+9               6 -155.55 326.47 23.37   0.00\n## 1+3+4+5+6+7+8+9      10 -148.00 326.49 23.39   0.00\n## 1+4+5+6+8+9           8 -152.14 326.54 23.45   0.00\n## 2+5+7+9               6 -155.59 326.55 23.45   0.00\n## 1+3+5+8+9+10          8 -152.15 326.56 23.46   0.00\n## 3+4+5+8+9+10          8 -152.17 326.60 23.50   0.00\n## 1+5+7+8+10            7 -154.01 326.69 23.60   0.00\n## 2+5+7                 5 -157.19 326.69 23.60   0.00\n## 1+2+3+5+6+7+8+9      10 -148.12 326.71 23.62   0.00\n## 1+2+3+5+6+8+9         9 -150.29 326.75 23.66   0.00\n## 1+3+5+6+8             7 -154.05 326.78 23.68   0.00\n## 1+3+5+6+9+10          8 -152.27 326.81 23.71   0.00\n## 2+5+8+9               6 -155.76 326.89 23.79   0.00\n## 1+6+7+8+9             7 -154.12 326.90 23.80   0.00\n## 3+5+7                 5 -157.30 326.91 23.82   0.00\n## 1+3+4+5+6+9+10        9 -150.38 326.93 23.84   0.00\n## 1+3+4+5               6 -155.80 326.95 23.86   0.00\n## 4+5+6+9               6 -155.80 326.96 23.86   0.00\n## 1+2+5+6+8             7 -154.20 327.06 23.97   0.00\n## 1+5+8+9+10            7 -154.21 327.09 23.99   0.00\n## 1+2+3+5+6+9+10        9 -150.46 327.11 24.01   0.00\n## 1+3+5+6+7+9+10        9 -150.47 327.12 24.02   0.00\n## 1+4+6+9               6 -155.88 327.13 24.03   0.00\n## 4+6+9+10              6 -155.89 327.13 24.04   0.00\n## 1+3+4+5+8             7 -154.27 327.20 24.10   0.00\n## 1+3+5+6+9             7 -154.28 327.23 24.14   0.00\n## 1+4+6+7+8+9+10        9 -150.53 327.24 24.14   0.00\n## 1+2+3+4+5+6+7+8+9+10 12 -143.44 327.31 24.21   0.00\n## 1+4+5+6+7+8+9         9 -150.57 327.33 24.23   0.00\n## 1+3+5+7+8+9           8 -152.53 327.33 24.23   0.00\n## 4+6+7+9+10            7 -154.33 327.33 24.24   0.00\n## 1+2+5+8+9             7 -154.33 327.34 24.24   0.00\n## 1+6+7+8+9+10          8 -152.55 327.36 24.26   0.00\n## 1+5+6+7+8+10          8 -152.55 327.37 24.27   0.00\n## 1+2+7+9               6 -156.01 327.38 24.28   0.00\n## 2+6+7+9               6 -156.03 327.41 24.32   0.00\n## 2+7+9+10              6 -156.03 327.42 24.32   0.00\n## 4+5+6+7+8+9+10        9 -150.65 327.48 24.38   0.00\n## 5+6+7+8+9+10          8 -152.64 327.55 24.46   0.00\n## 1+4+7+8+9+10          8 -152.65 327.56 24.46   0.00\n## 1+3+4+10              6 -156.11 327.59 24.49   0.00\n## 1+3+4+9               6 -156.13 327.62 24.52   0.00\n## 4+5+7+8+9+10          8 -152.69 327.63 24.54   0.00\n## 1+2+3+5+7+8+9+10     10 -148.59 327.65 24.56   0.00\n## 1+4+5+6+7+8+10        9 -150.76 327.70 24.60   0.00\n## 1+3+4+5+7             7 -154.53 327.73 24.63   0.00\n## 4+5+6+7+9             7 -154.59 327.85 24.76   0.00\n## 3+4+5+7+8+10          8 -152.80 327.86 24.77   0.00\n## 1+2+5+7+8+9+10        9 -150.86 327.91 24.81   0.00\n## 1+3+5+10              6 -156.28 327.91 24.81   0.00\n## 1+3+4+5+7+8           8 -152.83 327.92 24.83   0.00\n## 1+3+4+7+8+10          8 -152.84 327.94 24.85   0.00\n## 5+6+7+8+9             7 -154.65 327.96 24.87   0.00\n## 1+4+6+7+9             7 -154.66 327.99 24.90   0.00\n## 1+3+5+7+8+10          8 -152.87 327.99 24.90   0.00\n## 1+2+3+5+6+8+9+10     10 -148.77 328.02 24.92   0.00\n## 2+9+10                5 -157.86 328.02 24.93   0.00\n## 2+5+6+7+8+9+10        9 -150.95 328.09 24.99   0.00\n## 1+3+4+5+6+8+9+10     10 -148.81 328.10 25.01   0.00\n## 3+5                   4 -159.35 328.18 25.08   0.00\n## 3+5+8+9               6 -156.43 328.22 25.12   0.00\n## 1+2+5+6+7+8+10        9 -151.04 328.27 25.17   0.00\n## 2+5+6+8+9             7 -154.80 328.27 25.18   0.00\n## 2+6+7                 5 -157.99 328.28 25.18   0.00\n## 4+5+6+7+10            7 -154.81 328.29 25.20   0.00\n## 1+3+8+9               6 -156.47 328.31 25.21   0.00\n## 2+6+8+9               6 -156.47 328.31 25.21   0.00\n## 1+3+5+7+9             7 -154.83 328.32 25.22   0.00\n## 1+2+7                 5 -158.01 328.34 25.24   0.00\n## 2+7+10                5 -158.02 328.34 25.24   0.00\n## 1+4+5+8+9             7 -154.85 328.36 25.27   0.00\n## 1+3+4+7+9             7 -154.85 328.37 25.28   0.00\n## 2+5+9                 5 -158.03 328.38 25.28   0.00\n## 1+4+5+8+9+10          8 -153.06 328.38 25.28   0.00\n## 3+4+5                 5 -158.05 328.41 25.31   0.00\n## 1+3+5+6+7+8+9+10     10 -148.98 328.44 25.34   0.00\n## 1+4+5+6+8+9+10        9 -151.13 328.44 25.35   0.00\n## 1+2+3+5+6+7+8+10     10 -148.99 328.46 25.36   0.00\n## 1+3+5                 5 -158.08 328.47 25.38   0.00\n## 1+2+6+7+8+9+10        9 -151.16 328.50 25.40   0.00\n## 1+3+4+5+8+10          8 -153.12 328.50 25.40   0.00\n## 1+3+4+5+6+7+8+10     10 -149.04 328.55 25.46   0.00\n## 1+3+4+5+6+7+9+10     10 -149.04 328.56 25.46   0.00\n## 1+4+5+6               6 -156.61 328.57 25.48   0.00\n## 1+3+9+10              6 -156.64 328.63 25.54   0.00\n## 1+4+6+7+10            7 -154.99 328.64 25.55   0.00\n## 1+4+5+7+8+9           8 -153.20 328.66 25.56   0.00\n## 1+3+5+7+10            7 -155.01 328.68 25.59   0.00\n## 4+5+6+10              6 -156.68 328.71 25.62   0.00\n## 1+2+3+5+6+7+9+10     10 -149.13 328.73 25.63   0.00\n## 1+3+4+5+8+9           8 -153.29 328.84 25.74   0.00\n## 1+4+5+7+8+10          8 -153.29 328.84 25.75   0.00\n## 1+5+6+8+9+10          8 -153.31 328.88 25.78   0.00\n## 1+3+4+7+10            7 -155.11 328.88 25.78   0.00\n## 1+4+6+10              6 -156.78 328.91 25.82   0.00\n## 1+2+5+6+7+8+9         9 -151.37 328.93 25.83   0.00\n## 1+2+6+8+9             7 -155.13 328.93 25.84   0.00\n## 1+3+9                 5 -158.32 328.94 25.85   0.00\n## 1+3+4+7+8+9+10        9 -151.46 329.10 26.01   0.00\n## 1+3+5+8               6 -156.88 329.12 26.03   0.00\n## 1+2+5+7               6 -156.88 329.13 26.03   0.00\n## 1+2+5+6+8+9+10        9 -151.48 329.13 26.04   0.00\n## 1+3+7+9+10            7 -155.31 329.29 26.19   0.00\n## 1+5+7+8+9             7 -155.32 329.30 26.20   0.00\n## 1+4+5+6+7             7 -155.32 329.31 26.21   0.00\n## 1+3+5+7+8+9+10        9 -151.57 329.31 26.22   0.00\n## 2+5+7+10              6 -157.02 329.40 26.30   0.00\n## 1+5+7+8+9+10          8 -153.57 329.40 26.31   0.00\n## 3                     3 -161.29 329.44 26.35   0.00\n## 2+5+6+7+9             7 -155.44 329.54 26.45   0.00\n## 2+5+6+7               6 -157.10 329.56 26.46   0.00\n## 1+3+4+5+7+8+9         9 -151.70 329.58 26.48   0.00\n## 3+4+5+7+8+9+10        9 -151.70 329.58 26.49   0.00\n## 3+5+7+10              6 -157.11 329.58 26.49   0.00\n## 1+3+4+5+8+9+10        9 -151.70 329.59 26.49   0.00\n## 3+4+5+7               6 -157.12 329.59 26.50   0.00\n## 1+3+5+6+8+9           8 -153.68 329.62 26.53   0.00\n## 2+5+7+9+10            7 -155.50 329.67 26.57   0.00\n## 1+3+4+5+10            7 -155.50 329.67 26.57   0.00\n## 3+7                   4 -160.12 329.73 26.63   0.00\n## 1+2+5+7+9             7 -155.54 329.74 26.64   0.00\n## 1+3+4+5+9             7 -155.54 329.74 26.64   0.00\n## 2+5+9+10              6 -157.22 329.81 26.71   0.00\n## 1+5+6+7+8+9           8 -153.78 329.83 26.73   0.00\n## 3+5+7+9               6 -157.25 329.86 26.77   0.00\n## 3+5+10                5 -158.89 330.09 26.99   0.00\n## 4+5+6+9+10            7 -155.79 330.26 27.16   0.00\n## 1+4+5+6+9             7 -155.80 330.26 27.17   0.00\n## 1+2+9                 5 -159.01 330.32 27.23   0.00\n## 1+3+4+9+10            7 -155.87 330.41 27.32   0.00\n## 1+4+6+9+10            7 -155.88 330.42 27.33   0.00\n## 2+5                   4 -160.47 330.42 27.33   0.00\n## 2+6+9                 5 -159.08 330.47 27.37   0.00\n## 1+2+6+7+9             7 -155.98 330.63 27.53   0.00\n## 1+2+5+6+8+9           8 -154.20 330.65 27.56   0.00\n## 4+5+6+7+9+10          8 -154.20 330.66 27.57   0.00\n## 1+2+7+9+10            7 -156.00 330.66 27.57   0.00\n## 3+7+10                5 -159.18 330.68 27.58   0.00\n## 1+3+4+5+7+9           8 -154.23 330.71 27.62   0.00\n## 2+6+7+9+10            7 -156.03 330.72 27.62   0.00\n## 1+4+6+7+9+10          8 -154.27 330.81 27.71   0.00\n## 1+3+5+9+10            7 -156.08 330.82 27.73   0.00\n## 3+5+9                 5 -159.31 330.92 27.83   0.00\n## 1+5+6+7+8+9+10        9 -152.37 330.92 27.83   0.00\n## 6+7+9                 5 -159.32 330.95 27.85   0.00\n## 1+4                   4 -160.76 331.00 27.91   0.00\n## 2+6+9+10              6 -157.82 331.01 27.91   0.00\n## 1+2+9+10              6 -157.83 331.03 27.93   0.00\n## 2+10                  4 -160.81 331.10 28.00   0.00\n## 2+5+6+9               6 -157.87 331.10 28.01   0.00\n## 1+4+7                 5 -159.40 331.11 28.01   0.00\n## 1+2+5+9               6 -157.88 331.12 28.03   0.00\n## 1+2+3+5+6+7+8+9+10   11 -147.96 331.13 28.03   0.00\n## 1+3+4+5+6+7+8+9+10   11 -147.98 331.15 28.06   0.00\n## 3+4                   4 -160.84 331.16 28.06   0.00\n## 1+2+6+7               6 -157.95 331.25 28.16   0.00\n## 2+5+10                5 -159.48 331.26 28.16   0.00\n## 1+3+4+5+7+10          8 -154.52 331.31 28.21   0.00\n## 2+6+7+10              6 -157.98 331.31 28.22   0.00\n## 1+3+5+9               6 -157.98 331.32 28.23   0.00\n## 3+9                   4 -160.94 331.35 28.26   0.00\n## 1+4+5+6+7+8+9+10     10 -150.44 331.36 28.27   0.00\n## 3+4+5+9               6 -158.00 331.36 28.27   0.00\n## 1+3+4+5+7+8+10        9 -152.60 331.37 28.28   0.00\n## 1+2+7+10              6 -158.01 331.39 28.29   0.00\n## 1+3+5+8+9             7 -156.38 331.43 28.34   0.00\n## 1+4+5+6+7+9           8 -154.59 331.44 28.34   0.00\n## 3+4+5+10              6 -158.04 331.45 28.35   0.00\n## 1+4+5+7+8+9+10        9 -152.64 331.46 28.36   0.00\n## 3+7+9                 5 -159.65 331.61 28.51   0.00\n## 1+3+5+7+9+10          8 -154.77 331.79 28.70   0.00\n## 1+4+5+6+7+10          8 -154.81 331.88 28.78   0.00\n## 1+4+5+6+10            7 -156.61 331.88 28.78   0.00\n## 3+4+5+7+10            7 -156.63 331.92 28.82   0.00\n## 1+3+4+7+9+10          8 -154.84 331.94 28.84   0.00\n## 3+10                  4 -161.29 332.05 28.96   0.00\n## 1+2+5+6+7+8+9+10     10 -150.80 332.08 28.99   0.00\n## 2+9                   4 -161.35 332.18 29.08   0.00\n## 1+4+9                 5 -159.99 332.29 29.20   0.00\n## 1+2+5+7+10            7 -156.85 332.37 29.28   0.00\n## 1+2+5+6+7             7 -156.88 332.43 29.34   0.00\n## 2+5+6                 5 -160.08 332.46 29.36   0.00\n## 3+4+7                 5 -160.12 332.54 29.45   0.00\n## 2+5+6+7+10            7 -156.98 332.64 29.54   0.00\n## 1+3+4+5+9+10          8 -155.23 332.72 29.62   0.00\n## 3+4+5+7+9             7 -157.06 332.79 29.70   0.00\n## 1+4+7+9               6 -158.72 332.80 29.70   0.00\n## 3+5+7+9+10            7 -157.08 332.84 29.74   0.00\n## 1+2+6+9               6 -158.78 332.92 29.82   0.00\n## 1+3+4+5+7+8+9+10     10 -151.25 332.97 29.88   0.00\n## 3+5+9+10              6 -158.81 332.97 29.88   0.00\n## 3+4+7+10              6 -158.82 333.01 29.91   0.00\n## 5+6+7+9               6 -158.84 333.04 29.95   0.00\n## 2+5+6+7+9+10          8 -155.40 333.06 29.97   0.00\n## 1+2+5+9+10            7 -157.20 333.06 29.97   0.00\n## 2+5+6+9+10            7 -157.21 333.10 30.00   0.00\n## 3+4+9                 5 -160.40 333.10 30.00   0.00\n## 1+2+5+6+7+9           8 -155.44 333.13 30.04   0.00\n## 1+2+5                 5 -160.46 333.22 30.13   0.00\n## 1+2+5+7+9+10          8 -155.49 333.24 30.14   0.00\n## 3+7+9+10              6 -158.96 333.28 30.18   0.00\n## 2+6                   4 -161.91 333.31 30.21   0.00\n## 2+6+10                5 -160.52 333.35 30.26   0.00\n## 3+4+10                5 -160.58 333.46 30.37   0.00\n## 1+6+7+9               6 -159.08 333.52 30.43   0.00\n## 6+7+9+10              6 -159.08 333.53 30.43   0.00\n## 4+9                   4 -162.05 333.59 30.49   0.00\n## 1+4+10                5 -160.70 333.70 30.60   0.00\n## 1+2+5+10              6 -159.19 333.74 30.64   0.00\n## 1+4+7+10              6 -159.21 333.78 30.68   0.00\n## 1+4+5                 5 -160.74 333.79 30.69   0.00\n## 1+8                   4 -162.17 333.83 30.73   0.00\n## 7+9                   4 -162.18 333.84 30.74   0.00\n## 1+4+5+6+9+10          8 -155.79 333.85 30.75   0.00\n## 1+2+10                5 -160.79 333.89 30.80   0.00\n## 3+9+10                5 -160.89 334.08 30.99   0.00\n## 1+4+5+7               6 -159.37 334.10 31.01   0.00\n## 2+8+9                 5 -160.93 334.17 31.08   0.00\n## 1+2+6+7+9+10          8 -155.97 334.21 31.11   0.00\n## 2+5+6+10              6 -159.46 334.29 31.19   0.00\n## 1+2+6+9+10            7 -157.82 334.30 31.21   0.00\n## 1+2+5+6+9             7 -157.84 334.35 31.25   0.00\n## 1+4+5+6+7+9+10        9 -154.18 334.53 31.44   0.00\n## 1+2+6+7+10            7 -157.95 334.56 31.46   0.00\n## 1+3+4+5+7+9+10        9 -154.22 334.62 31.53   0.00\n## 4+5+9                 5 -161.16 334.63 31.54   0.00\n## 3+4+7+9               6 -159.64 334.64 31.54   0.00\n## 4+7+9                 5 -161.18 334.66 31.56   0.00\n## 3+4+5+9+10            7 -158.00 334.67 31.57   0.00\n## 1+4+9+10              6 -159.90 335.17 32.07   0.00\n## 1+7+9                 5 -161.43 335.18 32.08   0.00\n## 1+4+5+9               6 -159.92 335.21 32.11   0.00\n## 1+2+5+6               6 -159.98 335.32 32.22   0.00\n## 3+4+5+7+9+10          8 -156.61 335.49 32.39   0.00\n## 1+5+8                 5 -161.60 335.50 32.41   0.00\n## 6+7                   4 -163.07 335.63 32.53   0.00\n## 4+5+7+9               6 -160.14 335.64 32.55   0.00\n## 4                     3 -164.47 335.79 32.70   0.00\n## 1+4+7+9+10            7 -158.59 335.84 32.74   0.00\n## 3+4+9+10              6 -160.25 335.87 32.77   0.00\n## 1+8+9                 5 -161.78 335.87 32.78   0.00\n## 4+7+9+10              6 -160.26 335.89 32.79   0.00\n## 3+4+7+9+10            7 -158.63 335.93 32.83   0.00\n## 1+4+5+7+9             7 -158.64 335.95 32.85   0.00\n## 1+2+5+6+7+10          8 -156.85 335.97 32.87   0.00\n## 1+2+6+10              6 -160.33 336.03 32.93   0.00\n## 4+7+10                5 -161.87 336.06 32.96   0.00\n## 1+2+6                 5 -161.91 336.12 33.02   0.00\n## 5+6+7+9+10            7 -158.73 336.13 33.03   0.00\n## 4+5                   4 -163.33 336.14 33.04   0.00\n## 1+6+8                 5 -161.92 336.15 33.06   0.00\n## 1+5+6+7+9             7 -158.76 336.18 33.08   0.00\n## 5+8                   4 -163.37 336.22 33.13   0.00\n## 5+7+9                 5 -161.96 336.23 33.13   0.00\n## 4+9+10                5 -161.96 336.23 33.14   0.00\n## 6+9+10                5 -161.97 336.24 33.14   0.00\n## 7+9+10                5 -162.13 336.56 33.47   0.00\n## 1+6+7+9+10            7 -158.99 336.64 33.55   0.00\n## 1+2+5+6+9+10          8 -157.20 336.66 33.56   0.00\n## 4+7                   4 -163.59 336.67 33.57   0.00\n## 5+6+7                 5 -162.20 336.70 33.61   0.00\n## 1+4+5+10              6 -160.67 336.70 33.61   0.00\n## 1+2                   4 -163.64 336.76 33.66   0.00\n## 2+8                   4 -163.65 336.78 33.68   0.00\n## 1+2+5+6+10            7 -159.07 336.81 33.71   0.00\n## 4+5+7                 5 -162.30 336.90 33.80   0.00\n## 1+2+5+6+7+9+10        9 -155.40 336.98 33.89   0.00\n## 1+4+5+7+10            7 -159.18 337.03 33.94   0.00\n## 1+5+6+8               6 -160.88 337.12 34.02   0.00\n## 4+10                  4 -164.08 337.64 34.54   0.00\n## 4+5+7+10              6 -161.14 337.64 34.54   0.00\n## 4+5+9+10              6 -161.16 337.67 34.57   0.00\n## 4+5+7+9+10            7 -159.58 337.83 34.73   0.00\n## 1+6+8+9               6 -161.27 337.90 34.81   0.00\n## 6+7+10                5 -162.86 338.02 34.92   0.00\n## 1+7+9+10              6 -161.33 338.03 34.93   0.00\n## 1+5+8+9               6 -161.42 338.19 35.10   0.00\n## 1+5+7+9               6 -161.43 338.22 35.12   0.00\n## 5+8+9                 5 -162.97 338.24 35.15   0.00\n## 1+4+5+9+10            7 -159.83 338.32 35.23   0.00\n## 1+6+7                 5 -163.07 338.46 35.36   0.00\n## 4+5+10                5 -163.18 338.67 35.58   0.00\n## 5+6+9+10              6 -161.66 338.68 35.59   0.00\n## 1+7                   4 -164.61 338.70 35.60   0.00\n## 9+10                  4 -164.76 339.00 35.91   0.00\n## 5+6+8                 5 -163.37 339.05 35.95   0.00\n## 1+6+9+10              6 -161.95 339.27 36.17   0.00\n## 5+7+9+10              6 -161.96 339.28 36.19   0.00\n## 1+4+5+7+9+10          8 -158.51 339.29 36.19   0.00\n## 7                     3 -166.23 339.31 36.21   0.00\n## 1+5+6+7               6 -162.12 339.60 36.50   0.00\n## 5+6+7+10              6 -162.13 339.63 36.53   0.00\n## 1+5+6+8+9             7 -160.50 339.66 36.56   0.00\n## 1+5+6+7+9+10          8 -158.70 339.66 36.57   0.00\n## 1+9+10                5 -163.78 339.86 36.77   0.00\n## 2                     3 -166.97 340.80 37.70   0.00\n## 1+6+7+10              6 -162.80 340.95 37.85   0.00\n## 1+7+10                5 -164.34 340.98 37.89   0.00\n## 1+5+7                 5 -164.35 341.00 37.91   0.00\n## 5+6+8+9               6 -162.93 341.22 38.12   0.00\n## 1+5+7+9+10            7 -161.33 341.33 38.23   0.00\n## 5+9+10                5 -164.60 341.50 38.40   0.00\n## 8+9                   4 -166.01 341.51 38.41   0.00\n## 5+7                   4 -166.13 341.75 38.65   0.00\n## 7+10                  4 -166.15 341.78 38.69   0.00\n## 6+8                   4 -166.25 341.97 38.88   0.00\n## 1+5+6+9+10            7 -161.66 341.99 38.89   0.00\n## 1+5+6+7+10            7 -161.96 342.60 39.50   0.00\n## 6+8+9                 5 -165.20 342.70 39.60   0.00\n## 1+5+9+10              6 -163.77 342.90 39.80   0.00\n## 9                     3 -168.35 343.55 40.45   0.00\n## 1+5+7+10              6 -164.14 343.64 40.54   0.00\n## 5+7+10                5 -166.11 344.52 41.42   0.00\n## 5+9                   4 -167.62 344.73 41.63   0.00\n## 8                     3 -169.05 344.95 41.85   0.00\n## 1+9                   4 -167.83 345.14 42.04   0.00\n## 1+6+9                 5 -166.44 345.19 42.09   0.00\n## 6+10                  4 -167.93 345.33 42.24   0.00\n## 5+6+9                 5 -166.54 345.39 42.30   0.00\n## 6+9                   4 -168.33 346.14 43.04   0.00\n## 1+10                  4 -168.41 346.29 43.20   0.00\n## 5+6+10                5 -167.14 346.59 43.49   0.00\n## 1+5+6+9               6 -165.63 346.61 43.52   0.00\n## 1+6+10                5 -167.49 347.29 44.19   0.00\n## 1+5+9                 5 -167.60 347.50 44.41   0.00\n## 1+5+6+10              6 -166.45 348.26 45.16   0.00\n## 1+5+10                5 -168.00 348.31 45.21   0.00\n## 10                    3 -171.08 349.02 45.92   0.00\n## 5+10                  4 -171.06 351.59 48.50   0.00\n## 5+6                   4 -175.13 359.75 56.65   0.00\n## 5                     3 -176.59 360.03 56.94   0.00\n## 1+5                   4 -176.35 362.18 59.08   0.00\n## 1+5+6                 5 -175.06 362.43 59.34   0.00\n## (Null)                2 -180.19 364.79 61.69   0.00\n## 1                     3 -179.21 365.28 62.18   0.00\n## 6                     3 -179.93 366.72 63.62   0.00\n## 1+6                   4 -179.00 367.48 64.39   0.00\n## \n## Term codes: \n##   am carb  cyl disp drat gear  kml qsec   vs   wt \n##    1    2    3    4    5    6    7    8    9   10 \n## \n## Model-averaged coefficients:  \n## (full average) \n##             Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)  70.8539    98.1389    100.4683   0.705 0.480663    \n## carb         21.4717     5.4456      5.5937   3.839 0.000124 ***\n## disp          0.4453     0.1401      0.1432   3.109 0.001875 ** \n## wt          -20.6451    16.9931     17.3080   1.193 0.232944    \n## vs            4.9071    12.3728     12.6790   0.387 0.698736    \n## cyl           1.8423     5.4948      5.6138   0.328 0.742784    \n## drat         -1.2146     6.7301      6.9936   0.174 0.862127    \n## qsec         -2.1062     4.6592      4.7408   0.444 0.656841    \n## kml           0.1869     1.4603      1.5156   0.123 0.901832    \n## am            0.5697     7.5870      7.8770   0.072 0.942342    \n## gear          1.3335     6.8393      7.0322   0.190 0.849599    \n##  \n## (conditional average) \n##             Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n## (Intercept)  70.8539    98.1389    100.4683   0.705 0.480663    \n## carb         21.6289     5.1451      5.3027   4.079 4.53e-05 ***\n## disp          0.4508     0.1318      0.1352   3.334 0.000855 ***\n## wt          -28.6308    13.1083     13.6677   2.095 0.036190 *  \n## vs           17.9458    18.0517     18.8126   0.954 0.340123    \n## cyl           7.5820     8.9855      9.2835   0.817 0.414090    \n## drat         -6.5756    14.4903     15.1510   0.434 0.664283    \n## qsec         -6.5823     6.1951      6.3856   1.031 0.302633    \n## kml           1.0361     3.3075      3.4425   0.301 0.763436    \n## am            3.2174    17.7921     18.4899   0.174 0.861860    \n## gear          6.9068    14.2750     14.7525   0.468 0.639659    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# adäquatest model gemäss multimodel inference\nmodel_ad &lt;- lm(hp ~ carb + disp + wt, data = mtcars)\nsummary(model_ad)\n## \n## Call:\n## lm(formula = hp ~ carb + disp + wt, data = mtcars)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -45.225 -14.235   3.879  20.621  39.785 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  53.16715   18.16036   2.928  0.00671 ** \n## carb         23.57691    2.99391   7.875 1.41e-08 ***\n## disp          0.51663    0.07669   6.736 2.59e-07 ***\n## wt          -28.59214    9.87292  -2.896  0.00725 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 24.32 on 28 degrees of freedom\n## Multiple R-squared:  0.8863, Adjusted R-squared:  0.8742 \n## F-statistic: 72.78 on 3 and 28 DF,  p-value: 2.462e-13"
  },
  {
    "objectID": "statKons/StatKons4_Demo_GLM.html#poisson-regression",
    "href": "statKons/StatKons4_Demo_GLM.html#poisson-regression",
    "title": "StatKons4: Demo",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\n############\n# quasipoisson regression\n############\n\ncars &lt;- mtcars |&gt; \n   mutate(kml = (235.214583/mpg))\n\nglm.poisson &lt;- glm(hp ~ kml, data = cars, family = \"poisson\")\n\nsummary(glm.poisson) # klare overdisperion\n\n\nCall:\nglm(formula = hp ~ kml, family = \"poisson\", data = cars)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-6.438  -2.238  -1.159   2.457  10.576  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 3.894293   0.050262   77.48   &lt;2e-16 ***\nkml         0.081666   0.003414   23.92   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 958.27  on 31  degrees of freedom\nResidual deviance: 426.59  on 30  degrees of freedom\nAIC: 645.67\n\nNumber of Fisher Scoring iterations: 4\n\n# deshalb quasipoisson\nglm.quasipoisson &lt;- glm(hp ~ kml, data = cars, family = quasipoisson(link = log))\n\nsummary(glm.quasipoisson)\n\n\nCall:\nglm(formula = hp ~ kml, family = quasipoisson(link = log), data = cars)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-6.438  -2.238  -1.159   2.457  10.576  \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.89429    0.19508  19.963  &lt; 2e-16 ***\nkml          0.08167    0.01325   6.164 8.82e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 15.06438)\n\n    Null deviance: 958.27  on 31  degrees of freedom\nResidual deviance: 426.59  on 30  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n# visualisiere\nggplot2::ggplot(cars, aes(x = kml, y = hp)) + \n    geom_point(size = 8) + \n    geom_smooth(method = \"glm\", method.args = list(family = \"poisson\"), se = F,\n                color = \"green\", size = 2) + \n    scale_x_continuous(limits = c(0,35)) + \n    scale_y_continuous(limits = c(0,400)) + \n    theme_classic()\n\n\n\n#Rücktransformation meines Outputs für ein besseres Verständnis\nglm.quasi.back &lt;- exp(coef(glm.quasipoisson))\n\n#für ein schönes ergebnis\nglm.quasi.back |&gt;\n  broom::tidy() |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\nnames\nx\n\n\n\n\n(Intercept)\n49.121\n\n\nkml\n1.085\n\n\n\n\n#for more infos, also for posthoc tests\n#here: https://rcompanion.org/handbook/J_01.html"
  },
  {
    "objectID": "statKons/StatKons4_Demo_GLM.html#logistische-regression",
    "href": "statKons/StatKons4_Demo_GLM.html#logistische-regression",
    "title": "StatKons4: Demo",
    "section": "logistische Regression",
    "text": "logistische Regression\n\n############\n# logistische regression\n############\ncars &lt;- mtcars\n\n# erstelle das modell\nglm.binar &lt;- glm(vs ~ hp, data = cars, family = binomial(link = logit)) \n\n#achtung Model gibt Koeffizienten als logit() zurück\nsummary(glm.binar)\n\n\nCall:\nglm(formula = vs ~ hp, family = binomial(link = logit), data = cars)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.12148  -0.20302  -0.01598   0.51173   1.20083  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  8.37802    3.21593   2.605  0.00918 **\nhp          -0.06856    0.02740  -2.502  0.01234 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.860  on 31  degrees of freedom\nResidual deviance: 16.838  on 30  degrees of freedom\nAIC: 20.838\n\nNumber of Fisher Scoring iterations: 7\n\n# überprüfe das modell\ncars$predicted &lt;- predict(glm.binar, type = \"response\")\n\n# visualisiere\nggplot(cars, aes(x = hp, y = vs)) +    \n    geom_point(size = 8) +\n    geom_point(aes(y = predicted), shape  = 1, size = 6) +\n    guides(color = \"none\") +\n    geom_smooth(method = \"glm\", method.args = list(family = 'binomial'), \n                se = FALSE,\n                size = 2) +\n    # geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    mytheme\n\n\n\n#Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq(glm.binar$deviance,glm.binar$df.resid)  \n\n[1] 0.9744718\n\n#Modellgüte (pseudo-R²)\n1 - (glm.binar$dev / glm.binar$null)  \n\n[1] 0.6161072\n\n#Steilheit der Beziehung (relative Änderung der odds von x + 1 vs. x)\nexp(glm.binar$coefficients[2])\n\n       hp \n0.9337368 \n\n#LD50 (wieso negativ: weil zweiter koeffizient negative steigung hat)\nabs(glm.binar$coefficients[1]/glm.binar$coefficients[2])\n\n(Intercept) \n   122.1986 \n\n# kreuztabelle (confusion matrix): fasse die ergebnisse aus predict und \n# \"gegebenheiten, realität\" zusammen\ntab1 &lt;- table(cars$predicted&gt;.5, cars$vs)\ndimnames(tab1) &lt;- list(c(\"M:S-type\",\"M:V-type\"), c(\"T:S-type\", \"T:V-type\"))\ntab1 \n\n         T:S-type T:V-type\nM:S-type       15        2\nM:V-type        3       12\n\nprop.table(tab1, 2) \n\n          T:S-type  T:V-type\nM:S-type 0.8333333 0.1428571\nM:V-type 0.1666667 0.8571429\n\n#was könnt ihr daraus ablesen? Ist unser Modell genau?\n\n# Funktion die die logits in Wahrscheinlichkeiten transformiert\n# mehr infos hier: https://sebastiansauer.github.io/convert_logit2prob/\n# dies ist interessant, falls ihr mal ein kategorialer Prädiktor habt\nlogit2prob &lt;- function(logit){\n  odds &lt;- exp(logit)\n  prob &lt;- odds / (1 + odds)\n  return(prob)\n}"
  },
  {
    "objectID": "statKons/StatKons4_Demo_GLM.html#gams",
    "href": "statKons/StatKons4_Demo_GLM.html#gams",
    "title": "StatKons4: Demo",
    "section": "GAM’s",
    "text": "GAM’s\n\n###########\n# LOESS & GAM\n###########\n\nggplot2::ggplot(mtcars, aes(x = mpg, y = hp)) + \n  geom_point(size = 8) + \n  geom_smooth(method = \"gam\", se = F, color = \"green\", size = 2, formula = y ~ s(x, bs = \"cs\")) + \n  geom_smooth(method = \"loess\", se = F, color = \"red\", size = 2) + \n  geom_smooth(method = \"glm\", size = 2, color = \"blue\", se = F) + \n  scale_x_continuous(limits = c(0,35)) + \n    scale_y_continuous(limits = c(0,400)) + \n    mytheme\n\n\n\nggplot2::ggplot(mtcars, aes(x = mpg, y = hp)) + \n  geom_point(size = 8) + \n  geom_smooth(method = \"gam\", se = F, color = \"green\", size = 2, formula = y ~ s(x, bs = \"cs\")) + \n    # geom_smooth(method = \"loess\", se = F, color = \"red\", size = 2) + \n  geom_smooth(method = \"glm\", size = 2, color = \"grey\", se = F) + \n  scale_x_continuous(limits = c(0,35)) + \n  scale_y_continuous(limits = c(0,400)) + \n  mytheme"
  },
  {
    "objectID": "RaumAn.html#teil-1",
    "href": "RaumAn.html#teil-1",
    "title": "Räumliche Analysen",
    "section": "Teil 1",
    "text": "Teil 1\nDie erste Übung zur Raumanalyse illustriert das einfache Laden und Anzeigen von Geodaten im Vektor- und Raster-Datenformat. Zusätzlich veranschaulicht die Übung den Umgang mit Koordinatensystemen sowie die Vektor-Raster-Konvertierung. Einfach erste Analysen illustrieren den Spatial Join, die Annotieren von Punkten mit Attributen von die Punkte einbettenden Vektordaten. Zum Abschluss thematisiert die Übung die Aggregationsabhängigkeit räumlicher Daten durch die Illustration des Modifiable Areal Unit Problem (MAUP)."
  },
  {
    "objectID": "RaumAn.html#teil-2",
    "href": "RaumAn.html#teil-2",
    "title": "Räumliche Analysen",
    "section": "Teil 2",
    "text": "Teil 2\nIn dieser zweiten Übung wirst Du wiederum Geodatensätze verarbeiten und darstellen. Wir starten mit einem Punktdatensatz zu einem Messnetz zur Erhebung der Luftqualität in der Schweiz (Stickstoffdioxid NO2 um genau zu sein). Im Gegensatz zum Punktdatensatz zur Wasserverfügbarkeit aus der vorherigen Übung, sind die Messstellen des Messnetzes zur Luftqualität sehr unregelmässig im Raum verteilt. Trotzdem möchten wir versuchen einen kontinuierlichen Layer von Luftqualitätswerten für die ganze Schweiz zu interpolieren. Wir starten mit der einfachen Interpolations-Methode Inverse Distance Weighting IDW. Danach wollen wir für den gleichen Datensatz nach dem Ansatz der nächsten Nachbarn die Thiessen Polygone konstruieren. Im zweiten Teil der Übung wollen wir Dichteverteilung untersuchen. Dabei untersuchen wir einen Datensatz mit Bewegungsdaten eines Rotmilans in der Schweiz. Mittels einer Kernel Density Estimation (KDE) berechnen wir eine kontinuierliche Dichteverteilung, über die wir eine Annäherung an das Habitat des untersuchten Greifvogels berechnen können. Wer mag, kann sich in der optionalen Übung C die Punktdatensätze genauer anschauen, über die Berechnung der G-Function."
  },
  {
    "objectID": "RaumAn.html#teil-3",
    "href": "RaumAn.html#teil-3",
    "title": "Räumliche Analysen",
    "section": "Teil 3",
    "text": "Teil 3\nIm dritten Teil von Spatial Data Science berechnen wir mit dem Moran’s I einen Index zur Berechnung der räumlichen Autokorrelation einer Choroplethenkarte. Wir verwenden nochmals die aggregierten Choroplethenkarten zur Wasserverfügbarkeit aus der ersten Übung und schauen uns an, wie stark die Werte für die Kantone und die Bezirke autokorreliert sind. Anstatt einfach eine Funktion zur Berechnung von Moran’s I aufzurufen und diese dann wie eine Black Box anzuwenden, wollen wir Formel für die Berechnung des Index in Ihre Bausteine zerlegen und diese Schritt für Schritt selber nachrechnen. So seht Ihr, wie Moran’s I wirklich funktioniert und könnte dabei erst noch die zuvor gelernten Data Science Techniken repetieren."
  },
  {
    "objectID": "RaumAn.html#teil-4",
    "href": "RaumAn.html#teil-4",
    "title": "Räumliche Analysen",
    "section": "Teil 4",
    "text": "Teil 4\nIn der Übung 4 lernt ihr einfache räumliche Analyse-Werkezeuge kennen, und zwar für Vektor- und Rasterdaten. Zuerst kehren wir zurück zu den Vegeationsdaten des Campus Grüental. Jetzt wollen wir anstelle des Spatial Joins aber mit Puffern arbeiten. Die kreisförmigen Puffer sollen für jeden Datenpunkt eine räumliche Nachbarschaft der Raumnutzung berechnen. Im zweiten Teil nähern wir uns dem Rasterdatenformat an, und zwar über klassische Operationen über ein digitales Geländemodell. Dabei berechnen wir Hangneigung und Exposition."
  },
  {
    "objectID": "RaumAn.html#teil-5",
    "href": "RaumAn.html#teil-5",
    "title": "Räumliche Analysen",
    "section": "Teil 5",
    "text": "Teil 5\nDen krönenden Abschluss des Themenblocks Raumanalyse bildet die Durchführung einer Multi-Kriterien Analyse zur Bestimmung von optimalen Standorten für Windturbinen. Dabei orientieren wir uns an die in der Vorlesung behandelten Vorgaben und setzen die geforderten Kriterien mit Raumdaten um. Für die Vorbereitung der Datensätze kommen nun viele Konzepte und Verfahren wieder zum Zug, die Ihr bisher kennen gelernt habt. Zum Schluss gewichten wir die einzelnen Layer und verschneiden sie zu einer Eignungskarte.\nAuf geht’s!"
  },
  {
    "objectID": "rauman/Rauman1_Vorbereitung.html",
    "href": "rauman/Rauman1_Vorbereitung.html",
    "title": "Vorbereitung",
    "section": "",
    "text": "Im Rahmen von Rauman 1 - 5 werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog Kapitel 1 könnt ihr mit nachstehendem Code alle noch nicht installierten packages automatisch installieren.\n\n\nipak &lt;- function(pkg){\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if(length(new.pkg)){install.packages(new.pkg, dependencies = TRUE)}\n}\n\npackages &lt;- c(\"sf\", \"dplyr\", \"ggplot2\", \"spatstat.geom\", \"spatstat.explore\", \n\"gstat\", \"tidyr\", \"terra\", \"tmap\")\n\nipak(package)\n## Error in ipak(package): object 'package' not found"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-1-vektor-daten-runterladen-und-importieren",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-1-vektor-daten-runterladen-und-importieren",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 1: Vektor Daten runterladen und importieren",
    "text": "Aufgabe 1: Vektor Daten runterladen und importieren\nLade zunächst die Datensätze unter folgenden Links herunter:\n\nkantone.gpkg\ngemeinden.gpkg\n\nEs handelt sich um Geodatensätze im Format Geopackage (“*.gpkg”), eine alternatives Datenformat zum bekannteren Format “Shapefiles”. Importiere die Datensätze wie folgt:\n\nkantone &lt;- read_sf(\"datasets/rauman/kantone.gpkg\")\ngemeinden &lt;- read_sf(\"datasets/rauman/gemeinden.gpkg\") \n\nSchau Dir die importierten Datensätze an.\n\n\n\n\n\n\nHinweis\n\n\n\nAm meisten Informationen zu sf Objekten bekommst du, wenn du dir den Datensatz in der Konsole anschaust (in dem du den Variabel-Name in der Konsole eintippst). Mit dem RStudio Viewer werden sf Objekte nur sehr langsam geladen und die Metadaten werden nicht angezeigt."
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-2-daten-visualisieren",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-2-daten-visualisieren",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 2: Daten Visualisieren",
    "text": "Aufgabe 2: Daten Visualisieren\nEine sehr einfache Möglichkeit, sf-Objekte zu visualiseren ist die base-R Funktion plot(). Führe die angegebenen R-Befehle aus und studiere die entstehenden Plots. Welche Unterschiede findest Du? Wie erklärst Du diese Unterschiede?\n\n# ohne max.plot = 1 macht R einen Plot pro Spalte\nplot(gemeinden, max.plot = 1)\n\n\n\n\n# Alternativ kann man auch eine spezifische Spalte plotten\nplot(kantone[\"KANTONSFLA\"])"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#input-koodinatensysteme",
    "href": "rauman/Rauman1_Uebung_A.html#input-koodinatensysteme",
    "title": "Rauman 1: Übung A",
    "section": "Input: Koodinatensysteme",
    "text": "Input: Koodinatensysteme\nIn der obigen visualierung fällt folgendes auf:\n\ndie X/Y Achsen weisen zwei ganz unterschiedlichen Zahlenbereiche auf (vergleiche die Achsenbeschriftungen)\nder Umriss der Schweiz sieht in den beiden Datensätzen unterschiedlich aus (kantone ist gegenüber gemeinden gestaucht)\n\nDies hat natürlich damit zu tun, dass die beiden Datensätze in unterschiedlichen Koordinatensystemen erfasst wurden. Koordinatensysteme werden mit CRS (Coordinate Reference System) abgekürzt. Mit st_crs() können die zugewiesenen Koordinatensysteme abgefragt werden.\n\nst_crs(kantone)\n## Coordinate Reference System:\n##   User input: Undefined Cartesian SRS \n##   wkt:\n## ENGCRS[\"Undefined Cartesian SRS\",\n##     EDATUM[\"\"],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"Meter\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"Meter\",1]]]\nst_crs(gemeinden)\n## Coordinate Reference System:\n##   User input: Undefined Cartesian SRS \n##   wkt:\n## ENGCRS[\"Undefined Cartesian SRS\",\n##     EDATUM[\"\"],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"Meter\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"Meter\",1]]]\n\nLeider sind in unserem Fall keine Koordinatensysteme zugewiesen. Mit etwas Erfahrung kann man das Koordinatensystem aber erraten, so viele kommen nämlich gar nicht in Frage. Am häufigsten trifft man hierzulande eines der drei folgenden Koordinatensysteme an:\n\nCH1903 LV03: das alte Koordinatensystem der Schweiz\nCH1903+ LV95: das neue Koordinatensystem der Schweiz\nWGS84: ein häufig genutztes weltumspannendes geodätisches Koordinatensystem, sprich die Koordinaten werden in Länge und Breite angegeben (Lat/Lon).\n\nNun gilt es, anhand der Koordinaten die in der Spalte geometry ersichtlich sind, das korrekte Koordinatensystem festzustellen. Wenn man auf map.geo.admin.ch mit der rechten Maustaste einen Ort anwählt, erfährt man die Koordinaten dieses Ortes in verschiedenen Koordinatenbezugssystemen.\n\n\nWenn man diese Koordinaten mit den Koordinaten unserer Datensätze vergleicht, dann ist schnell klar, dass es sich beim Datensatz kantone um das Koordinatenbezugsystem (CRS) WGS84 handelt. Wir können diese Information nutzen um das CRS unserers Datensatzes mit st_set_crs() zu setzen.\n\n# Zuweisen mit st_set_crs()...\nkantone &lt;- st_set_crs(kantone, \"WGS84\")\n\nWenn wir die CRS Information nun abrufen, sehen wir das diese Zuweisung funktioniert hat.\n\n# ... abfragen mit st_crs()\nst_crs(kantone)\n## Coordinate Reference System:\n##   User input: WGS84 \n##   wkt:\n## GEOGCRS[\"WGS 84\",\n##     DATUM[\"World Geodetic System 1984\",\n##         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n##             LENGTHUNIT[\"metre\",1]]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"geodetic latitude (Lat)\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"geodetic longitude (Lon)\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     ID[\"EPSG\",4326]]\n\nEtwas komplizierter ist es, wenn wir das CRS vom Datensatz gemeinden setzen wollen. Im Vergleich mit map.geo.admin.ch sehen wir, dass es sich hier um das CRS CH1903+ LV95 handeln muss. Wenn wir diesen Namen für unsere CRS Zuweisung verwenden möchten, funktioniert das nicht:\n\n# Zuweisen mit st_set_crs()...\ngemeinden &lt;- st_set_crs(gemeinden, \"CH1903+ LV95\")\n\n# ... abfragen mit st_crs()\nst_crs(gemeinden)\n## Coordinate Reference System: NA\n\nDie ausgeschriebenen Namen dieser CRS sind fehleranfällig. Deshalb ist es besser, mit den jeweiligen EPSG Codes der Bezugssysteme zu arbeiten. Diese EPSG Codes kann man auf folgender Website erfahren: epsg.io/map. Es lohnt sich aber, die EPSG Codes der für uns relevanten CRS zu notieren:\n\nCH1903 LV03: EPSG:21781\nCH1903+ LV95: EPSG:2056\nWGS84: EPSG:4326\n\nDieser Code können wir nutzen, um das CRS des Datensatz gemeinde zu setzen:\n\n# Zuweisen mit st_set_crs()...\ngemeinden &lt;- st_set_crs(gemeinden, 2056)\n\n# ... abfragen mit st_crs()\nst_crs(gemeinden)\n## Coordinate Reference System:\n##   User input: EPSG:2056 \n##   wkt:\n## PROJCRS[\"CH1903+ / LV95\",\n##     BASEGEOGCRS[\"CH1903+\",\n##         DATUM[\"CH1903+\",\n##             ELLIPSOID[\"Bessel 1841\",6377397.155,299.1528128,\n##                 LENGTHUNIT[\"metre\",1]]],\n##         PRIMEM[\"Greenwich\",0,\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         ID[\"EPSG\",4150]],\n##     CONVERSION[\"Swiss Oblique Mercator 1995\",\n##         METHOD[\"Hotine Oblique Mercator (variant B)\",\n##             ID[\"EPSG\",9815]],\n##         PARAMETER[\"Latitude of projection centre\",46.9524055555556,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8811]],\n##         PARAMETER[\"Longitude of projection centre\",7.43958333333333,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8812]],\n##         PARAMETER[\"Azimuth of initial line\",90,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8813]],\n##         PARAMETER[\"Angle from Rectified to Skew Grid\",90,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8814]],\n##         PARAMETER[\"Scale factor on initial line\",1,\n##             SCALEUNIT[\"unity\",1],\n##             ID[\"EPSG\",8815]],\n##         PARAMETER[\"Easting at projection centre\",2600000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8816]],\n##         PARAMETER[\"Northing at projection centre\",1200000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8817]]],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"metre\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"metre\",1]],\n##     USAGE[\n##         SCOPE[\"Cadastre, engineering survey, topographic mapping (large and medium scale).\"],\n##         AREA[\"Liechtenstein; Switzerland.\"],\n##         BBOX[45.82,5.96,47.81,10.49]],\n##     ID[\"EPSG\",2056]]\n\nJetzt wo das CRS der Datensätze bekannt ist, können wir ggplot2 nutzen um usere Daten zu visualisieren. In InfoVis 1 & 2 haben wir intensiv mit ggplot2 gearbeitet und dort die Layers geom_point() und geom_line() kennen gelernt. Zusätzlich beinhaltet ggplot die Möglichkeit, mit geom_sf() Vektordaten direkt und sehr einfach zu plotten.\n\n\nCode\nggplot() + \n  # bei geom_sf müssen weder x noch y Spalten angegeben werden\n  geom_sf(data = gemeinden)"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-3-koordinatensyteme-transformieren",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-3-koordinatensyteme-transformieren",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 3: Koordinatensyteme transformieren",
    "text": "Aufgabe 3: Koordinatensyteme transformieren\nIn der vorherigen Übung haben wir das bestehende Koordinatensystem zugewiesen. Dabei haben wir die bestehenden Koordinaten (in der Spalte geom) nicht manipuliert. Ganz anders ist eine Transformation der Daten von einem Koordinatensystem in das andere. Bei einer Transformation werden die Koordinaten in das neue Koordinatensystem umgerechnet und somit manipuliert. Aus praktischen Gründen wollen  wir all unsere Daten ins neue Schweizer Koordinatensystem CH1903+ LV95 transfomieren. Transformiere den Datensatz kantone mit st_transform()in CH1903+ LV95, nutze dafür den korrekten EPSG-Code.\nVor der Transformation (betrachte die Attribute Bounding box, Projected CRS sowie die Werte in der Spalte geom):\n\nkantone\n## Simple feature collection with 51 features and 6 fields\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 5.955902 ymin: 45.81796 xmax: 10.49217 ymax: 47.80845\n## Geodetic CRS:  WGS 84\n## # A tibble: 51 × 7\n##    NAME       KANTONSNUM SEE_FLAECH KANTONSFLA KT_TEIL EINWOHNERZ\n##  * &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;\n##  1 Graubünden         18         NA     710530 0           197888\n##  2 Bern                2      11897     595952 1          1031126\n##  3 Valais             23       1060     522463 0           341463\n##  4 Vaud               22      39097     321201 1           793129\n##  5 Ticino             21       7147     281216 0           353709\n##  6 St. Gallen         17       7720     202820 1           504686\n##  7 Zürich              1       6811     172894 0          1504346\n##  8 Fribourg           10       7818     167142 1           315074\n##  9 Luzern              3       6438     149352 0           406506\n## 10 Aargau             19        870     140380 1           670988\n## # ℹ 41 more rows\n## # ℹ 1 more variable: geom &lt;POLYGON [°]&gt;\n\n\n\nCode\nkantone &lt;- st_transform(kantone, 2056)\n\n\nNach der Transformation (betrachte die Attribute Bounding box, Projected CRS sowie die Werte in der Spalte geom):\n\nkantone\n## Simple feature collection with 51 features and 6 fields\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2485410 ymin: 1075268 xmax: 2833858 ymax: 1295934\n## Projected CRS: CH1903+ / LV95\n## # A tibble: 51 × 7\n##    NAME       KANTONSNUM SEE_FLAECH KANTONSFLA KT_TEIL EINWOHNERZ\n##  * &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;\n##  1 Graubünden         18         NA     710530 0           197888\n##  2 Bern                2      11897     595952 1          1031126\n##  3 Valais             23       1060     522463 0           341463\n##  4 Vaud               22      39097     321201 1           793129\n##  5 Ticino             21       7147     281216 0           353709\n##  6 St. Gallen         17       7720     202820 1           504686\n##  7 Zürich              1       6811     172894 0          1504346\n##  8 Fribourg           10       7818     167142 1           315074\n##  9 Luzern              3       6438     149352 0           406506\n## 10 Aargau             19        870     140380 1           670988\n## # ℹ 41 more rows\n## # ℹ 1 more variable: geom &lt;POLYGON [m]&gt;"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-4-tidyverse-funktionen",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-4-tidyverse-funktionen",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 4: Tidyverse Funktionen",
    "text": "Aufgabe 4: Tidyverse Funktionen\nsf Objekte sind im wesentlichen data.frames mit ein paar Metadaten und einer speziellen geometry-Spalte. Wir können ihnen die gleichen Operationen durchführen, wie mit data.frames. Beispielsweise können wir aus den Spalten EINWOHNERZ und KANTONSFLA die Einwohnerdichte berechnen:\n\nkantone &lt;- kantone |&gt;\n  mutate(\n    # hektaren in km2 konvertieren\n    flaeche_km2 = KANTONSFLA/100,\n    # dichte pro km2 berechnen\n    bevoelkerungsdichte = EINWOHNERZ/flaeche_km2\n    )\n\nBerechne nun die Einwohnerdichte auf der Ebene der Gemeinden.\n\ngemeinden &lt;- gemeinden |&gt;\n  mutate(\n    flaeche_km2 = GEM_FLAECH/100,\n    bevoelkerungsdichte = EINWOHNERZ/flaeche_km2)"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-5-choroplethen-karte",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-5-choroplethen-karte",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 5: Choroplethen Karte",
    "text": "Aufgabe 5: Choroplethen Karte\nNun wollen wir die Gemeinden respektive die Kantone nach ihrer Bevölkerungsdichte einfärben. Dafür verwenden wir wie gewohnt die Methode aes(fill = ...) von ggplot.\n\n\nCode\nggplot(kantone) + \n  geom_sf(aes(fill = bevoelkerungsdichte))\n\n\n\n\n\nHier sind farblich kaum Unterschiede erkennbar, weil die extrem hohe Bevölkerungsdichte vom Halbkanton Basel-Stadt (&gt;5’000 Einwohner pro km2!) die ganze Farbskala dominiert. Der Statistischer Atlas der Schweiz löst das Problem, indem es Klassen mit irregulären Schwellwerte verwendet und alle zahlen &gt;2’000 gruppiert. Diese Vorgehensweise können wir mit cut() rekonstruieren.\n\n# Schwellwerte analog BFS \"Statistischer Atlas der Schweiz\"\nbreaks = c(0,50,100,150,200,300,500,750,1000,2000,Inf)\n\n# Klassen auf der Basis dieser Schwellenwerte bilden\nkantone &lt;- kantone |&gt;\n  mutate(bevoelkerungsdichte_klassen = cut(bevoelkerungsdichte, breaks))\n\n# Farbpalette erstellen: Wir brauchen so viele Farben, wie wir \"breaks\" haben, minus 1\nncols &lt;- length(breaks)-1\n\n# Farbpalette erstellen (siehe RColorBrewer::display.brewer.all())\nred_yellow_green &lt;- RColorBrewer::brewer.pal(ncols, \"RdYlGn\")\n\n# Farbpalette umdrehen (zu green-red-yellow)\ngreen_red_yellow &lt;- rev(red_yellow_green)\n\np_kantone &lt;- ggplot(kantone, aes(fill = bevoelkerungsdichte_klassen)) +\n  geom_sf(colour = NA) +\n  scale_fill_manual(values = green_red_yellow) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\nErstelle die gleichen Klassen für die Bevölkerungsdichte der Gemeinden und vergleiche die Plots.\n\n\nCode\ngemeinden &lt;- gemeinden |&gt;\n  mutate(bevoelkerungsdichte_klassen = cut(bevoelkerungsdichte, breaks))  \n\np_gemeinden &lt;- ggplot(gemeinden, aes(fill = bevoelkerungsdichte_klassen)) +\n  geom_sf(colour = NA) +\n  scale_fill_manual(values = green_red_yellow) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n(a) Kantone\n\n\n\n\n\n\n\n(b) Gemeinde\n\n\n\n\nAbbildung 52.1: Der Vergleich dieser beiden Darstellungen veranschaulicht die MAUP Problematik sehr deutlich"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_B.html",
    "href": "rauman/Rauman1_Uebung_B.html",
    "title": "Rauman 1: Übung B",
    "section": "",
    "text": "Für die kommende Übung arbeiten wir mit nachstehendem Datensatz: gruental.gpkg. Lade diesen Herunter und importiere ihn in R. Zudem brauchen wir die folgenden libraries:\n\nlibrary(\"dplyr\")\nlibrary(\"sf\")\nlibrary(\"ggplot2\")\n\n\nAufgabe 1: Geopackage “Layers”\nAllenfalls ist euch beim Importieren des Geopackage gruental.gpkg folgende Warnmeldung aufgefallen:\nWarning message:\nIn evalq((function (..., call. = TRUE, immediate. = FALSE, noBreaks. = FALSE,  :\n  automatically selected the first layer in a data source containing more than one.\nDiese Warnmeldung weist darauf hin, dass das Geopackage gruental.gpkg mehrere Layers (rep. Datensätze) enthält und nur der erste Layer importiert wurde. Bringe mit dem Befehl st_layers die Layer Namen in Erfahrung und nutze diese im Anschluss in st_read (als Argument layer =) um die layers einzeln zu importieren und in variablen zu speichern (zB in als Variable wiesen und baeume).\n\n\nCode\nst_layers(\"datasets/rauman/gruental.gpkg\")\n## Driver: GPKG \n## Available layers:\n##   layer_name geometry_type features fields       crs_name\n## 1     wiesen       Polygon       37      1 CH1903+ / LV95\n## 2     baeume         Point      185      2 CH1903+ / LV95\n\nwiesen &lt;- read_sf(\"datasets/rauman/gruental.gpkg\", \"wiesen\")\nbaeume &lt;- read_sf(\"datasets/rauman/gruental.gpkg\", \"baeume\")\n\n\n\n\nAufgabe 2: Datensätze erkunden\nNimm dir etwas Zeit und erkunde die beiden Datensätze. Nutze dafür auch die Visualisierungsmöglichkeiten von ggplot (insbesondere geom_sf). Du kannst mehrere geom_sf() übereinander lagern, um gleichzeitig mehrere Datensätze darzustellen.\n\n\nCode\nggplot(wiesen) +\n  geom_sf(aes(fill = flaechen_typ)) +\n  geom_sf(data = baeume) +\n  theme_void()\n\nggplot(wiesen) +\n  geom_sf() +\n  geom_sf(data = baeume, aes(colour = baum_typ)) +\n  theme_void()\n\n\n\n\n\n\n\nAbbildung 53.1: Wiesen-Flächen eingefärbt nach Typ\n\n\n\n\n\n\n\nAbbildung 53.2: Bäume eingefärbt nach Baumtyp\n\n\n\n\n\n\n\n\nAufgabe 3: Spatial Join mit Punkten\nWir wollen nun für jeden Baum wissen, ob er sich in einer Wiese befindet oder nicht. Dazu nutzen wir die GIS-Technik Spatial Join, die in der Vorlesung beschrieben wurde. In sf können wir Spatial Joins mit der Funktion st_join durchführen, dabei gibt es nur left sowie inner-Joins (vgl. PrePro 1 & 2). So müssen die Punkte “Links”, also an erste Stelle aufgeführt werden, da wir ja Attribute an die Punkte anheften wollen.\nBeachte, dass der Output eine neue Spalte flaechen_typ aufweist. Diese ist leer (NA) wenn sich der entsprechende Baum nicht in einer Wiese befindet. Wie viele Bäume befinden sich in einer Wiese, wie viele nicht?\n\n\nCode\nbaeume_join &lt;- st_join(baeume, wiesen) \n\nanzahl_in_wiese &lt;- sum(!is.na(baeume_join$flaechen_typ))\nanzahl_nicht_in_wiese &lt;- sum(is.na(baeume_join$flaechen_typ))"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html#aufgabe-1-rotmilan-bewegungsdaten-visualisieren",
    "href": "rauman/Rauman2_Uebung_A.html#aufgabe-1-rotmilan-bewegungsdaten-visualisieren",
    "title": "Rauman 2: Übung A",
    "section": "Aufgabe 1: Rotmilan Bewegungsdaten visualisieren",
    "text": "Aufgabe 1: Rotmilan Bewegungsdaten visualisieren\nDie erste Frage, die bei solchen Bewegungsstudien typischerweise gestellt wird, lautet: Wo hält sich das Tier hauptsächlich auf? Um diese Frage zu beantworten, kann man als erstes einfach die Datenpunkte in einer einfachen Karte visualisieren. Erstellt zur Beantwortung dieser Frage nachstehende Karte.\n\n\nCode\nggplot(schweiz) + \n  geom_sf() + \n  geom_sf(data = rotmilan) +\n  theme_void()"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html#aufgabe-2-kernel-density-estimation-berechnen",
    "href": "rauman/Rauman2_Uebung_A.html#aufgabe-2-kernel-density-estimation-berechnen",
    "title": "Rauman 2: Übung A",
    "section": "Aufgabe 2: Kernel Density Estimation berechnen",
    "text": "Aufgabe 2: Kernel Density Estimation berechnen\nIn einer ersten Annäherung funktioniert dies, doch wir sehen hier ein klassisches Problem des “Overplotting”. Das heisst, dass wir durch die Überlagerung vieler Punkte in den dichten Regionen nicht abschätzen können, wie viele Punkte dort effektiv liegen und ggf. übereinander liegen. Es gibt hier verschiedene Möglichkeiten, die Punktdichte klarer zu visualisieren. Eine unter Biologen sehr beliebte Methode ist die Dichteverteilung mit einer Kernel Density Estimation (KDE). Dies v.a. darum, weil mit KDE das Habitat (Streifgebiet) eines Tieres abgeschätzt werden kann. Homeranges werden oft mit KDE95 und Core Areas mit KDE50 definiert (Fleming C., Calabrese J., 2016).\nFür die Berechnung der Density verwenden wir die Funktion density.ppp aus spatstat. Diese library ist etwas komplex in der Anwendung, damit wir dieses Verfahren aber dennoch auf unsere Rotmilan-Daten anwenden können, haben wir eine eigene KDE-Funktion erstellt, die wir Euch zur Verfügung stellen.\nWir ermutigen alle, die dafür Kapazität haben, unsere Function eingehend zu studieren und allenfalls ganz auf die Funktion zu verzichten und stattdessen direkt spatstat zu verwenden. Wenn ihr mit unserer Funktion arbeiten möchtet, müsst ihr den unten stehenden Code in euer Skript kopieren und ausführen.\n\nmy_kde &lt;- function(points, cellsize, bandwith, extent = NULL){\n  library(\"spatstat.geom\")    # um sf in ppp zu konvertieren\n  library(\"spatstat.explore\") # um die Dichte zu berechnen\n  \n  points_ppp &lt;- as.ppp(points) # konvertiert sf &gt; ppp\n\n  if(!is.null(extent)){\n    # falls ein extent angegeben ist, wird dieser verwendet\n    # um das \"Beobachtungsfenster\" zu setzen\n    Window(points_ppp) &lt;- as.owin(st_bbox(extent))\n  } \n\n  # macht eine Dichteschätzung\n  points_density &lt;- density.ppp(x = points_ppp, sigma = bandwith, eps = cellsize)\n\n  # konvertiert den Output in ein DataFrame\n  points_density_df &lt;- as.data.frame(points_density)\n\n  points_density_df\n}\n\nDie Parameter der Funktion sollten relativ klar sein:\n\npoints: Ein Punktdatensatz aus der Class sf\ncellsize: Die Zellgrösse des output-Rasters\nbandwith: Der Suchradius für die Dichteberechnung\nextent (optional): Der Perimeter, in dem die Dichteverteilung berechnet werden soll. Wenn kein Perimeter angegeben wird, wird die “bounding box” von points genutzt.\n\nWenn wir nun mit my_kde() die Dichteverteilung berechnen, erhalten wir ein data.frame mit X und Y Koordinaten sowie eine Spalte value zurück. Nutzt diese drei Spalten mit geom_raster() um eure Daten mit ggplot zu visualisieren (aes(x = X, y = Y, fill = value).\n\nrotmilan_kde &lt;- my_kde(points = rotmilan,cellsize = 1000, bandwith = 10000, extent = schweiz)\n## Error in library(\"spatstat.geom\"): there is no package called 'spatstat.geom'\n\nhead(rotmilan_kde)\n## Error in head(rotmilan_kde): object 'rotmilan_kde' not found\n\n\n\nCode\nggplot() + \n  geom_raster(data = rotmilan_kde, aes(x, y, fill = value)) +\n  geom_sf(data = schweiz, fill = NA) +\n  scale_fill_viridis_c() +\n  theme_void()\n## Error in fortify(data): object 'rotmilan_kde' not found\n\n\nDie Kernel Density Estimation ist nun sehr stark von den tiefen Werten dominiert, da die Dichte in den meisten Zellen unseres Untersuchungsgebiets nahe bei Null liegt. Wie erwähnt sind Wissenschaftler häufig nur an den höchsten 95% der Werte interessiert. Folge folgende Schritte um das Resultat etwas besser zu verantschaulichen:\n\nBerechne die 95. Perzentile aller Werte mit der Funktion quantile und benne diesen q95\nErstelle eine neue Spalte in rotmilan_kde, wo alle Werte tiefer als q95 NA entsprechen\n(Optional): Transformiere die Werte mit log10, um einen differenzierteren Farbverlauf zu erhalten\n\nWir können die tiefen Werte ausblenden, indem wir nur die höchsten 5% der Werte darstellen. Dafür berechnen wir mit raster::quantile die 95. Perzentile aller Werte und nutzen diesen Wert als “Grenzwert” für die Darstellung.\nZusätzlich hilft eine logarithmische Transformation der Werte, die Farbskala etwas sichtbarer zu machen.\n\n\nCode\nq95 &lt;- quantile(rotmilan_kde$value,probs = 0.95)\n## Error in quantile(rotmilan_kde$value, probs = 0.95): object 'rotmilan_kde' not found\n\nrotmilan_kde &lt;- rotmilan_kde |&gt;\n  mutate(\n    value_new = ifelse(value&gt;q95,value,NA),\n    value_new = log10(value_new)\n  )\n## Error in mutate(rotmilan_kde, value_new = ifelse(value &gt; q95, value, NA), : object 'rotmilan_kde' not found\n\nggplot() + \n  geom_raster(data = rotmilan_kde, aes(x, y, fill = value_new)) +\n  geom_sf(data = schweiz, inherit.aes = FALSE, fill = NA) +\n  scale_fill_viridis_c(na.value = NA) +\n  theme_void()\n## Error in fortify(data): object 'rotmilan_kde' not found"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html#aufgabe-3-dichteverteilung-mit-thiessen-polygonen",
    "href": "rauman/Rauman2_Uebung_A.html#aufgabe-3-dichteverteilung-mit-thiessen-polygonen",
    "title": "Rauman 2: Übung A",
    "section": "Aufgabe 3: Dichteverteilung mit Thiessen Polygonen",
    "text": "Aufgabe 3: Dichteverteilung mit Thiessen Polygonen\nThiessen Polygone bieten eine spannende Alternative um Unterschiede in der Dichteverteilung von Punktdatensätzen zu visualisieren. Wir wollen dies nun ausprobieren und konstruieren zum Schluss die Thiessenpolygone für die Rotmilan-Daten für das Untersuchungsgebiet Schweiz. Nutze die Anleitung für das Erstellen von Thiessenpolygonen aus der Übung B um Thiessenpolygone für die Rotmilanpositionen zu erstellen.\n\n\nCode\nthiessenpolygone &lt;- rotmilan |&gt;\n  st_union() |&gt;\n  st_voronoi()\nschweiz &lt;- st_union(schweiz)\n\nthiessenpolygone &lt;- st_cast(thiessenpolygone)\n\nthiessenpolygone_clip &lt;- st_intersection(thiessenpolygone,schweiz)\n\n\n\n\nCode\nggplot() + \n  geom_sf(data = schweiz) + \n  geom_sf(data = thiessenpolygone_clip, fill = NA) + \n  theme_void()\n\n\n\n\n\nAbbildung 54.1: Wenn wir jetzt die Thiessenpolygone (ohne Punkte) darstellen, wird deutlicher, wie die Dichteverteilung im Innern des Clusters aussieht.\n\n\n\n\n\n\n\n\nScherler, Patrick. 2020. „Drivers of Departure and Prospecting in Dispersing Juvenile Red Kites (Milvus milvus).“ Phdthesis, University of Zurich."
  },
  {
    "objectID": "rauman/Rauman2_Uebung_B.html",
    "href": "rauman/Rauman2_Uebung_B.html",
    "title": "Rauman 2: Übung B",
    "section": "",
    "text": "In dieser Übung geht es darum, zwei verschiedene Interpolationsverfahren in R umzusetzen. Im ersten Interpolationsverfahren verwenden wir die inverse distance weighted interpolation, später verwenden wir die nearest neighbour methode. Dazu braucht ihr die folgenden Packages:\n\nlibrary(\"sf\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\n\nlibrary(\"gstat\") # &lt;- ggf. installieren!\n\nWeiter benötigt ihr die nachstehenden Datensätze:\nluftqualitaet &lt;- read_sf(\"datasets/rauman/luftqualitaet.gpkg\")\nschweiz &lt;- read_sf(\"datasets/rauman/schweiz.gpkg\") \nDie Library gstat bietet verschiedene Möglichkeiten, Datenpunkte zu interpolieren, unter anderem auch die inverse distance weighted Methode. Leider ist das Package noch nicht so benutzerfreundlich wie sf: Das Package wird aber aktuell überarbeitet und in mittlerer Zukunft sollte es ebenso einfach zugänglich sein. Damit Ihr Euch nicht mit den Eigenheiten dieser Library umschlagen müsst, haben wir eine Function vorbereitet, die Euch die Verwendung der IDW-Interpolation erleichtern soll.\nWir nehmen Euch damit etwas Komplexität weg und liefern Euch ein pfannenfertiges Werkzeug. Das hat auch Nachteile und wir ermutigen alle, die dafür Kapazität haben, unsere Function eingehend zu studieren und allenfalls ganz auf die Function zu verzichten und stattdessen direkt gstat zu verwenden. Wenn ihr mit unserer Function arbeiten möchtet, müsst ihr den unten stehenden Code in euer Skript kopieren und ausführen.\n\nmy_idw &lt;- function(groundtruth,column,cellsize, nmax = Inf, maxdist = Inf, idp = 2, extent = NULL){\n  library(\"gstat\")\n  library(\"sf\")\n  \n  if(is.null(extent)){\n    extent &lt;- groundtruth\n  }\n  \n  samples &lt;- st_make_grid(extent,cellsize,what = \"centers\")\n  my_formula &lt;- formula(paste(column,\"~1\"))\n  idw_sf &lt;- gstat::idw(formula = my_formula,groundtruth, newdata = samples, nmin = 1, nmax = nmax, maxdist = maxdist, idp = idp)\n  \n  idw_matrix &lt;- cbind(as.data.frame(st_coordinates(idw_sf)),pred = st_drop_geometry(idw_sf)[,1])\n  idw_matrix\n}\n\nNun könnt Ihr mit my_idw() den Datensatz luftqualitaet folgendermassen interpolieren.\n\nmy_idw(groundtruth = luftqualitaet, column = \"value\", cellsize = 10000, extent = schweiz)\n\nFolgende Parameter stehen Euch zur Verfügung:\n\nNotwendige Parameter:\n\ngroundtruth: Punktdatensatz mit den Messwerten (sf-Objekt)\ncolumn: Name der Spalte mit den Messwerten (in Anführungs- und Schlusszeichen)\ncellsize: Zellgrösse des output Rasters\n\nOptionale Parameter\n\nnmax: Maximale Anzahl Punkte, die für die Interpolation berücksichtigt werden sollen. Default: Inf (alle Werte im gegebenen Suchradius)\nmaxdist: Suchradius, welcher für die Interpolation verwendet werden soll. Default Inf (alle Werte bis nmax)\nidp: Inverse Distance Power: die Potenz, mit der der Nenner gesteigert werden soll. Default: 2. Werte werden im Kehrwert des Quadrates gewichtet: \\(\\frac{1}{dist^{idp}}\\).\nextent: Gebiet, für welches die Interpolation durchgeführt werden soll. Wenn nichts angegeben wird (Default NULL), wird die Ausdehnung von groundtruth verwendet.\n\nOuput\n\nder Output der Funktion ist eine data.frame mit 3 Spalten:\n\nX, Y Koordinaten der interpolierten Werte\npred: der Interpolierte Wert\n\n\n\nBeim Output handelt sich hier um einen Raster-ähnlichen Datentyp (siehe Vorlesung Spatial DataScience 1). Diesen können wir mit geom_raster mit ggplot visualisieren. Dafür müsst ihr in aes die X und Y Koordinaten angeben, und der interpolierte Wert mit fill einfärben.\n\nAufgabe 1: Raeumliche Interpolation mit IDW\nRechnet so den IDW für die Luftqualitätsmessungen mit verschiedenen Parametern und visualisiert jeweils die Resultate. Experimentiert mit nmax sowie maxdist. Was stellt ihr fest?\nTips:\n\nWas für Distanzen bei maxdist Sinn machen, könnt ihr dem Output aus der G-Funktion (vorherige Übung) entnehmen\nWählt am Anfang eine etwas Konvervative (grosse) cellsize und verringert diesen nur wenn euer Rechner damit gut klar kommt\nDa der Output aus der Interpolation im gleichen Koordinatenbezugssystem sind wie schweiz.gpkg kann man diese beiden Datensätze im gleichen ggplot darstellen. Dafür müsst ihr die aesthetics (aes()) für jeden Layer einzeln setzen, und nicht auf der Ebene von ggplot().\n\n\n\n\nCode\nlibrary(\"purrr\")\nlibrary(\"tidyr\")\n\nplots &lt;- lapply(1:4, function(idp){\n  idw &lt;- my_idw(groundtruth = luftqualitaet,column = \"value\",cellsize = 1000, nmax = Inf,maxdist = Inf,idp = idp,extent = schweiz)\n  ggplot() +\n    geom_raster(data = idw, aes(X,Y, fill = pred)) +\n    geom_sf(data = schweiz, fill = NA) +\n    geom_sf(data = luftqualitaet, size = 1, shape = 3, alpha = 0.3) +\n    scale_fill_gradientn(colours = rev(RColorBrewer::brewer.pal(11,\"RdYlBu\")),limits = c(0, 60), na.value = NA) +\n    labs(fill = \"μg/m3\",\n         title = paste(\"Inverse Distance Power (IDP):\",idp)) +\n    theme_void() +\n    theme(legend.position = \"bottom\", legend.title = element_blank(),\n      legend.key.width = unit(0.10, 'npc'),\n      legend.key.height = unit(0.02, 'npc'))\n})\n## [inverse distance weighted interpolation]\n## [inverse distance weighted interpolation]\n## [inverse distance weighted interpolation]\n## [inverse distance weighted interpolation]\n\ncowplot::plot_grid(plotlist = plots)\n\nggsave(\"rauman/images/idw.png\", height = 18, width = 20, units = \"cm\")\n\n\n\n\n\nAbbildung 55.1: Stickstoffdioxid (NO2) in μg/m3, Interpoliert über die ganze Schweiz mit der Inverse Distance Weighted Methode. Die verschiedenen Plots zeigen die Veränderung der Interpolation bei steigendem IDP-Wert\n\n\n\n\n\n\n\nAufgabe 2: Interpolation mit Nearest Neighbour\nEine weitere einfache Möglichkeit zur Interpolation bietet die Erstellung eines Voronoi-Diagrammes, auch als Thiessen-Polygone oder Dirichlet-Zerlegung bekannt. sf liefert dazu die Funktion st_voronoi(), die einen Punktdatensatz annimmt und eben um die Punkte die Thiessenpolygone konstruiert. Dazu braucht es lediglich einen kleinen Vorverarbeitungsschritt: sf möchte für jedes Feature, also für jede Zeile in unserem Datensatz, ein Voronoidiagramm. Das macht bei uns wenig Sinn, weil jede Zeile nur aus einem Punkt besteht. Deshalb müssen wir vorher luftqualitaet mit st_union() von einem POINT in ein MULTIPOINT Objekt konvertieren, in welchem alle Punkte in einer Zeile zusammengefasst sind.\n\n\nCode\nluftqualitaet_union &lt;- st_union(luftqualitaet)\n\nthiessenpolygone &lt;- st_voronoi(luftqualitaet_union)\n\n\n\n\n\n\n\nst_voronoi hat die Thiessenpolygone etwas weiter gezogen als wir sie wollen. Dies ist allerdings eine schöne Illustration der Randeffekte von Thiessenpolygonen, die zum Rand hin (wo es immer weniger Punkte hat) sehr gross werden können. Wir können die Polygone auf die Ausdehnung der Schweiz mit st_intersection() clippen. Auch hier braucht es zwei kleine Vorverarbeitungsschritte:\n\nwie vorher müssen wir die einzelnen Kantons-Polygone miteinander verschmelzen. Dies erreichen wir mit st_union(). Wir speichern den Output als schweiz, was als Resultat ein einzelnes Polygon der Schweizergrenze retourniert.\nfür die Thiessen-Polygone machen wir genau das Umgekehrte: st_voronoi() liefert ein einzelnes Feature mit allen Polygonen, welches sich nicht gerne clippen lässt. Mit st_cast() wird die GEOMETRYCOLLECTION in Einzelpolygone aufgeteilt.\n\n\n\nCode\nthiessenpolygone &lt;- st_cast(thiessenpolygone)\n\nthiessenpolygone_clip &lt;- st_intersection(thiessenpolygone,schweiz)\n\n\n\n\n\n\n\nJetzt müssen wir nur noch den jeweiligen Wert für jedes Polygon ermitteln. Dies erreichen wir wieder durch st_join. Auch hier ist noch ein kleiner Vorverarbeitungsschritt nötig: Wir konvertieren das sfc Objekt (nur Geometrien) in ein sf Objekt (Geometrien mit Attributtabelle).\n\n\nCode\nthiessenpolygone_clip &lt;- st_as_sf(thiessenpolygone_clip)\nthiessenpolygone_clip &lt;- st_join(thiessenpolygone_clip,luftqualitaet)\n\n\n\n\nCode\nggplot() + \n  geom_sf(data = schweiz) +\n  geom_sf(data = thiessenpolygone_clip, aes(fill = value)) +\n  geom_sf(data = luftqualitaet) +\n  scale_fill_gradientn(colours = rev(RColorBrewer::brewer.pal(11,\"RdYlBu\"))) +\n  theme_void() +\n  theme(legend.position = \"bottom\", legend.title = element_blank(),\n      legend.key.width = unit(0.10, 'npc'),\n      legend.key.height = unit(0.02, 'npc'))\n\n\n\n\n\nAbbildung 55.2: Stickstoffdioxid (NO2) in μg/m3, Interpoliert über die ganze Schweiz nach der Nearest Neighbour Methode."
  },
  {
    "objectID": "rauman/Rauman2_Uebung_C.html#aufgabe-1",
    "href": "rauman/Rauman2_Uebung_C.html#aufgabe-1",
    "title": "Rauman 2: Übung C (Optional)",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\n\nlibrary(\"sf\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\n\nrotmilan &lt;- read_sf(\"datasets/rauman/rotmilan.gpkg\")\n\nschweiz &lt;- read_sf(\"datasets/rauman/schweiz.gpkg\")\n\nluftqualitaet &lt;- read_sf(\"datasets/rauman/luftqualitaet.gpkg\")\n\n\nggplot(rotmilan) +\n  geom_sf(data = schweiz) +\n  geom_sf(aes(colour = timestamp), alpha = 0.2) +\n  scale_color_datetime(low = \"blue\", high = \"red\")\n\n\n\n\nAbbildung 56.1: Eine solche Visualisierung zeigt dir beispielsweise die räumliche Ausdehnung der Datenpunkte"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_C.html#aufgabe-2",
    "href": "rauman/Rauman2_Uebung_C.html#aufgabe-2",
    "title": "Rauman 2: Übung C (Optional)",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nAls erstes berechnen wir die G-Function für die Rotmilanpositionen:\n\nSchritt 1\nMit st_distance() können Distanzen zwischen zwei sf Datensätze berechnet werden. Wird nur ein Datensatz angegeben, wird eine Kreuzmatrix erstellt wo die Distanzen zwischen allen Features zu allen anderen Features dargestellt werden. Wir nützen diese Funktion zur Berechnung der nächsten Nachbarn.\n\n\nCode\nrotmilan_distanzmatrix &lt;- st_distance(rotmilan)\n\nnrow(rotmilan_distanzmatrix)\n## [1] 2305\nncol(rotmilan_distanzmatrix)\n## [1] 2305\n# zeige die ersten 6 Zeilen und Spalten der Matrix\n# jeder Wert ist 2x vorhanden (vergleiche Wert [2,1] mit [1,2])\n# die Diagonale ist die Distanz zu sich selber (gleich 0)\nrotmilan_distanzmatrix[1:6,1:6] \n## Units: [m]\n##          1         2         3        4        5        6\n## 1     0.00 14362.044 20272.492 35596.07 52519.10 64156.67\n## 2 14362.04     0.000  8149.486 29752.74 44809.10 53775.25\n## 3 20272.49  8149.486     0.000 22580.04 36848.93 45662.55\n## 4 35596.07 29752.737 22580.037     0.00 17223.26 31439.57\n## 5 52519.10 44809.096 36848.926 17223.26     0.00 16499.19\n## 6 64156.67 53775.250 45662.554 31439.57 16499.19     0.00\n\n\n\n\nSchritt 2\nNun wollen wir wissen, wie gross die kürzeste Distanz von jedem Punkt zu seinem nächsten Nachbarn beträgt, also die kürzeste Distanz pro Zeile. Bevor wir diese ermitteln müssen wir die diagonalen Werte noch entfernen, denn diese stellen ja jeweils die Distanz zu sich selber dar und sind immer 0. Danach kann mit apply() eine Funktion (FUN = min) über die Zeilen (MARGIN = 1) einer Matrix (X = rotmilan_distanzmatrix) gerechnet werden. Zusätzlich müssen wir noch na.rm = TRUE setzen, damit NA Werte von der Berechnung ausgeschlossen werden. Das Resultat ist ein Vektor mit gleich vielen Werten wie Zeilen in der Matrix.\n\n\nCode\ndiag(rotmilan_distanzmatrix) &lt;- NA # entfernt alle diagonalen Werte\n\nrotmilan_distanzmatrix[1:6,1:6] \n## Units: [m]\n##          1         2         3        4        5        6\n## 1       NA 14362.044 20272.492 35596.07 52519.10 64156.67\n## 2 14362.04        NA  8149.486 29752.74 44809.10 53775.25\n## 3 20272.49  8149.486        NA 22580.04 36848.93 45662.55\n## 4 35596.07 29752.737 22580.037       NA 17223.26 31439.57\n## 5 52519.10 44809.096 36848.926 17223.26       NA 16499.19\n## 6 64156.67 53775.250 45662.554 31439.57 16499.19       NA\n\nrotmilan_mindist &lt;- apply(rotmilan_distanzmatrix,1,min, na.rm = TRUE)\n\n\n\n\nSchritt 3\nNun müssen wir die Distanzen nach ihrer Grösse sortieren\n\n\nCode\nrotmilan_mindist &lt;- sort(rotmilan_mindist) \n\n\n\n\nSchritt 4\nJetzt berechnen wir die kummulierte Häufigkeit von jeder Distanz berechnen. Die kummulierte Häufikgeit vom ersten Wert ist 1 (der Index des ersten Wertes) dividiert durch die Anzahl Werte insgesamt. Mit seq_along erhalten wir die Indizes aller Werte, mit lenth die Anzahl Werte insgesamt.\n\n\nCode\nkumm_haeufgikeit &lt;- seq_along(rotmilan_mindist) / length(rotmilan_mindist)\n\n\n\n\nSchritt 5\nNun wollen wir die kumulierte Häufigkeit der Werte in einer Verteilungsfunktion (engl: Empirical Cumulative Distribution Function, ECDF) darstellen. Dafür müssen wir die beiden Vektoren zuerst noch in einen Dataframe packen, damit ggplot damit klar kommt.\n\n\nCode\nrotmilan_mindist_df &lt;- data.frame(distanzen = rotmilan_mindist,\n                                  kumm_haeufgikeit = kumm_haeufgikeit)\n\np &lt;- ggplot() + \n  geom_line(data = rotmilan_mindist_df, aes(distanzen, kumm_haeufgikeit)) +\n  labs(x = \"Distanz (Meter)\", y = \"Häufigkeit (kummuliert)\")\n\np\n\n\n\n\n\nLesehilfe:\n\n\nCode\nprob &lt;- 0.95\nres &lt;- quantile(ecdf(rotmilan_mindist_df$distanzen), prob)\nres2 &lt;- quantile(ecdf(rotmilan_mindist_df$distanzen), 0.99)\nxlim &lt;- c(5000, NA)\nylim &lt;- c(.5, .75)\np + \n  geom_segment(aes(x = res, xend = res, y = -Inf, yend = prob), colour = \"lightblue\") +\n  geom_segment(aes(x = -Inf, xend = res, y = prob, yend = prob), colour = \"lightblue\") +\n  geom_point(aes(x = res, y = prob), size =3, colour = \"lightblue\") +\n  ggrepel::geom_label_repel(aes(x = 0, y = prob, label = paste0(prob*100,\"% der Werte...\")),\n                            xlim = xlim, ylim = ylim,  hjust = 0, min.segment.length = 0,fill = \"lightblue\") +\n  ggrepel::geom_label_repel(aes(x = res, y = 0, label = paste0(\"... sind kleiner als \",round(res,0),\"m\")),\n                            xlim = xlim, ylim = ylim, hjust = 0,vjust = 1, fill = \"lightblue\",min.segment.length = 0,inherit.aes = FALSE) +\n  scale_y_continuous(breaks = c(0, .25,.5,.75,prob,1))"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_C.html#aufgabe-3",
    "href": "rauman/Rauman2_Uebung_C.html#aufgabe-3",
    "title": "Rauman 2: Übung C (Optional)",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nFühre nun die gleichen Schritte mit luftqualitaet durch und vergleiche die ECDF-Plots.\n\n\nCode\nluftqualitaet_distanzmatrix &lt;- st_distance(luftqualitaet)\n\ndiag(luftqualitaet_distanzmatrix) &lt;- NA\n\nluftqualitaet_mindist &lt;- apply(luftqualitaet_distanzmatrix,1,min,na.rm = TRUE)\n\nluftqualitaet_mindist &lt;- sort(luftqualitaet_mindist)\n\nkumm_haeufgikeit_luftquali &lt;- seq_along(luftqualitaet_mindist) / length(luftqualitaet_mindist)\n\n\nluftqualitaet_mindist_df &lt;- data.frame(distanzen = luftqualitaet_mindist,\n                                  kumm_haeufgikeit = kumm_haeufgikeit_luftquali)\n\nluftqualitaet_mindist_df$data &lt;- \"Luftqualitaet\"\nrotmilan_mindist_df$data &lt;- \"Rotmilan\"\n\nmindist_df &lt;- rbind(luftqualitaet_mindist_df,rotmilan_mindist_df)\n\nggplot(mindist_df,) + \n  geom_line(aes(distanzen, kumm_haeufgikeit, colour = data)) +\n  labs(x = \"Distanz (Meter)\", y = \"Häufigkeit (kummuliert)\", colour = \"Datensatz\")"
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html#aufgabe-1-morans-i-für-kantone",
    "href": "rauman/Rauman3_Uebung_A.html#aufgabe-1-morans-i-für-kantone",
    "title": "Rauman 3: Übung",
    "section": "Aufgabe 1: Morans \\(I\\) für Kantone",
    "text": "Aufgabe 1: Morans \\(I\\) für Kantone\n\nBerechnung von \\(n\\) (zaehler1)\n\\[I = \\frac{\\color{red}n}{\\sum_{i=1}^{\\color{orange}n} (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^{\\color{orange}n}  w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^{\\color{orange}n}  w_{ij}}\\]\nBeginnen wir mit der Variabel \\(n\\), bzw. der Variabel zaehler1 (rot in der obigen Formel). Dies ist lediglich die Anzahl Messwerte in unserem Datensatz, also die Anzahl Kantone. Wir merken uns n vor, da wir diese noch mehrmals in der Formel antreffen werden (orange).\n\n\nCode\nn &lt;- nrow(zweitwohnung_kanton)\nn\n## [1] 26\n\nzaehler1 &lt;- n\n\n\n\n\nAbweichung vom Mittelwert (nenner1)\n\\[I = \\frac{n}{\\color{red}\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{{\\color{orange}\\sum_{i=1}^n} \\sum_{j=1}^n w_{ij}{\\color{orange}(y_i - \\bar{y})(y_j - \\bar{y})}}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nDieses Summenfunktion bedeutet, das wir eine Schlaufe (einen Loop) durchführen sollen. Vom ersten Kanton (\\(\\sum_{\\color{red}i=1}^n\\)) bist zum letzten Kanton (\\(\\sum_{i=1}^{\\color{red}n}\\)) sollen wir:\nDen Mittleren Ja-Stimmen-Anteil aller Kantone vom Ja-Stimmen-Anteil des jeweiligen Kantons abziehen und das Resultat quadrieren.\nDafür erstellen wir als erstes die Variabel y, sowie ein Vektor der Länge 26 (Anzahl Kantone) um die Resultate zu speichern (hier: result)\n\n\nCode\ny &lt;- zweitwohnung_kanton$ja_in_percent\n\n# Wir erstellen ein Vector der länge 26 (1 pro Kanton) \n# um die Resultate zu speichern.\nresultat &lt;- double(length = n)\n\n# jetzt können wir den For Loop ausführen\nfor (i in 1:n){\n  resultat[i] &lt;- (y[i]-mean(y))^2\n}\n\nresultat\n##  [1] 0.0008726497 0.0029255763 0.0004834762 0.0121020552 0.0045148421\n##  [6] 0.0065361452 0.0057467371 0.0001244131 0.0021889669 0.0001613522\n## [11] 0.0044765772 0.0172266246 0.0052080053 0.0060126495 0.0041475008\n## [16] 0.0007085313 0.0005014960 0.0046799162 0.0001270748 0.0010402720\n## [21] 0.0012336230 0.0011046664 0.0536451330 0.0033645831 0.0043781582\n## [26] 0.0019028888\n\n\nNach der Durchführung aller Iterationen, haben wir 26 Einzelwerte, die wir Summieren müssen.\n\n\nCode\nnenner1 &lt;- sum(resultat)\n\n\n\n\n\n\n\n\nFür aufmerksame\n\n\n\n\n\nDa R per Default vektorisiert funktioniert, ist der obige For Loop streng genommen nicht nötig, wir verwenden ihn aber für didaktische Zwecke. Alternativ ginge die Berechnung von nenner1 auch folgendermassen:\n\n# Von jedem Wert den Durchschnittswert abziehen:\ndy &lt;- y - mean(y)\n\n# Diese Werte müssen wir quadrieren:\ndy_squared &lt;- dy^2\n\n# Und danach die Summe bilden:\nnenner1 &lt;- sum(dy_squared, na.rm = TRUE)\n\n\n\n\n\n\nSumme der Gewichte (nenner2)\n\\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\color{red}\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nIm zweiten Term müssen wir jeweils zwei doppelte Summenzeichen auflösen. Der Nenner des zweiten Terms ist etwas einfacher zu verstehen, deshalb beginnen wir mit diesem.\n\nRäumliche Gewichtung \\(w_{ij}\\)\n\\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n {\\color{orange}w_{ij}}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n {\\color{red}w_{ij}}}\\]\n\\(w\\) beschreibt die räumlichen Gewichte der Kantone (den “Schalter” aus der Vorlesung). \\(w_{ij}\\) ist das Gewicht vom Kanton \\(i\\) im Vergleich zum Kanton \\(j\\). Sind Kantone \\(i\\) und \\(j\\) räumlich nah, gilt ein Gewicht von 1, sind sie weit entfernt, gilt ein Gewicht von 0. Dabei ist die Definition von “räumlich nah” nicht festgelegt. Denkbar wären zum Beispiel folgende Optionen:\n\nDie Kantone müssen sich berühren\nDie Kantone müssen innerhalb einer bestimmten Distanz zueinander liegen\n\nFür beide Optionen gibt es eine implementation im Package sf. st_touches prüft zwischen allen Kantonen, ob sie sich berühren. Mit der Option sparse = TRUE wird eine Kreuzmatrix mit 26 Zeilen und 26 Spalten erstellt, wo jeder Kanton mit jedem anderen verglichen wird. Berühren sie sich, steht in der entsprechenden Stelle der Wert TRUE, was in R gleichbedeutend ist wie 1. Berühren sie sich nicht, steht der Wert FALSE, was gleichbedeutend ist wie 0. Das gleiche gilt für die Funktion st_is_within_distance() wobei dort zusätzlich noch der Distanzparameter angegeben werden muss.\n\n\nCode\n# st_touches berechnet eine Kreuzmatrix aller Objekte\nw &lt;- st_touches(zweitwohnung_kanton, sparse = FALSE)\n\n# Schauen wir uns die ersten 5 Reihen und Zeilen an\nw[1:5, 1:5]\n##       [,1]  [,2]  [,3]  [,4]  [,5]\n## [1,] FALSE FALSE FALSE FALSE  TRUE\n## [2,] FALSE FALSE  TRUE  TRUE FALSE\n## [3,] FALSE  TRUE FALSE FALSE  TRUE\n## [4,] FALSE  TRUE FALSE FALSE  TRUE\n## [5,]  TRUE FALSE  TRUE  TRUE FALSE\n\n\nIn der obigen Matrix ist ersichtlich, dass sich der erste Kanton (erste Zeile) den fünften Kanton (fünfte Spalte) berührt. Um zu überprüfen ob das stimmt, können wir die Namen dieser beiden Kantone aus der Spalte KANTONSNAME herauslesen und mit unseren Geografiekentnissen abgleichen:\n\n\nCode\nzweitwohnung_kanton$KANTONSNAME[c(1,5)]\n## [1] \"Zürich\" \"Schwyz\"\n\n\nDer erste Kanton ist Zürich, der fünfte Schwyz, das scheint also Sinn zu stimmen. Challenge: Erstelle einen Plot mit allen Kantone, die den Kanton Zürich berühren. Tipp: Nutze dafür w[1,].\n\n\nCode\nberuehrt_1 &lt;- w[1, ]\n\nggplot(zweitwohnung_kanton[beruehrt_1, ]) +\n  geom_sf(aes(fill = KANTONSNAME)) +\n  labs(title = \"Welche Kanton berühren den Kanton Zürich (st_touches)\")\n\n\n\n\n\n\n\nDoppelte Summenzeichen\n\\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{{\\color{orange}\\sum_{i=1}^n \\sum_{j=1}^n} w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{{\\color{red}\\sum_{i=1}^n \\sum_{j=1}^n} w_{ij}}\\]\nNun können wir das doppelte Summenzeichen auflösen. Genau wie bei einem einfachen Summenzeichen handelt es sich hier um For Loop, diesmal aber um zwei ineinnader verschachtelte For Loops. Der äussere For loop (\\(\\sum_{i=1}^n\\)) iteriert von 1 bis n mit der Variabel i. Der innere For loop (\\(\\sum_{j=1}^n\\)) iteriert von 1 bis n mit der Variabel j.\n\n\nCode\nresultat_aussen &lt;- integer(length = n)\nfor (i in 1:n){\n  resultat_innen &lt;- integer(length = n)\n  for (j in 1:n){\n    resultat_innen[j] &lt;- w[i, j]\n  }\n  resultat_aussen[i] &lt;- sum(resultat_innen)\n}\nnenner2 &lt;- sum(resultat_aussen)\n\n\n\n\n\nBerechnung des Kreuzprodukts (zaehler2)\n\\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\color{red}\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nJetzt haben wir das fehlende Puzzelstück für die Berechnung von zaehler2. Challenge: Nutze die matrix w in dem verschachtelten For Loop aus Kapitel 57.1.3.2\n\n\nCode\n\nresultat_aussen &lt;- double(length = n)\n\nfor (i in 1:n){\n  resultat_innen &lt;- double(length = n)\n  for (j in 1:n){\n    resultat_innen[j] &lt;- w[i,j] * (y[i]-mean(y))*(y[j]-mean(y))\n  }\n  resultat_aussen[i] &lt;- sum(resultat_innen)\n}\n\nzaehler2 &lt;- sum(resultat_aussen)\n\n\n\n\nAuflösung der Formel\nNun haben wir alle Bestandteile von Morans \\(I\\) Berechnet und müssen diese nur noch Zusammenrechnen.\n\n\nCode\nMI_kantone &lt;- zaehler1/nenner1 * zaehler2/nenner2\n\nMI_kantone\n## [1] 0.3148631\n\n\nDer Global Morans \\(I\\) für die Abstimmungsdaten beträgt auf Kantonsebene also 0.31. Wie interpretiert ihr dieses Resultate? Was erwartet ihr für eine Resultat auf Gemeinde- oder Bezirksebene?"
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html#aufgabe-2-morans-i-für-bezirke-berechnen",
    "href": "rauman/Rauman3_Uebung_A.html#aufgabe-2-morans-i-für-bezirke-berechnen",
    "title": "Rauman 3: Übung",
    "section": "Aufgabe 2: Morans I für Bezirke berechnen",
    "text": "Aufgabe 2: Morans I für Bezirke berechnen\nNun könnt ihr Morans \\(I\\) auf der Ebene der Bezirke und untersuchen, ob und wie sich Morans \\(I\\) verändert. Importiert dazu den Layer bezirk aus dem Datensatz zweitwohnungsinitiative.gpkg. Visualisiert in einem ersten Schritt die Abstimmungsresultate. Tipp: wir verwenden das Package cowplot um zwei ggplots nebeneinander darzustellen. Formuliert nun eine Erwartungshaltung: ist Morans \\(I\\) auf der Ebene Bezirke tiefer oder Höher als auf der Ebene Kantone?\n\n\n\n\n\n\nFür Fortgeschrittene\n\n\n\nErstellt aus dem erarbeiten Workflow eine function um Morans I auf der Basis von einem sf Objekt sowie einer Spalte dessen zu berechnen.\n\n\nCode\nmorans_i &lt;- function(sf_object,col) {\n  library(\"sf\")\n  # Zähler 1 ####################################\n  n &lt;- nrow(sf_object)\n  zaehler1 &lt;- n\n  y &lt;- sf_object[[col]] # &lt;- siehe Kommentar unten\n  ###############################################\n  # Nenner 1 ####################################\n  resultat &lt;- double(length = n)\n  for (i in 1:n){\n    resultat[i] &lt;- (y[i]-mean(y))^2\n  }\n  nenner1 &lt;- sum(resultat)\n  ###############################################\n  # Nenner 2 ####################################\n  w &lt;- st_touches(sf_object,sparse = FALSE)\n  resultat_aussen &lt;- integer(length = n)\n  for (i in 1:n){\n    resultat_innen &lt;- integer(length = n)\n    for (j in 1:n){\n      resultat_innen[j] &lt;- w[i, j]\n    }\n    resultat_aussen[i] &lt;- sum(resultat_innen)\n  }\n  nenner2 &lt;- sum(resultat_aussen)\n  ##############################################\n  # Zähler 2 ###################################\n  resultat_aussen &lt;- double(length = n)\n\n  for (i in 1:n){\n    resultat_innen &lt;- double(length = n)\n    for (j in 1:n){\n      resultat_innen[j] &lt;- w[i,j] * (y[i]-mean(y))*(y[j]-mean(y))\n    }\n    resultat_aussen[i] &lt;- sum(resultat_innen)\n  }\n\n  zaehler2 &lt;- sum(resultat_aussen)\n  ##############################################\n  # Auflösung ##################################\n  MI &lt;- zaehler1/nenner1 * zaehler2/nenner2\n\n  return(MI)\n}\n\n# Kommentar\n# Wir können hier nicht das $ Zeichen verwenden, weil \"col\" ein String ist.\n# Mit der doppelten, eckigen klammer stellen wir sicher, dass y erstens ein\n# Vektor ist (schau dir \"y\" an wenn du nur eine Klammer verwendest)\n\n\n\n\n\n\nCode\nzweitwohnung_bezirke &lt;- read_sf(\"datasets/rauman/zweitwohnungsinitiative.gpkg\", \"bezirk\")\nMI_bezirke &lt;- morans_i(zweitwohnung_bezirke, \"ja_in_percent\")"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A.html#aufgabe-1",
    "href": "rauman/Rauman4_Uebung_A.html#aufgabe-1",
    "title": "Rauman 4: Übung A",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nAls erster Schritt müssen wir jeden Baum mit einem 20m Puffer verstehen. Nutze dazu st_buffer um speichere den Output als baeume_20m. Schau dir baeume_20m nun genau an. Um welchen Geometrietyp handelt es sich dabei nun?\n\n\nCode\nbaeume_20m &lt;- st_buffer(baeume_sample, 20)\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = wiesen) +\n  geom_sf(data = baeume_sample) +\n  geom_sf(data = baeume_20m, fill = NA)\n\n\n\n\n\nAbbildung 58.1: Dargestellt sind die Bäume als Punkte mit einem 20m Puffer, sowie die Wiesen im Hintergrund."
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A.html#aufgabe-2",
    "href": "rauman/Rauman4_Uebung_A.html#aufgabe-2",
    "title": "Rauman 4: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nBerechnen nun die Schnittmenge aus baeume_20m und wiesen mit der Funktion st_intersection und speichere den Output als baeume_wiesen. Exploriere nun baeume_wiesen. Was ist passiert? Überprüfe die Anzahl Zeilen pro Datensatz. Haben die sich verändert? Wenn ja, warum?\n\n\nCode\nbaeume_wiesen &lt;- st_intersection(baeume_20m, wiesen) \n\nggplot() +\n  geom_sf(data = wiesen, fill = \"blue\", alpha = .2) +\n  geom_sf(data = baeume_20m, fill = \"red\", alpha = .2) +\n  geom_sf(data = baeume_wiesen, fill = \"green\", alpha = 0.2)"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A.html#aufgabe-3",
    "href": "rauman/Rauman4_Uebung_A.html#aufgabe-3",
    "title": "Rauman 4: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nBerechnen nun die Flächengrösse pro Geometrie mit der Funktion st_area(). Speichere den Output in einer neuen Spalte von baeume_wiesen (z.B. mit dem Namen wiesen_flaeche). Tipp: Konvertiere den Output aus st_area in einen nummerischen Vektor mit as.numeric().\n\n\nCode\nbaeume_wiesen$wiesen_flaeche &lt;- as.numeric(st_area(baeume_wiesen))"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A.html#aufgabe-4-optional",
    "href": "rauman/Rauman4_Uebung_A.html#aufgabe-4-optional",
    "title": "Rauman 4: Übung A",
    "section": "Aufgabe 4 (Optional)",
    "text": "Aufgabe 4 (Optional)\nBerechne nun aus wiesen_flaeche den wiesen_anteil. Tipp: 100% ist die Kreisfläche aus \\(r^2\\times \\pi\\), wobei in unserem Fall \\(r = 20\\) entspricht.\n\n\nCode\nkreisflaeche &lt;- 20^2*pi\nbaeume_wiesen$wiesen_anteil &lt;- baeume_wiesen$wiesen_flaeche/kreisflaeche\n\n\nÜberführe anschliessend die berechneten Anteilswerte in den Datensatz baeume mit einem left_join zwischen baeume und baeume_wiesen. Welche Spalte wäre für diesen Join geeignet? Hinweis: Nutze st_drop_geometry() um die Geometriespalte in baeme_wiesen vor dem Join zu entfernen.\n\n\nCode\nbaeume_wiesen_df &lt;- st_drop_geometry(baeume_wiesen)\n\nbaeume_2 &lt;- left_join(baeume_sample, baeume_wiesen_df, by = \"baum_id\")\n\nggplot() +\n  geom_sf(data = wiesen) +\n  geom_sf(data = baeume_2, aes(colour = wiesen_anteil)) +\n  scale_color_binned(\"Wiesen Anteil\",low = \"blue\", high = \"red\", limits = c(0,1), label = scales::label_percent()) +\n  coord_sf(datum = 2056)\n\n\n\n\n\nAbbildung 58.2: Nach dieser Übung kannst du das Resultat in dieser Weise visualisieren."
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A.html#sec-raster-intro1",
    "href": "rauman/Rauman4_Uebung_A.html#sec-raster-intro1",
    "title": "Rauman 4: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nNun habt ihr ein paar Vektoroperationen wie st_buffer() und st_intersection() und st_area() durchgeführt. Gewisse Fragen lassen sich aber besser in der Raster-Welt beantworten. Wollen wir beispielsweise für jeden Punkt im Raum wissen, wie weit der nächstgelegene Baum ist, lässt sich das besser in einem Raster darstellen.\nBevor wir die Frage aber beantworten können, müssen wir den Vektordatensatz in ein Rasterdatensatz konvertieren. Dafür wiederum braucht es ein Raster “Template”, damit R in etwa weiss, wie der Raster Output auszusehen hat.\n\n\nCode\n# Um mit Raster arbeiten zu können brauchen wir das Package \"terra\"\nlibrary(\"terra\")\n\n# Um ein Vektor Datensatz zu vektorieren, brauchen wir ein Template.\n# Für das Template nutzen wir \"wiesen\" und setzen eine Zellgrösse (resolution)\ntemplate &lt;- rast(wiesen, resolution = 20)\n\n# Mit rasterize können wir \"baeume\" in einen Raster konvertieren\n# Nutzt hier wieder alle bäume, nicht baeume_sample\nbaeume_rast &lt;- terra::rasterize(baeume, template)\n\n\nDer Unterschied zwischen Raster und Vektor kann sehr anschaulich dargestellt werden, wenn die beiden Datensätze übereinander gelagert werden.\n\n\nCode\nplot(baeume_rast, col = \"grey\")\nplot(baeume, add = TRUE, col = \"red\", pch = \"x\")\n\n\n\n\n\nMit baeume_rast können wir nun mit der Funktion distance() die Distanz zu jedem Baum berechnen:\n\n\nCode\nbaeume_dist &lt;- distance(baeume_rast)\nplot(baeume_dist)\nplot(baeume, add = TRUE, pch = \"x\")"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_B.html#aufgabe-1",
    "href": "rauman/Rauman4_Uebung_B.html#aufgabe-1",
    "title": "Rauman 4: Übung B",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nIn dieser Übung werden wir weiter mit terra arbeiten, um zu zeigen, wie wir einen Rasterdatensatz importieren, visualisieren und weiter verarbeiten können. Unter dem nachstehenden Link können Sie eine tif-Datei herunterladen, die das “Digitale Höhenmodell” (DHM)* des Kantons Schwyz darstellt. Laden Sie den Datensatz herunter und führen Sie den angegebenen Code aus.\nDatensatz: dhm250m.tif\n\nlibrary(\"terra\")\n\nImportieren Sie Ihr Raster mit der Funktion rast\n\ndhm_schwyz &lt;- rast(\"datasets/rauman/dhm250m.tif\")\n\nSie erhalten einige wichtige Metadaten über den Rasterdatensatz, wenn Sie den Variablennamen in die Konsole eingeben.\n\ndhm_schwyz \n## class       : SpatRaster \n## dimensions  : 150, 186, 1  (nrow, ncol, nlyr)\n## resolution  : 250, 250  (x, y)\n## extent      : 2672175, 2718675, 1193658, 1231158  (xmin, xmax, ymin, ymax)\n## coord. ref. : CH1903+ / LV95 (EPSG:2056) \n## source      : dhm250m.tif \n## name        :   dhm250m \n## min value   :  389.1618 \n## max value   : 2850.0203\n\nUm einen schnellen Überblick eines Rasterdatensatz zu erhalten, können wir einfach die plot() Funktion verwenden.\n\n\nCode\nplot(dhm_schwyz)\n\n\n\n\n\nLeider ist das Verwenden von Rastern in ggplot nicht sehr einfach. Da ggplot ein universelles Plot-Framework ist, stossen wir schnell an die Grenzen des Möglichen, wenn wir etwas so Spezielles wie Karten erstellen. Mit plot können wir zwar sehr schnell plotten, aber auch hier stossen wir schnell an Grenzen.\nAus diesem Grund werden wir ein neues Plot-Framework einführen, das auf Karten spezialisiert ist und in einem sehr ähnlichen Design wie ggplot gebaut wurde: tmap. Laden Sie dieses Paket jetzt in Ihre Session:\n\nlibrary(\"tmap\")\n\nGenau wie ggplot basiert tmap auf der Idee von “Ebenen”, die durch ein + verbunden sind. Jede Ebene hat zwei Komponenten:\n\neine Datensatzkomponente, die immer tm_shape(dataset) ist (ersetzen Sie dataset durch Ihre Variable)\neine Geometriekomponente, die beschreibt, wie das vorangegangene tm_shape() visualisiert werden soll. Dies kann tm_dots() für Punkte, tm_polygons() für Polygone, tm_lines() für Linien usw. sein. Für Einzelbandraster (was bei dhm_schwyz der Fall ist) ist es tm_raster()\n\n\n\nCode\ntm_shape(dhm_schwyz) + \n  tm_raster() \n\n\n\n\n\nBeachten Sie, dass tm_shape() und tm_raster() (in diesem Fall) zusammengehören. Das eine kann nicht ohne das andere leben.\nWenn Sie die Hilfe von ?tm_raster konsultieren, werden Sie eine Vielzahl von Optionen sehen, mit denen Sie die Visualisierung Ihrer Daten verändern können. Zum Beispiel ist der Standardstil von tm_raster() die Erstellung von “Bins” mit einer diskreten Farbskala. Wir können dies mit style = \"cont\" ausser Kraft setzen.\n\n\nCode\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\") \n\n\n\n\n\nDas sieht schon ziemlich toll aus, aber vielleicht wollen wir die Standard-Farbpalette ändern. Glücklicherweise ist das in tmap viel einfacher als in ggplot2. Um sich die verfügbaren Paletten anzusehen, geben Sie tmaptools::palette_explorer() oder RColorBrewer::display.brewer.all() in der Konsole ein (für Ersteres müssen Sie möglicherweise zusätzliche Pakete installieren, z.B. shinyjs).\n\n\nCode\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\", palette = \"Spectral\") \n\n\n\n\n\nEine grosse Stärke von tmap ist die Tatsache, das mit dem gleichen Befehl sowohl statische wie auch interative Plots erstellt werden können. Dafür muss der Modus von statisch auf interaktiv gewechselt werden.\n\n\nCode\ntmap_mode(\"view\") # wechselt auf interakive Plots\n\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\", palette = \"Spectral\")\n\n\n\n\n\n\nCode\n\ntmap_mode(\"plot\")   # wechselt zurück auf statische Plots"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_B.html#sec-raster-slope",
    "href": "rauman/Rauman4_Uebung_B.html#sec-raster-slope",
    "title": "Rauman 4: Übung B",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nMit terra können wir eine Vielzahl von Rasteroperationen über unser Höhenmodell laufen lassen. Eine klassische Rasteroperation ist zum Beispiel das Berechnen der Hangneigung (“slope”) oder dessen Orientierung (“aspect”). Nutzen Sie die Funktion terrain() aus terra um die Hangneigung und Orientierung zu berechnen. Visualisieren Sie die Resultate.\n\n\nCode\nterrain(dhm_schwyz, \"slope\") |&gt;\n plot()\n\n\n\n\n\n\n\nCode\nschwyz_aspect &lt;- terrain(dhm_schwyz, \"aspect\")\n\nplot(schwyz_aspect)\n\n\n\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nBei “aspect” handelt es sich ja um Werte, die von 0 bis 360 reichen. In klassischen Palettes liegen die beiden Extremwerte (in diesem Fall 0 und 360) farblich weit auseinander. Bei Aspect sollten diese aber nahe beieinander liegen (da eine Ausrichtung von 1° nur 2 Grad von einer Ausrichtung von 359° entfernt ist). Um dieser Tatsache Rechnung zu Tragen können wir eine eine eigene Colourpalette erstellen, wo die erste Farbe wiederholt ist.\n\n\n\n\nCode\ntm_shape(schwyz_aspect) + \ntm_raster(\n  palette = c(\"#EF476F\",\"#FFD166\",\"#06D6A0\", \"#118AB2\",\"#EF476F\"), \n  style = \"cont\", breaks = seq(0,360, 90)\n  )"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_B.html#aufgabe-3",
    "href": "rauman/Rauman4_Uebung_B.html#aufgabe-3",
    "title": "Rauman 4: Übung B",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nMit Hangneigung und -ausrichtung können wir einen Hillshading-Effekt berechnen. Hillshading bedeutet, dass der Schattenwurf des Oberflächenmodells bei gegebenen Einfallswinkel der Sonne (Höhe und Azimut) berechnet wird. Der typische Einfallswinkel liegt bei 45° über dem Horizont und von Nordwesten bei 315°.\nUm einen Hillshading Effekt zu erzeugen, berechne zuerst slope und aspect von dhm_schwyz analog der letzten Aufgabe, achte aber darauf das die Einheit radians entspricht. Nutze diese beiden Objekte in der Funktion shade() um den Hillshade zu berechnen. Visualisiere den Output anschliessend mit plot oder tmap.\n\n\nCode\ndhm_slope &lt;- terrain(dhm_schwyz, \"slope\", unit = \"radians\")\ndhm_aspect &lt;- terrain(dhm_schwyz, \"aspect\", unit = \"radians\")\n\ndhm_hillshade &lt;- shade(dhm_slope, dhm_aspect, 45, 315)\n\ntm_shape(dhm_hillshade) + \n  tm_raster(style = \"cont\", palette = \"cividis\", legend.show = FALSE) +\n  tm_layout(frame = FALSE)\n\n\n\n\n\nFür diese Visualisierung verwende ich tmap und als colour palette cividis"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html#aufgabe-1-vektor-daten-laden-und-anzeigen",
    "href": "rauman/Rauman5_Uebung_A.html#aufgabe-1-vektor-daten-laden-und-anzeigen",
    "title": "Rauman 5: Übung A",
    "section": "Aufgabe 1: Vektor Daten laden und anzeigen",
    "text": "Aufgabe 1: Vektor Daten laden und anzeigen\nLaden Sie das File windkraft_geodata.gpkg von Moodle herunter (siehe Tabelle 60.1). Dieses beinhaltet alle Vektordaten, die für die Bearbeitung der Multikriterien-Evaluation benötigt wird (Bewohnte Flächen, Nationale Schutzgebiete, Seeflächen, Strassen, Waldgebiete sowie die Kantonsgrenze von Schwyz). Die Namen der verfügbaren Listen können Sie mit sf::st_layers() ermitteln.\nImportiere die benötigten Vektordatensätze und exploriere die Daten. Zur Visualisierung könnt ihr die Funktionen plot oder die Packages tmap oder ggplot2 verwenden.\nSchau dir auch das Koordinatensystem an. Was fällt dir auf? Wir würden gerne mit dem neuen Schweizer Koordinatensystem arbeiten (LV95). Um ein Koordinatensystem umzuwandeln benutze die Funktion st_transform().\n\n\nCode\ngpkg_path  &lt;- \"datasets/rauman/windkraft_geodata.gpkg\"\n\n#Vector data\nst_layers(gpkg_path)\n## Driver: GPKG \n## Available layers:\n##                   layer_name     geometry_type features fields      crs_name\n## 1           Bewohnte_Flaeche     Multi Polygon      326      1 CH1903 / LV03\n## 2    Nationale_Schutzgebiete     Multi Polygon        1      1 CH1903 / LV03\n## 3                Seeflaechen     Multi Polygon      205      5 CH1903 / LV03\n## 4                   Strassen Multi Line String    28682     13 CH1903 / LV03\n## 5 Untersuchungsgebiet_Schwyz     Multi Polygon        1      1 CH1903 / LV03\n## 6                Waldgebiete     Multi Polygon     5580      3 CH1903 / LV03\n\nkt_schwyz &lt;- read_sf(gpkg_path, \"Untersuchungsgebiet_Schwyz\") |&gt; st_transform(2056)\nschutzgebiete &lt;- read_sf(gpkg_path, \"Nationale_Schutzgebiete\") |&gt; st_transform(2056)\nstrassen &lt;- read_sf(gpkg_path, \"Strassen\") |&gt; st_transform(2056)"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html#sec-raster-distance",
    "href": "rauman/Rauman5_Uebung_A.html#sec-raster-distance",
    "title": "Rauman 5: Übung A",
    "section": "Aufgabe 2: Erschliessung berechnen",
    "text": "Aufgabe 2: Erschliessung berechnen\nBeginnen wir mit dem Kriterium “Erschliessung”. Wir müssen für den ganzen Kanton Schwyz wissen, wie weit die nächste Strasse entfernt ist. Wie wir bereits in Kapitel 58.5 erläutert haben, lässt sich diese Information am besten in einem Raster abbilden.\nAnalog Kapitel 58.5 müssen wir hierfür den Vektordatensatz auf der Basis eines Templates in ein Raster konvertieren. Für die Erstellung des Templates verwenden wir an dieser Stelle die Kantonsgrenze vom Kanton Schwyz.\n\n\nCode\n# Template mit der Ausdehnung und dem CRS vom Kt. kt_schwyz\n# erstellen, und mit einer Auflösung on 250m\ntemplate &lt;- rast(kt_schwyz, resolution = 250)\n\nstrassen_raster &lt;- rasterize(strassen, template)\nstrassen_dist &lt;- distance(strassen_raster)\n\n\nNutze der obige Code um den Wald Datensatz zu rasterisieren und die Distanz zum Wald mit der Funktion distance() zu berechnen. Plausibilisiere den Output indem du ihn visualisierst.\n\n\nCode\nplot(strassen_raster, col = \"green\")\nplot(strassen_dist)\n\n\n\n\n\n\n\nDie rasterisierte Form des “Strassen” Datensatzes\n\n\n\n\n\n\n\nDie Distanz zur nächstgelegenen Strasse für jeden Punkt im Kanton Schwyz”\n\n\n\n\n\n\nFühre nun die gleiche Operation durch um die Entfernung zu nationalen Schutzgebieten zu ermitteln.\n\n\nCode\nschutzgebiete_raster &lt;- rasterize(schutzgebiete, template)\nschutzgebiete_dist &lt;- distance(schutzgebiete_raster)\n\nplot(schutzgebiete_raster, col = \"green\")\nplot(schutzgebiete_dist)\n\n\n\n\n\n\n\nDie rasterisierte Form des “Schutzgebiete” Datensatzes\n\n\n\n\n\n\n\nDie Distanz zum nächstgelegenen Schutzgebiet für jeden Punkt im Kanton Schwyz”"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html#sec-classify1",
    "href": "rauman/Rauman5_Uebung_A.html#sec-classify1",
    "title": "Rauman 5: Übung A",
    "section": "Aufgabe 3: Distanzkriterien Bewerten",
    "text": "Aufgabe 3: Distanzkriterien Bewerten\nJetzt haben wir die Distanzen zu den relevanten Gebieten berechnet, nun müssen wir diese Distanzen bewerten. Dafür teilen wir die kontinuierlichen Distanzwerte in diskrete Kategorien ein. Wir verwenden für die Aufgabe folgende Einteilung:\nHierfür brauchen wir die Function classify(). Wie wir aus der Dokumentation der Funktion (mit ?classify) entnehmen können, gibt es verschiedene Wege wie wir einen Raster Reklassifizieren können (siehe Beschreibung für das Argument rcl). Eine sehr explizite Variante ist, dass wir für rcl eine Matrix mit 3 Spalten verwenden. Diese drei Spalte stellen from, to und becomes dar.\nWir könnten diese Tabelle in einem Spreadsheet-Programm schreiben und in R einlesen. Alternativ können wir sie auch “von Hand” in R erstellen. Um in R tabellarische Daten zu schreiben empfehlen wir die Funktion tribble(), welche eine sehr anschauliche Art bietet, Tabellen in R zu generieren.\n\n\nCode\nstrassen_klassen &lt;- tribble(\n  ~von, ~bis, ~zu,\n     0,  250, 1.0,\n   250,  500, 0.8,\n   500,  750, 0.6,\n   750, 1000, 0.4,\n  1000, 1250, 0.2,\n  1250,  Inf, 0.0\n) \n\n# Mit geom_rect können wir unsere Distanz klassen visualisieren\nggplot(strassen_klassen, aes(xmin = von, xmax = bis, ymax = zu, fill = zu)) +\n  geom_rect(ymin = 0) +\n  scale_x_continuous(breaks = strassen_klassen$bis) +\n  scale_y_continuous(breaks = strassen_klassen$zu)\n\n\n\n\n\nCode\n\n# tribble erstellt eine data.frame, \n# wir brauchen aber eine matrix\nstrassen_klassen &lt;- as.matrix(strassen_klassen)  \n\n\nJetzt wo wir diese Matrix haben, können wir sie nutzen um den Kanton Schwyz hinsichtlich der Distanz zum Wald zu bewerten. Dafür verwenden wir die Funktion classify() mit dem Argument include.lowest = TRUE damit eine Distanz von 0m ebenfalls in 1 reklassifiziert wird.\n\n\nCode\nstrassen_classify &lt;- classify(strassen_dist, strassen_klassen,  include.lowest = TRUE)\n\n# Visualisierung des Resultats\ntm_shape(strassen_classify) +\n  tm_raster(palette = \"-Spectral\") + \n  tm_layout(legend.outside = TRUE) +\n  tm_shape(strassen) + \n  tm_lines()\n\n\n\n\n\nBewerte auf die gleiche Art die Distanz zu den Schutzgebieten. Wir nutzen die Schwellwerte, wie sie in der der nachstehenden Tabelle ersichtlich ist. Du kannst diese aber frei wählen.\n\n\nCode\nschutzgebiete_klassen &lt;- tribble(\n  ~von, ~bis, ~zu,\n     0,  250, 0.0,\n   250,  500, 0.2,\n   500,  750, 0.4,\n   750, 1000, 0.6,\n  1000, 1250, 0.8,\n  1250,  Inf, 1.0\n) \n\nschutzgebiete_klassen &lt;- as.matrix(schutzgebiete_klassen)  \n\nschutzgebiete_classify &lt;- classify(schutzgebiete_dist, schutzgebiete_klassen)\n\n\n\n\n\n\n\nvon\nbis\nzu\n\n\n\n\n0\n250\n0.0\n\n\n250\n500\n0.2\n\n\n500\n750\n0.4\n\n\n750\n1000\n0.6\n\n\n1000\n1250\n0.8\n\n\n1250\nInf\n1.0"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html#sec-raster-overlay1",
    "href": "rauman/Rauman5_Uebung_A.html#sec-raster-overlay1",
    "title": "Rauman 5: Übung A",
    "section": "Aufgabe 4: Raster Overlay",
    "text": "Aufgabe 4: Raster Overlay\nWir haben zwar erst zwei der Kriterien berechnet, die wir für unsere Standortsuche berücksichtigen wollen, doch mit denen können wir schon mal eine erste, unvollständige Beurteilung wagen.\nWeil wir für alle Raster das gleiche Template verwendet haben, sind diese perfekt aneinander ausgerichtet. So können wir auf die denkbar einfachste Art die einezelnen Zellen miteinander verrechnen. Auf folgende Weise können wir beispielsweise den Mittlwert pro Zelle berechnen:\n\n\nCode\noverlay_prelim_1 &lt;- (strassen_classify + schutzgebiete_classify)/2\n\ntm_shape(overlay_prelim_1) + \n  tm_raster(palette = \"-Spectral\") +\n  tm_shape(kt_schwyz) + \n  tm_borders(lwd = 5, col = \"black\")"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html#sec-rauman5-mask",
    "href": "rauman/Rauman5_Uebung_A.html#sec-rauman5-mask",
    "title": "Rauman 5: Übung A",
    "section": "Aufgabe 5: Mask raster",
    "text": "Aufgabe 5: Mask raster\nIm letzten Plot fällt auf, dass wir auch eine Bewertung für Gebiete ausserhalb des Untersuchungsgebiets haben. Da wir für diese Gebiete keine Geodaten verwendet haben, sind die Resultate ausserhalb des Untersuchungsgebiets nicht gültig. Deshalb ist es sinnvoll, die Werte ausserhalb des Untersuchungsgebeits zu entfernen. Dafür verwenden wir die Funktion mask() zusammen mit dem Vektordatensatz kt_schwzy. Diese setzt alle Werte ausserhalb des Polygons zu NA:\n\n\nCode\noverlay_prelim_1 &lt;- mask(overlay_prelim_1, kt_schwyz)\n\ntm_shape(overlay_prelim_1) + \n  tm_raster(palette = \"-Spectral\")"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_A.html#sec-raster-overlay1b",
    "href": "rauman/Rauman5_Uebung_A.html#sec-raster-overlay1b",
    "title": "Rauman 5: Übung A",
    "section": "Aufgabe 6: Weighted Raster Overlay",
    "text": "Aufgabe 6: Weighted Raster Overlay\nIn im obigen Raster Overlay haben wir alle Kriterien gleich stark gewichtet. Wir können aber auch eine gewichtete Verschneidung durchführen: Wenn wir beispielsweise die Distanz zu den Strassen stärker gewichten wollen als die Distanz zu den Schutzgebieten. Auch das ist sehr einfach:\n\n\nCode\n# Wir dividieren durch die Summe der Gewichte, \n# um wieder Werte zwischen 0 und 1 zu erhalten.\noverlay_prelim_2 &lt;- (strassen_classify*5 + schutzgebiete_classify*1)/(5+1)\n\n# Resultate ausserhalb des Kantons entfernen:\noverlay_prelim_2 &lt;- mask(overlay_prelim_2, kt_schwyz)\n\n# Resultate visualisieren:\ntm_shape(overlay_prelim_2) + \n  tm_raster(palette = \"-Spectral\")\n\n\n\n\n\n\n\n\n\nGilgen, Kurt, und Alma Sartoris. 2010. „Empfehlung zur Planung von Windenergieanlagen: Die Anwendung von Raumplanungsinstrumenten und Kriterien zur Standortwahl“. Eidgenössisches Departement für Umwelt, Verkehr, Energie und Kommunikation UVEK.\n\n\nTegou, Leda-Ioanna, Heracles Polatidis, und Dias A. Haralambopoulos. 2010. „Environmental management framework for wind farm siting: Methodology and case study“. Journal of Environmental Management 91 (11): 2134–47. https://doi.org/10.1016/j.jenvman.2010.05.010."
  },
  {
    "objectID": "rauman/Rauman5_Uebung_B.html#aufgabe-1-rasterdaten-einlesen",
    "href": "rauman/Rauman5_Uebung_B.html#aufgabe-1-rasterdaten-einlesen",
    "title": "Rauman 5: Übung B",
    "section": "Aufgabe 1: Rasterdaten einlesen",
    "text": "Aufgabe 1: Rasterdaten einlesen\nZur Bewertung der Standorte hinsichtlich Windgeschwindigkeit steht uns folgender wind250m.tif zur Verfügung (siehe Tabelle 60.1). Lade den Datensatz herunter und mit der Funktion rast() in R ein. Explorieren Sie den Datensatz visuell und versuchen Sie ein Verständnis für die Datensätze zu bekommen.\n\n\nCode\nwind250m &lt;- rast(\"datasets/rauman/wind250m.tif\")\n\n\n\n\nCode\ntm_shape(wind250m) + \n  tm_raster(style = \"cont\") \n\n\n\n\n\nDatensatz ‘wind250m’ zur Windgeschwindigkeit in m pro Sekunde"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_B.html#aufgabe-2-wind-bewerten",
    "href": "rauman/Rauman5_Uebung_B.html#aufgabe-2-wind-bewerten",
    "title": "Rauman 5: Übung B",
    "section": "Aufgabe 2: Wind bewerten",
    "text": "Aufgabe 2: Wind bewerten\nDiese Rasterdaten müssen wir nicht weiter verarbeiten, wir können sie direkt bewerten. Führen Sie diese Bewertung aufgrund nachstehender Tabelle durch. Nutzen Sie dafür die Funktion classify() analog Kapitel 60.3. Sie können die Schwellwerte frei wählen, wir werden diejenigen verwenden, die in Tabelle 61.1 festgehalten sind.\n\n\nCode\n#### reclassify wind\n\nwind_klassen &lt;- tribble(\n  ~von, ~bis, ~zu,\n    0,  20, 0.0,\n   20,  30, 0.2,\n   30,  40, 0.4,\n   40,  50, 0.6,\n   50,  60, 0.8,\n   60, Inf, 1.0\n) \n\nwind_klassen &lt;- as.matrix(wind_klassen)\n\nwind_classify &lt;- classify(wind250m, wind_klassen)"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_B.html#aufgabe-3-slope-berechnen-und-bewerten",
    "href": "rauman/Rauman5_Uebung_B.html#aufgabe-3-slope-berechnen-und-bewerten",
    "title": "Rauman 5: Übung B",
    "section": "Aufgabe 3: Slope berechnen und Bewerten",
    "text": "Aufgabe 3: Slope berechnen und Bewerten\nFür die Berechnung und anschilessende Bewertung der Hangneigung brauchen wir ein Höhenmodell. Lade das Höhenmodell dhm250m.tif herunter (siehe Tabelle 60.1) und in R ein. Berechne anschliessend die Hangneigung mit der Funktion terrain() analog Kapitel 59.2 (beachten Sie die Einheit des Output!).\nBewerten Sie die Hangneigung danach gemäss Tabelle Tabelle 61.1.\n\n\nCode\ndhm250m &lt;- rast(\"datasets/rauman/dhm250m.tif\")\n\nneigung &lt;- terrain(dhm250m, v=\"slope\", unit=\"degrees\")\n\n#### reclassify slope\nneigung_klassen &lt;- tribble(\n  ~von, ~bis, ~zu,\n    0,   4, 1.0,\n    4,   8, 0.8,\n    8,  12, 0.6,\n   12,  16, 0.4,\n   16,  20, 0.2,\n   20,  90, 0.0\n) \n\nneigung_klassen &lt;- as.matrix(neigung_klassen)\n\nneigung_classify &lt;- classify(neigung, neigung_klassen)\n\n\n\n\n\n\n\nTabelle 61.1: Bewertungstabelle die Windgeschwindigkeit (m/s), Vereisungshäufigkeit (Tage/Jahr) und Hangneigung (Grad)\n\n\n\n\n\n\n\n\n\n\n\nWindgeschwindigkeit\n\n\nHangneigung\n\n\n\nvon\nbis\nzu\nvon\nbis\nzu\n\n\n\n\n0\n20\n0.0\n0\n4\n1.0\n\n\n20\n30\n0.2\n4\n8\n0.8\n\n\n30\n40\n0.4\n8\n12\n0.6\n\n\n40\n50\n0.6\n12\n16\n0.4\n\n\n50\n60\n0.8\n16\n20\n0.2\n\n\n60\nInf\n1.0\n20\n90\n0.0"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_B.html#sec-raster-overlay2",
    "href": "rauman/Rauman5_Uebung_B.html#sec-raster-overlay2",
    "title": "Rauman 5: Übung B",
    "section": "Aufgabe 4: Raster Overlay",
    "text": "Aufgabe 4: Raster Overlay\nAnalog Kapitel 60.4 können wir an dieser Stelle eine vorläufige Beurteilung der Gebiete durchführen.\n\n\nCode\noverlay_prelim_3 &lt;- (strassen_classify + schutzgebiete_classify + wind_classify + neigung_classify)/4\n\ntm_shape(overlay_prelim_3) + \n  tm_raster(palette = \"-Spectral\", breaks = seq(0,1,0.2), style = \"cont\", title = \"Eignung\") +\n  tm_layout(frame = FALSE)\n\n\n\n\n\nAbbildung 61.1: Ungewichtetes Überlagern aller Kriterien mit Ausnahme der Ausschlussgebiete"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_B.html#aufgabe-5-ausschlusskriterien",
    "href": "rauman/Rauman5_Uebung_B.html#aufgabe-5-ausschlusskriterien",
    "title": "Rauman 5: Übung B",
    "section": "Aufgabe 5: Ausschlusskriterien",
    "text": "Aufgabe 5: Ausschlusskriterien\nAls Auschlussgebiete gelten Flächen, wo keine Windkraftanlagen gebaut werden können. Dazu gehören bewohnte Flächen, nationale Schutzgebiete, Waldgebiete und Seen. (Zwar werden Schutzgebiete in unserer Analyse bereits berücksichtigt, aber nicht kategorisch vom Resultat ausgeschlossen.)\n\n\nCode\nschutzgebiete &lt;- read_sf(gpkg_path, \"Nationale_Schutzgebiete\")  |&gt; st_transform(2056)\nsiedlungsgebiet &lt;- read_sf(gpkg_path, \"Bewohnte_Flaeche\")  |&gt; st_transform(2056)\nwald &lt;- read_sf(gpkg_path, \"Waldgebiete\")  |&gt; st_transform(2056)\nseen &lt;- read_sf(gpkg_path, \"Seeflaechen\")  |&gt; st_transform(2056)\nkt_schwyz &lt;- read_sf(gpkg_path, \"Untersuchungsgebiet_Schwyz\")  |&gt; st_transform(2056)\n\n\nUm diese Flächen aus von unserem Resultat auzuschliessen, können wir wieder die Funktion mask() verwenden (siehe Kapitel 60.5). Doch diesmal möchten wir nicht die Flächen ausserhalb der Polygone mit NA ersetzen, sondern die Flächen innerhalb der Polygone. Deshalb verwenden wir mask() mit dem Argument inverse = TRUE.\nVersuche mit mask(), den oben erwähnten Vektordatensätze sowie der Option inverse = TRUE die Ausschlussgebiete vom Raster-Overlay zu entfernen und visualisiere das Resultat.\n\n\nCode\noverlay_prelim_4 &lt;- overlay_prelim_3 |&gt;\n  mask(schutzgebiete, inverse = TRUE) |&gt;\n  mask(siedlungsgebiet, inverse = TRUE) |&gt;\n  mask(wald, inverse = TRUE) |&gt;\n  mask(seen, inverse = TRUE) |&gt;\n  mask(kt_schwyz)\n\ntmap_mode(\"view\")\n\ntm_shape(overlay_prelim_4) +\n  tm_raster(palette = \"-Spectral\", breaks = seq(0,1,0.2), style = \"cont\",alpha = 0.6) +\n  tm_basemap(\"Esri.WorldImagery\")"
  },
  {
    "objectID": "fallstudie_s/1_Einleitung.html#hintergrund",
    "href": "fallstudie_s/1_Einleitung.html#hintergrund",
    "title": "KW40: Einleitung",
    "section": "Hintergrund",
    "text": "Hintergrund\nDas rund 1100 ha grosse Naturschutzgebiet Wildnispark Zürich Sihlwald, welches im periurbanen Raum südlich von Zürich liegt, gilt seit dem 1. Januar 2010 als erster national anerkannter Naturerlebnispark. Er ist Teil des Wildnisparks Zürich und wichtiges Naherholungsgebiet für die Stadt Zürich.\nDas Schutzgebiet befindet sich im Spannungsfeld zwischen Schutz und Nutzen, denn einerseits sollen die Besuchenden den Wald erleben dürfen, andererseits soll sich dieser, in der Kernzone, frei entwickeln dürfen. Im Perimeter gelten darum verschiedene Regeln. So darf z. B. nur auf bestimmten Wegen mit den Velo gefahren werden.\n\nDas Management braucht solide, empirisch erhobene Daten zur Natur und zu den Besuchenden damit die Ziele von Nutzen und Schürzen erreicht werden können. Das Besuchermonitoring deckt den zweiten Teil dieser notwendigen Daten ab. Im Wildnispark Zürich sind dazu mehrere automatische Zählstellen in Betrieb. Die Zählstellen erfassen stundenweise die Besuchenden auf den Wegen. Einige Zählstellen erfassen richtungsgetrennt und / oder können zwischen verschiedenen Nutzergruppen wie Personen, die zu Fuss gehen, und Velofahrenden unterscheiden.\nIm Rahmen des Moduls Research Methods werden in dieser Fallstudie mehrere dieser automatischen Zählstellen genauer untersucht. Die Daten, welche im Besitz des WPZ sind, wurden bereits kalibriert. Das heisst, Zählungen während Wartungsarbeiten, bei Felhbetrieb o.ä. wurden bereits ausgeschlossen. Dies ist eine zeitintensive Arbeit und wir dürfen hier mit einem sauber aufbereiteten “Datenschatz” arbeiten.\nPerimeter des Wildnispark Zürichs mit den ungefähren Standorten von zwei ausgewählten automatischen Zählstellen.\n\n\n\n\nHinweis:\n\nDie Zähler 211 und 502 erfassen sowohl Fussgänger:innen als auch Fahrräder. Die Erfassung erfolgt richtungsgetrennt.\n\nDer Wildnispark wertet die Zahlen auf verschiedene Weise aus. So sind z. B. Jahresgänge (an welchen Monaten herrscht besonders viel Betrieb?) und die absoluten Nutzungszahlen bekannt. Vertiefte Auswertungen, die beispielsweise den Zusammenhang zwischen Besuchszahlen und dem Wetter untersuchen, werden nicht gemacht.\nUnsere Analysen in diesem Modul helfen dem Management, ein besseres Verständnis zum Verhalten der Besuchenden zu erlangen und bilden Grundlagen für Managemententscheide in der Praxis."
  },
  {
    "objectID": "fallstudie_s/1_Einleitung.html#ziel",
    "href": "fallstudie_s/1_Einleitung.html#ziel",
    "title": "KW40: Einleitung",
    "section": "Ziel",
    "text": "Ziel\nIn dieser Fallstudie zeigen wir, welche Einflüsse die Covid19-Pandemie im Frühjahr 2020 auf die täglichen Besuchszahlen im Wildnispark Zürich hatte. Dabei setzen wir den Fokus auf die Dämmerung und die Nacht, den in diesen Zeiten sind Wildtiere besonders sensibel gegenüber Störungen.\nIn unsere Analysen ziehen wir auch weitere erklärende Faktoren wie Wetter, Wochentag, Kalenderwoche und Schulferien mit ein. Die statistischen Auswertungen erlauben und somit klare Rückschlüsse auf die Effekte der Faktoren und deren Stärke zu ziehen."
  },
  {
    "objectID": "fallstudie_s/1_Einleitung.html#grundlagen",
    "href": "fallstudie_s/1_Einleitung.html#grundlagen",
    "title": "KW40: Einleitung",
    "section": "Grundlagen",
    "text": "Grundlagen\nZur Verfügung stehen:\n\ndie stündlichen Zählungen von Fussgänger:innen und Velos an den Zählstellen\nMeteodaten (Temperatur, Sonnenscheindauer, Niederschlagssumme)\nR-Skripte mit Hinweisen zur Auswertung"
  },
  {
    "objectID": "fallstudie_s/1_Einleitung.html#aufbau-der-fallstudie",
    "href": "fallstudie_s/1_Einleitung.html#aufbau-der-fallstudie",
    "title": "KW40: Einleitung",
    "section": "Aufbau der Fallstudie",
    "text": "Aufbau der Fallstudie\nIn dieser Fallstudie erheben wir zuerst selbst Daten auf dem Grüntal, welche wir dann deskriptiv auswerten. Anschliessend beschäftigen wir uns mit den Daten aus dem Wildnispark Zürich, welche wir ebenfalls deskriptiv auswerten und auch sttistische Modelle damit programmieren. Diese Ergebnisse werden dann im Abschlussbericht dokumentiert."
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung.html#einführung-und-installation",
    "href": "fallstudie_s/2_Felderhebung.html#einführung-und-installation",
    "title": "KW 40 - KW 42: Felderhebung",
    "section": "Einführung und Installation",
    "text": "Einführung und Installation\nEs gibt eine Vielzahl an möglichen Methoden zur Erfassung der Besuchszahlen. Automatische Zählgeräte bieten die Möglichkeit lange und durchgehende Zeitreihen zu erfassen. Inputs dazu, wie diese ausgewertet werden können, erhält ihr in dieser Aufgabe."
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung.html#ziele",
    "href": "fallstudie_s/2_Felderhebung.html#ziele",
    "title": "KW 40 - KW 42: Felderhebung",
    "section": "Ziele",
    "text": "Ziele\n\nDie Studierenden können das eingesetzte Zählgerät installieren und kennen die Vor- und Nachteile verschiedener Methoden.\nDie Studierenden können die Daten auslesen und explorativ analysieren."
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung.html#grundlagen",
    "href": "fallstudie_s/2_Felderhebung.html#grundlagen",
    "title": "KW 40 - KW 42: Felderhebung",
    "section": "Grundlagen",
    "text": "Grundlagen\nDie Geräte werden innerhalb der unten eingezeichneten Elipsen platziert. Damit soll überprüft werden, wie stark frequentiert die Waldränder der ökologisch aufgewerteten Seeparzelle sind.\n\nDatenschutz ist ein wichtiges Thema. Die Besuchenden werden über den Zweck der Kameras informiert, die Daten nach der Bearbeitung wieder gelöscht und nicht weitergegeben."
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung.html#auswertung",
    "href": "fallstudie_s/2_Felderhebung.html#auswertung",
    "title": "KW 40 - KW 42: Felderhebung",
    "section": "Auswertung",
    "text": "Auswertung\nAUFGABE ab dem 17. 10. 2022\nNachdem die Kameras für zwei Wochen im Einsatz standen, sichten wir zusammen die Ergebnisse.\nDa die Anzahl Passagen auf der Seeparzelle keine schöne Auswertung erlauben, arbeiten wir ab jetzt mit einem Datensatz aus dem WPZ. Die Vorteile für euch sind:\n\nihr habt genügend Daten für die Auswertung mit R und\ndie Daten sind im selben Format wir für die späteren Aufgaben.\n\n\nAufgabe 1: Vorarbeiten\n\nÜberlegt euch mögliche Darstellungsformen für die Anzahl Passagen und die beobachteten Aktivitäten an den untersuchten Standorten.\nSkizziert eure Ideen mittels Stift und Papier.\n\n\n\nAufgabe 2: Darstellung in R\nR bietet sehr viele Optionen zur Analyse und zur Darstellung der Daten. Nehmt bitte den bereitgestellten Datensatz zur Hand und visualisiert eure Ideen mit R.\nUntenstehend sind einige Ideen zur Umsetzung.\n\n### Bibliothek laden\n\n# zuerst muss sie installiert sein:\n# install.packages(\"tidyverse\")\nlibrary(tidyverse) # Arbeiten mit Datumsformaten\n\n### Datensatz einlesen\n\n# dabei speichere ich ihn gleich unter der Variable \"depo\" ab.\ndepo &lt;- read.csv(\"datasets/fallstudie_s/Felderhebungen/Bsp_Data.csv\", sep = \";\")\n\n### Datum und Uhrzeit\n# das Datum und die Uhrzeit sind in einer Spalte. R liest das als \"Buchstaben\" ein. Wir definieren es als Datum:\ndepo$DatumUhrzeit &lt;- as.POSIXct(depo$DatumUhrzeit, format = \"%d.%m.%Y %H:%M\")\n\n\n### Kennzahlen\n\n#zuerst schaue ich mir jeweils den Aufau und die Kennzahlen zum Datensaz an:\nstr(depo)\n\n# hat es im Datensatz noch fehlende Werte?\nsum(is.na(depo))\n\n# wie viele Personen sind IN das Gebiet gegangen?\nsum(depo$Fuss_IN)\n\n# wie viele insgesamt?\n# dafür erstellen wir zuerst eine neue Spalte mit der Totalen Anzahl pro Datum und Zeitstempel:\ndepo$Total = depo$Fuss_IN + depo$Fuss_OUT + depo$Velo_IN + depo$Velo_OUT\n# und berechnen nachher die Summe dieser neuen Spalte\nsum(depo$Total)\n\n\n# Darstellen der Anzahl Passagen pro Stunde und Tag\nplot(x = depo$DatumUhrzeit, y = depo$Total,\n     pch = 21,  # Form\n     cex = 1.5, # Grösse\n     bg=\"blue\") # Füllung\n\n\n\n\n\n# Darstellung der verschiedenen Nutzergruppen\nslieces &lt;- c(sum(depo$Fuss_IN), sum(depo$Fuss_OUT), sum(depo$Velo_IN), sum(depo$Velo_OUT))\nlbls &lt;- c(\"Fuss_IN\", \"Fuss_OUT\", \"Velo_IN\", \"Velo_OUT\") \npie(slieces, labels = lbls)\n\n\n\n\n\n\nAufgabe 3: für Fortgeschrittene\nDie Anzahl Passagen pro Zählstelle können nicht nur als statische Diagramme dargestellt werden. R ist auch ein GIS! Hier seht ihr, wie mit R interaktive Karten gestaltet werden können.\n\n# zuerst berechnen wir mit der Bibliothek tidyverse das Total pro Standort\ntotal_Standort &lt;- depo |&gt; \n  group_by(Standort, lon, lat) |&gt; \n  summarise(Total = sum(Total))\n\n# dann überführen wir den Datensatz in ein räumliches Format \n# (Hinweis: dafür muss die Bibliothes \"sf\" installiert sein)\ntotal_Standort &lt;- sf::st_as_sf(total_Standort, \n                         coords = c(\"lon\", \"lat\"),\n                         crs = 2056)\n\n# Transformiere die CH1903 Koordinaten in WGS84\ntotal_Standort &lt;- sf::st_transform(total_Standort, crs = 4326)\n\n# Plotte nun eine interaktive Karte\n# install.packages(\"tmap\")\nlibrary(tmap)\n\n# setze den Modus auf Interaktiv\ncurrent.mode &lt;- tmap_mode(\"view\") \n\n\n# plotte\ntm_basemap(server = c(Topo = \"Esri.WorldTopoMap\",\n                      Ortho = \"Esri.WorldImagery\",\n                      OSM = \"OpenStreetMap.HOT\"))+\n  tm_shape(total_Standort)+\n  tm_bubbles(size = \"Total\", border.col = \"black\", col = \"red\", scale = 5, alpha = 0.5)\n\n\n\n\n\n\nInteraktive Karte von zwei ausgewählten Zählstellen im Untersuchungsgebiet mit fiktiven Besuchszahlen."
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung.html",
    "href": "fallstudie_s/3_Aufgabenstellung.html",
    "title": "KW 42: Aufgabenstellung WPZ",
    "section": "",
    "text": "Multivariate Analyse: Abschlussbericht\nHinweis: Bitte bearbeitet dieses Skript am 18.10.2022 erst nach der Präsentation / Diskussion eurer Visualisierungen aus der Aufgabenstellung [Einführung und Installation]."
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung.html#ziele",
    "href": "fallstudie_s/3_Aufgabenstellung.html#ziele",
    "title": "KW 42: Aufgabenstellung WPZ",
    "section": "Ziele",
    "text": "Ziele\nIhr habt selbst ein (kleines) Besuchermonitoring auf dem Grüental durchgeführt und euch bereits mit dem WPZ beschäftigt. De Aufgaben im Zusammenhang mit dem Grüental sind nun abgeschlossen und wir beschäftigen uns ausschliesslich mit dem WPZ.\nIm Rahmen unserer Analyse programmieren wir multivariate Modelle, welche den Zusammenhang zwischen der Anzahl Besuchenden und verschiedenen Einflussfaktoren beschreiben. Dank den Modellen können wir sagen, wie die Besucher:innen auf die untersuchten Faktoren reagiert haben (siehe dazu auch [Einleitung], Ziele).\nKonkret sollen folgende Fragestellungen beantwortet werden:\n\n\nWelchen Einfluss haben neben den Phasen der Covid-Pandemie auch die Wetterparameter (Sonnenscheindauer, Tageshöchsttemperatur, Niederschlagssumme) sowie der Wochentag, die Ferien, die Kalenderwoche und das Jahr auf die Besuchszahlen am Tag, in der Dämmerung und in der Nacht?\nWie stark sind die jeweiligen Einflüsse, welche Effektrichtungen sind beobachtbar und welche der untersuchten Parameter sind signifikant?\nKönnen deutliche Unterschiede zwischen den “normalen”, vor-Covid19-Jahren und danach bei der Tageszeitliche Nutzung, den Wochen-, und / oder Saisongängen sowie den wichtigsten, deskriptiven Kennzahlen gefunden werden?\n\n\n\nJede Gruppe wertet Daten von einem Zähler aus. Sprecht miteinander ab, wer welchen Zähler behandelt (211 oder 502; Spezifikationen siehe [Einleitung], Hinweis). Jeder Zähler soll nur von einer Gruppe ausgewertet werden!\nBezieht in eure Auswertungen den gesamten zur Verfügung stehenden Zeitraum ein.\nFür euren Zähler stehen Zahlen zu Fussgänger:innen und Velos zur Verfügung (siehe [Einleitung], Hinweis). Entscheidet euch selbst, ob ihr Fussgänger:innen ODER Velos auswerten wollt. Die anderen Daten dürft ihr vernachlässigen.\nIm Bericht sollen die Informationen und Erfahrungen aus dem gesamten Verlauf der Fallstudie in geeigneter Weise einfliessen. Bezüglich der Felderhebung Grüntal erwarten wir keine Angaben."
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung.html#erwartungen",
    "href": "fallstudie_s/3_Aufgabenstellung.html#erwartungen",
    "title": "KW 42: Aufgabenstellung WPZ",
    "section": "Erwartungen",
    "text": "Erwartungen\n\nStruktur / Aufbau\n\n\nAbstract / Zusammenfassung (wenn Bericht auf Deutsch, dann Zusammenfassung auch auf Deutsch) im Umfang von 250 - 300 Wörter. Im Abstract sollen alle Bestandteile des Berichts aufgenommen sein.\nEs wird keine Einleitung erwartet. Sie kann aber gerne kurz ausgeführt werden.\nFragestellung (siehe oben; die Fragestellung ist vorgegeben, darf aber natürlich für den Bericht geschärft und optimal formuliert und konkretisiert werden.)\nMethoden (kurzes Kapitel mit den statistischen Analysen)\nResultate (deskriptive Statistik, multivariates Modell; kurzer Fliesstext sowie die notwendigen Tabellen und eine Auswahl möglichst informativer Grafiken)\nDiskussion (Diskussion der deskriptiven Analysen und der Modellergebnisse; dieser Abschnitt sollte die eigenen Resultate auch im Zusammenhang mit aktueller Fachliteratur reflektieren.)\nLiteraturverzeichnis (Tipp: Das Literaturverzeichnis sollte vollständig sein, sowie formal korrekt und einheitlich daherkommen. Wir erwarten speziell in der Diskussion eine Abstützung auf aktuelle Fachliteratur. Auf Moodle haben wir Euch eine Auswahl relevanter Papers bereitgestellt.)\nAnhang (für alle Auswertungen relevanter R-Code in geeigneter Form)\n\n\nGesamtumfang max. 7500 Zeichen (inkl. Leerzeichen; exkl. Zusammenfassung, Einleitung, Tabellen, Literaturverzeichnis und Anhang)\nAbgabe am 8.1.2023 per Mail an hoce@zhaw.ch"
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung.html#bewertungskriterien",
    "href": "fallstudie_s/3_Aufgabenstellung.html#bewertungskriterien",
    "title": "KW 42: Aufgabenstellung WPZ",
    "section": "Bewertungskriterien",
    "text": "Bewertungskriterien\n\nIst die Methode klar und verständlich formuliert?\nSind die deskriptiven Analysen klar beschrieben und geeignet visualisiert?\nIst die Variablenselektion klar beschrieben, plausibel und nachvollziehbar?\nSind die Modellresultate in Text- und Tabellenform korrekt beschrieben und geeignet visualisiert?\nIst die Diskussion klar formuliert und inhaltlich schlüssig?\nWie gut ist die Diskussion auf relevante und aktuelle Fachliteratur abgestützt?\nZusätzliche bewerten wir die inhaltliche Dichte der Arbeit und die formale Qualität (Sprache, Struktur, Aufbau, Darstellung, Literaturverzeichnis, Umgang mit Literatur im Text)\n\nZusammensetzung der Fallstudiennote:\n\nFallstudie-Leistungsnachweis 1 - Forschungsplan: 30 %\nFallstudie-Leistungsnachweis 2 - Multivariate Analyse: 70 %"
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html",
    "href": "fallstudie_s/4_Projektierung.html",
    "title": "KW 42: Projektierung",
    "section": "",
    "text": "Arbeiten mit Projekten\nHinweis: Bitte bearbeitet dieses Skript am 18.10.2022 erst nach der Einführung [Multivariate Analyse: Abschlussbericht]."
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#vorbereitung",
    "href": "fallstudie_s/4_Projektierung.html#vorbereitung",
    "title": "KW 42: Projektierung",
    "section": "Vorbereitung",
    "text": "Vorbereitung\nVor den eigentlichen Auswertungen müssen einige Vorbereitungen unternommen werden. Die Zeit, die man hier investiert, wird in der späteren Projektphase um ein Mehrfaches eingespart.\nIch empfehle generell mit Projekten zu arbeiten, da diese sehr einfach ausgetauscht (auf verschiedene Rechner) und somit auch reproduziert werden können. Wichtig ist, dass es keine absoluten Arbeitspfade sondern nur relative gibt. Der Datenimport (und -export) kann mithilfe dieser relativen Pfade stark vereinfacht werden. –&gt; Kurz gesagt: Projekte helfen alles am richtigen Ort zu behalten (mehr zur Arbeit mit Projekten: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects).\n\nErstellt an einem passenden Speicherort ein neues Projekt mit einem treffenden Namen:\n\n–&gt; File / New Project"
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-1-projektaufbau",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-1-projektaufbau",
    "title": "KW 42: Projektierung",
    "section": "Aufgabe 1: Projektaufbau",
    "text": "Aufgabe 1: Projektaufbau\nHinweise:\nNutzt für allen Text, welcher nicht im Code integriert ist, das Symbol #. Wenn ihr den Text als Titel definieren wollt, so dass er in der Übersicht erscheint, müssen vor dem Wort # und nach dem Wort #### eingefügt werden.\n\n# Texte, vor denen ein # und nach denen #### stehen, sind Überschriften\n\n# Ich bin eine Überschrift ####\n\n# Texte, vor denen ein # steht, erklaeren den Ablauf\n\n# Dann folgen die Arbeitsschritte\n1+1\n\n# Wenn man auf \"Outline\" klickt (oder CTRL + SHIFT + O), \n# öffnet sich die Übersicht zu den Überschriften\n\nTipps:\n\nAlt + - = &lt;-\nCtrl + Shift + C = # vor der ausgewaehlten Zeile\n\nAufbau eines Skripts\nZuerst immer den Titel des Projekts sowie den Autor/ die Autorin des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklärt werden, wer Dateneigentümer ist (WPZ und Meteo Schweiz).\nIm Skript soll immer die Ordnerstruktur des Projekts genannt werden. So kann der Arbeitsvorgang auf verschiedenen Rechnern einfach reproduziert werden (ich verwende hier ein Projektordner mit den Unterordnern __skripts, data, results).\nBeschreibt zudem folgendes die verwendete Meteodaten (siehe dazu Metadata Meteodaten, –&gt; order_XXX_legend.txt)\nEin Skript soll in R eigentlich immer (mehr oder weniger) nach dem selbem Schema aufgebaut sein. Dieses Schema enthällt (nach den bereits erwähnten Definitionen) 4 Kapitel:\n\nMetadaten und Definitionen\nDatenimport,\nVorbereitung,\nDeskriptive Analyse und Visualisierung und\nMultifaktorielle Analyse und Visualisierung.\n\nBereitet euer Skript mit diesen Kapitel vor.\n\n#.###########################################################################################\n# Einfluss von COVID19 auf das Naherholungsverhalten in WPZ ####\n# Fallstudie Modul Research Methods, HS22. Autor/in ####\n#.##########################################################################################\n\n#.##########################################################################################\n# METADATA UND DEFINITIONEN ####\n#.##########################################################################################\n\n# Datenherkunft ####\n# ...\n\n#.##########################################################################################\n# 1. DATENIMPORT #####\n#.##########################################################################################\n\nIn einem Bericht sollen die Abbildung einheitlich sein Dafür braucht es u.a. eine Farbpalette. Ich definiere meine Auswahl bereits hier; das hat den Vorteil, dass man die Farbnamen nur einmal schreiben muss und später die selbst definierte Palette unter der Variable “mycolors” abrufen kann.\n\nmycolors &lt;- c(\"orangered\",\"gold\", \"mediumvioletred\", \"darkblue\")"
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-2-laden-der-bibliotheken",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-2-laden-der-bibliotheken",
    "title": "KW 42: Projektierung",
    "section": "Aufgabe 2: Laden der Bibliotheken",
    "text": "Aufgabe 2: Laden der Bibliotheken\n\nGeplottet wird mit ggplot, daher wird tidyverse geladen. Diese Bibliothek ergaenzt BASE R in vielerlei Hinsicht uns ist eigentlich fast immer nötig.\nDa wir es bei Besucherdaten immer mit einem zeitlichen Bezug zu tun haben, benoetigen wir eine passende Bibliothek. Ich arbeite mit lubridate, POSIXct waere natuerlich auch moeglich.\nggpubr brauchen wir für das Darstellen von mehreren verschiedenen Plots in nur einem. - PerformanceAnalytics, MuMIn, AICcmodavg, fitdistrplus, lme4 und sjPlot werden fuer die spaeteren multivariaten Analysen benoetigt. -Die Modellguete werden wir mittels lattice, blmeco und lattice pruefen.\n\n–&gt; Lädt nun die benoetigten Bibliotheken.\n\nDiese müssen zuerst mit install.packages(“NAME”) installiert werden.\n\n\n# Benoetigte Bibliotheken ####\nlibrary(tidyverse) # Data wrangling und piping\nlibrary(lubridate) # Arbeiten mit Datumsformaten\nlibrary(ggpubr)    # to arrange multiple plots in one graph\nlibrary(PerformanceAnalytics) # Plotte Korrelationsmatrix\nlibrary(MuMIn)     # Multi-Model Inference\nlibrary(AICcmodavg)# Modellaverageing\nlibrary(fitdistrplus)# Prueft die Verteilung in Daten\nlibrary(lme4)      # Multivariate Modelle\nlibrary(blmeco)    # Bayesian data analysis using linear models\nlibrary(sjPlot)    # Plotten von Modellergebnissen (tab_model)\nlibrary(lattice)   # einfaches plotten von Zusammenhängen zwischen Variablen"
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-3-zeitliche-definitionen",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-3-zeitliche-definitionen",
    "title": "KW 42: Projektierung",
    "section": "Aufgabe 3: Zeitliche Definitionen",
    "text": "Aufgabe 3: Zeitliche Definitionen\nWIr lesen später zwei verschiedene Datensätze ein. Beide sollen exakt denselben Zeitraum umfassen. Definiert dazu den ersten und letzten Tag gemäss den vorhandenen Zähldaten.\n\ndepo_start &lt;- as.Date(\"2017-01-01\")\ndepo_end &lt;- as.Date(\"2022-7-31\")\n\nWichtiger Teil unserer Auswertungen ist der Einfluss des Lockdown auf das Besuchsverhalten.\n-Wir müssen also Start und Ende der beiden Lockdowns in der Schweiz definieren:\n\nlock_1_start_2020 &lt;- as.Date(\"2020-03-16\")\nlock_1_end_2020 &lt;- as.Date(\"2020-05-11\")\n\nlock_2_start_2021 &lt;- as.Date(\"2020-12-22\")\nlock_2_end_2021 &lt;- as.Date(\"2021-03-01\")\n\nEbenfalls müssen die erste und letzte Kalenderwoche der Untersuchungsfrist definiert werden. Diese werden bei wochenweisen Analysen ausgeklammert da sie i.d.R. unvollstaendig sind (das ist ein späterer Arbeitsschritt). Geht wie oben vor. Tipp: der Befehl isoweek() liefert euch die Kalenderwoche.\nFerienzeiten können einen grossen Einfluss auf das Besucheraufkommen haben. Die relevanten Ferienzeiträume (je nach dem müsst ihr das anpassen) müssen daher bekannt sein. Zur Definition der Ferien kann z.B. folgend vorgegangen werden:\n\n# (https://www.schulferien.org/schweiz/ferien/2020/)\nWinterferien_2016_start &lt;- as.Date(\"2017-01-01\") \nWinterferien_2016_ende &lt;- as.Date(\"2017-01-08\")\n\nFruehlingsferien_2017_start &lt;- as.Date(\"2017-04-15\") \nFruehlingsferien_2017_ende &lt;- as.Date(\"2017-04-30\") \nSommerferien_2017_start &lt;- as.Date(\"2017-07-15\") \nSommerferien_2017_ende &lt;- as.Date(\"2017-08-20\") \nHerbstferien_2017_start &lt;- as.Date(\"2017-10-07\") \nHerbstferien_2017_ende &lt;- as.Date(\"2017-10-22\") \nWinterferien_2017_start &lt;- as.Date(\"2017-12-23\") \nWinterferien_2017_ende &lt;- as.Date(\"2018-01-07\") \n\nFruehlingsferien_2018_start &lt;- as.Date(\"2018-04-21\") \nFruehlingsferien_2018_ende &lt;- as.Date(\"2018-05-06\") \nSommerferien_2018_start &lt;- as.Date(\"2018-07-14\") \nSommerferien_2018_ende &lt;- as.Date(\"2018-08-19\") \nHerbstferien_2018_start &lt;- as.Date(\"2018-10-06\") \nHerbstferien_2018_ende &lt;- as.Date(\"2018-10-21\") \nWinterferien_2018_start &lt;- as.Date(\"2018-12-22\") \nWinterferien_2018_ende &lt;- as.Date(\"2019-01-06\") \n\nFruehlingsferien_2019_start &lt;- as.Date(\"2019-04-20\") \nFruehlingsferien_2019_ende &lt;- as.Date(\"2019-05-05\") \nSommerferien_2019_start &lt;- as.Date(\"2019-07-13\") \nSommerferien_2019_ende &lt;- as.Date(\"2019-08-18\") \nHerbstferien_2019_start &lt;- as.Date(\"2019-10-05\") \nHerbstferien_2019_ende &lt;- as.Date(\"2019-10-20\") \nWinterferien_2019_start &lt;- as.Date(\"2019-12-21\") \nWinterferien_2019_ende &lt;- as.Date(\"2020-01-05\")\n\nFruehlingsferien_2020_start &lt;- as.Date(\"2020-04-11\")\nFruehlingsferien_2020_ende &lt;- as.Date(\"2020-04-26\")\nSommerferien_2020_start &lt;- as.Date(\"2020-07-11\")\nSommerferien_2020_ende &lt;- as.Date(\"2020-08-16\")\nHerbstferien_2020_start &lt;- as.Date(\"2020-10-03\")\nHerbstferien_2020_ende &lt;- as.Date(\"2020-10-18\")\nWinterferien_2020_start &lt;- as.Date(\"2020-12-19\")\nWinterferien_2020_ende &lt;- as.Date(\"2021-01-03\")\n\nFruehlingsferien_2021_start &lt;- as.Date(\"2021-04-24\")\nFruehlingsferien_2021_ende &lt;- as.Date(\"2021-05-09\")\nSommerferien_2021_start &lt;- as.Date(\"2021-07-17\")\nSommerferien_2021_ende &lt;- as.Date(\"2021-08-22\")\nHerbstferien_2021_start &lt;- as.Date(\"2021-10-09\")\nHerbstferien_2021_ende &lt;- as.Date(\"2021-10-24\")\nWinterferien_2021_start &lt;- as.Date(\"2021-12-18\")\nWinterferien_2021_ende &lt;- as.Date(\"2022-01-02\")\n\nFruehlingsferien_2022_start &lt;- as.Date(\"2022-04-16\")\nFruehlingsferien_2022_ende &lt;- as.Date(\"2022-05-01\")\nSommerferien_2022_start &lt;- as.Date(\"2022-07-16\")\nSommerferien_2022_ende &lt;- as.Date(\"2022-08-21\")\nHerbstferien_2022_start &lt;- as.Date(\"2022-10-08\")\nHerbstferien_2022_ende &lt;- as.Date(\"2022-10-23\")\nWinterferien_2022_start &lt;- as.Date(\"2022-12-24\")\nWinterferien_2022_ende &lt;- as.Date(\"2023-01-08\")\n\nNun sind alle Vorbereitungen gemacht, die Projektstruktur aufgebaut und die eigentliche Arbeit kann beginnen."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "",
    "text": "Aufgabe 2: Meteodaten"
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#aufgabe-1-zähldaten",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#aufgabe-1-zähldaten",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "Aufgabe 1: Zähldaten",
    "text": "Aufgabe 1: Zähldaten\nDie Projektstruktur steht. Nun können die Daten eingelesen und die nötigen Datentypen definiert werden.\nLädt die Daten zuerst von Moodle herunter.\nHinweise:\n\nSiehe [Einleitung] für den Standort der Zähler 211 und 502.\nDie Daten sind auf Moodle unter ReMe HS22 MSc ENR / Fallstudie Biodiversity & Ecosystems / S_Daten abgelegt.\n\n\nZähldaten zu eurem Standort (211_sihlwaldstrasse_2017_2022.csv, 502_sihluferweg_2016_2022.csv)\nMeteodaten (order_105742_data.txt)\n\nDie Zähldaten des Wildnispark Zürich wurden vorgängig bereinigt (z.B. wurden Stundenwerte entfernt, an denen am Zähler Wartungsarbeiten stattgefunden haben). Das macht es für uns einfach, denn wir können die Daten ohne vorgängige Bereinigung einlesen. Behaltet aber im Hinterkopf, dass die Datenaufbereitung, die Datenbereinigung mit viel Aufwand verbunden ist.\n\nLest die Zählaten ein, speichert ihn unter der Variable depo und sichtet den Datensatz (z.B. str(), head(), view() usw.).\n\n\ndepo &lt;- read.csv(\"./HIER RELATIVEN DATEIPFAD EINGEBEN\", sep = \"HIER SEPERATOR EINGEBEN\") \n# Speicherort sowie Dateiname anpassen\n\nHinweis: Im Stundenformat zeigen die Werte bei 11:00 die Zähldaten zwischen 11:00 bis 12:00 Uhr."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "1a)",
    "text": "1a)\n\nIm Datensatz des Wildnisparks sind Datum und Uhrzeit in einer Spalte. Diese müssen getrennt werden (Ich schlage hier den Ansatz des piping ( |&gt; ) vor. Damit können in einem “Rutsch” mehrere Operationen ausgeführt werden).\nEbenfalls muss das Datum als solches definiert werden. Welches Format hat es (im Code: format = “HIER DATUMSFORMAT”)?\n\n\nstr(depo)\n\ndepo &lt;- depo |&gt;\n  mutate(Datum = as.character(Datum)) |&gt; \n  mutate(Datum = as.Date(Datum, format = \"HIER DATUMSFORMAT\")) # hier wird Text zum Datum"
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "1b)",
    "text": "1b)\nIhr könnt selbst wählen, ob ihr Fussgänger:innen oder Velos untersuchen wollt (je nachdem ob sie in eurem Datensatz vorhanden sind).\n\nEntfernt die überflüssigen Spalten aus dem Datensatz.Ich schlage vor, dass ihr dafuer den Befehl dplyr::select() verwendet."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "1c)",
    "text": "1c)\n\nBerechnen des Totals (IN + OUT), da dieses in den Daten nicht vorhanden ist (wiederum mit piping).\n\nTipp: Wenn man R sagt: “addiere mir Spalte x mit Spalte y”, dann macht R das für alle Zeilen in diesen zwei Spalten. Wenn man nun noch sagt: “speichere mir das Ergebnis dieser Addition in einer neuen Spalte namens Total”, dann hat man die Aufgabe bereits gelöst. Arbeitet mit mutate()).\nHinweis: Ihr habt das auch schon in Kapitel [Einführung und Installation] gemacht.\n\nEntfernt nun alle NA-Werte mit na.omit()."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-1",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-1",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "2a)",
    "text": "2a)\n\nLest die Meteodaten ein und speichert sie unter meteo."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-1",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-1",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "2b)",
    "text": "2b)\n\nAuch hier müssen die Datentypen manuell gesetzt werden.\n\nTipp: Das Datum wird als Integer erkannt. Zuerst muss es in Text umgewandelt werden aus dem dann das eigentliche Datum herausgelesen werden kann. Das ist mühsam - darum hier der Code.\n\nmeteo &lt;- transform(meteo, time = as.Date(as.character(time), \"%Y%m%d\"))\n\nHinweis Was ist eigentlich Niederschlag:\nhttps://www.meteoschweiz.admin.ch/home/wetter/wetterbegriffe/niederschlag.html\n\nWerden den anderen Spalten die richtigen Typen zugewiesen? Falls nicht, ändert die Datentypen.\nNun schneiden wir den Datensatz auf die Untersuchungsdauer zu."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-1",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-1",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "2c)",
    "text": "2c)\n\nJetzt müssen auch hier alle nicht verfügbare Werte (NA’s) herausgefiltert werden.\n\nTipp: Entweder geht das mit na.omit() für alle Spalten oder, etwas konservativer, können mit filter() die zu filternden Spalten definiert werden. Mit folgendem Codeblock können z.B. alle Werte gefiltert werden, die in der Spalte stn nicht gleich NA sind (es werden also die Werte behalten, die vorhanden sind). Der Code muss für die anderen relevanten Spalten noch ergänzt werden.\n\nmeteo &lt;- meteo |&gt;\n  filter(!is.na(stn))|&gt;\n  ...|&gt;\n  ...\n\nHinweis: … steht im Code für folgende oder vorhergehende Zeilen im Code (in einer Pipe)\n\nPrüft nun, wie die Struktur des data.frame (df) aussieht und ob alle NA Werte entfernt wurden (sum(is.na(df$Variable))). Stimmen alle Datentypen?"
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-2",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-2",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "3a)",
    "text": "3a)\nJetzt fügen wir viele Convinience Variabeln hinzu. Wir brauchen:\n\nWochentag; der Befehl dazu ist weekdays()\n\nTipp: R sortiert die Levels alphabetisch. Da das in unserem Fall aber sehr unpraktisch ist, müssen die Levels manuell bestimmt werden\n\n  ...\n  mutate(Wochentag = base::factor(Wochentag, \n                            levels = c(\"Montag\", \"Dienstag\", \"Mittwoch\", \n                                       \"Donnerstag\", \"Freitag\", \"Samstag\", \"Sonntag\")))\n  ...\n\nFrage: Was bedeutet base:: vor den eigentlichen Befehl?\n\nWerktag oder Wochenende?\n\n\n  ...\n  mutate(Wochenende = if_else(Wochentag == \"Montag\" | Wochentag == \"Dienstag\" | \n                           Wochentag == \"Mittwoch\" | Wochentag == \"Donnerstag\" | \n                           Wochentag == \"Freitag\", \"Werktag\", \"Wochenende\"))\n  ...\n\nFrage: Was bedeuten die | (zu erstellen mit AltGr + 7)? Welches ist das if Argument, welches das else?\n\nKalenderwoche: isoweek()\nMonat: month()\nJahr: year()\nPhase Covid (Code untenstehend). Wir definieren sechs Phasen:\n\n\nvon Anfang Untersuchungsperiode bis 1 Jahr vor Lockdown 1 (pre)\n1 Jahr vor Corona (normal)\nLockdown 1\nzwischen den Lockdowns\nLockdown 2\nEnde 2. Lockdown bis Ende Untersuchungsperiode\n\nHinweis:\n\nIch mache den letzten Punkt nachgelagert, da zu viele Operationen in einem Schritt auch schon mal etwas durcheinander erzeugen können.\nWir packen alle Phasen (normal, die beiden Lockdowns und Covid aber ohne Lockdown) in eine Spalte –&gt; long-format ist schöner (und praktischer für das plotten) als wide-format.\n\n\ndepo &lt;- depo |&gt;\n  mutate(Phase = if_else(Datum &gt;= lock_1_start_2020 & Datum &lt;= lock_1_end_2020,\n                         \"Lockdown_1\",\n                         if_else(Datum &gt;= lock_2_start_2021 & Datum &lt;= lock_2_end_2021,\n                                 \"Lockdown_2\",\n                                 if_else(Datum&gt;= (lock_1_start_2020 - years(1)) & Datum &lt; lock_1_start_2020,\n                                         \"Normal\", \n                                         ifelse(Datum&gt;lock_1_end_2020 & Datum &lt;lock_2_start_2021,\n                                                \"Inter\",\n                                         if_else(Datum &gt; lock_2_end_2021,\n                                                 \"Post\", \"Pre\"))))))\n\n# hat das gepklappt?!\nunique(depo$Phase)\n\nFrage: Welches ist das if Argument, welches das else?\n\nÄndert die Datentypen der Spalten Wochenende, KW, Phase zu factor und sortiert die Levels, so dass diese Sinn machen (z.B. in Phase = Pre, Normal, Lockdown 1, Lockdown 2, Post)."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-2",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-2",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "3b)",
    "text": "3b)\n\nNun soll noch die volle Stunde als Integer im Datensatz stehen. Diese Angabe muss etwas mühsam aus den Daten gezogen werden (darum hier der fertige Code dazu):\n\n\ndepo$Stunde &lt;- as.numeric(format(as.POSIXct(depo$Zeit,format=\"%H:%M:%S\"),\"%H\"))\n\n# ersetze 0 Uhr mit 24 Uhr (damit wir besser rechnen können)\ndepo$Stunde[depo$Stunde == 0] &lt;- 24\nunique(depo$Stunde)\ntypeof(depo$Stunde)"
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-2",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-2",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "3c)",
    "text": "3c)\nDie Daten wurden durch den WPZ kalibriert (Kommastellen).\n\nRundet sie auf 0 Nachkommastellen (Ganzzahl; unser Modell kann nicht mit Kommazahlen in der ahbängigen Variable umgehen).\nDefiniert sie sicherheitshalber als Integer\nMacht das für IN, OUT und Total.\n\n\ndepo$... &lt;- round(..., digits = 0)\ndepo$... &lt;- as.integer(...)"
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#d-tageszeit",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#d-tageszeit",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "3d) Tageszeit",
    "text": "3d) Tageszeit\nWir setzen den Fokus unserer Untersuchung auf die Veränderung der Besuchszahlen in der Abend- und Morgendämmerung sowie der Nacht. Dafür müssen wir diese tageszeitliche Einteilung der Daten erst machen. Da dies über den Umfang dieser Fallstudie geht, liefere ich euch hier den Code dazu.\nDie wichtigsten Punkte:\n\nDie Tageslänge wurde für den Standort Zürich (Zeitzone CET) mit dem Package “suncalc” berechnet. Dabei wurden Sommer- und Winterzeit berücksichtigt.\nDie Einteilung der Tageszeit beruht auf dem Start und dem Ende der astronomischen Dämmerung sowie der Golden Hour. Der Morgen und der Abend wurden nach dieser Definition berechnet und um je eine Stunde Richtung Tag verlängert.\n\n\n# Einteilung Standort Zuerich\nLatitude &lt;- 47.38598\nLongitude &lt;- 8.50806\n\n# Zur Berechnung der Tageslaege muessen wir zuerst den Start und das Ende der Sommer-\n# zeit definieren\n# https://www.schulferien.org/schweiz/zeit/zeitumstellung/\n\nSo_start_2017 &lt;- as.Date(\"2017-03-26\") \nSo_end_2017 &lt;- as.Date(\"2017-10-29\") \nSo_start_2018 &lt;- as.Date(\"2018-03-25\") \nSo_end_2018 &lt;- as.Date(\"2018-10-28\") \nSo_start_2019 &lt;- as.Date(\"2019-03-31\") \nSo_end_2019 &lt;- as.Date(\"2019-10-27\") \nSo_start_2020 &lt;- as.Date(\"2020-03-29\")\nSo_end_2020 &lt;- as.Date(\"2020-10-25\")\nSo_start_2021 &lt;- as.Date(\"2021-03-28\")\nSo_end_2021 &lt;- as.Date(\"2021-10-31\")\nSo_start_2022 &lt;- as.Date(\"2022-03-27\")\nSo_end_2022 &lt;- as.Date(\"2022-10-30\")\n\n# Welche Zeitzone haben wir eigentlich?\n# Switzerland uses Central European Time (CET) during the winter as standard time, \n# which is one hour ahead of Coordinated Universal Time (UTC+01:00), and \n# Central European Summer Time (CEST) during the summer as daylight saving time, \n# which is two hours ahead of Coordinated Universal Time (UTC+02:00).\n# https://en.wikipedia.org/wiki/Time_in_Switzerland\n\n# Was sind Astronomische Dämmerung und Golden Hour ueberhaupt?\n# https://sunrisesunset.de/sonne/schweiz/zurich-kreis-1-city/\n# https://www.rdocumentation.org/packages/suncalc/versions/0.5.0/topics/getSunlightTimes\n\n# Wir arbeiten mit folgenden Variablen:\n# \"nightEnd\" : night ends (morning astronomical twilight starts)\n# \"goldenHourEnd\" : morning golden hour (soft light, best time for photography) ends\n# \"goldenHour\" : evening golden hour starts\n# \"night\" : night starts (dark enough for astronomical observations)\n\nlumidata &lt;-\n  getSunlightTimes(\n    date = seq.Date(depo_start, depo_end, by = 1),\n    keep = c(\"nightEnd\", \"goldenHourEnd\", \"goldenHour\", \"night\"),\n    lat = Latitude,\n    lon = Longitude,\n    tz = \"CET\")\n\nlumidata &lt;- lumidata |&gt; \n  mutate(Jahreszeit = ifelse(date &gt;= So_start_2017 & date &lt;=  So_end_2017 |\n                               date &gt;= So_start_2018 & date &lt;=  So_end_2018 |\n                               date &gt;= So_start_2019 & date &lt;=  So_end_2019 |\n                               date &gt;= So_start_2020 & date &lt;= So_end_2020 |\n                               date &gt;= So_start_2021 & date &lt;= So_end_2021 |\n                               date &gt;= So_start_2022 & date &lt;= So_end_2022, \n                               \"Sommerzeit\", \"Winterzeit\"))\n\n# CH ist im Im Sommer CET + 1. \n# Darum auf alle relevanten Spalten eine Stunde addieren\n# hinweis: ich verzichte hier auf ifelse, da es einfacher und nachvollziehbarer scheint,\n# hier mit einem filter die betreffenden Spalten zu waehlen\nlumidata_So &lt;- lumidata |&gt; \n  filter(Jahreszeit==\"Sommerzeit\") |&gt; \n  mutate(nightEnd = nightEnd + hours(1),\n         goldenHourEnd =  goldenHourEnd + hours(1),\n         goldenHour = goldenHour + hours(1),\n         night = night + hours(1))\n\nlumidata_Wi &lt;- lumidata |&gt; \n  filter(Jahreszeit==\"Winterzeit\") \n# verbinde sommer- und winterzeit wieder\nlumidata &lt;- rbind(lumidata_So, lumidata_Wi) |&gt; \n  arrange(date)\n\n# change data type\nlumidata$date &lt;- as.Date(lumidata$date, format= \"%Y-%m-%d\")\n\n# drop unnecessary cols\nlumidata &lt;- lumidata |&gt; dplyr::select(-lat, -lon)\n\n# jetzt haben wir alle noetigen Angaben zu Sonnenaufgang, Tageslaenge usw. \n# diese Angaben koennen wir nun mit unseren Zaehldaten verbinden:\ndepo &lt;- left_join(depo,lumidata, by = c(\"Datum\" =\"date\"))\n\n# aendere alle Zeit- und Datumsangaben so, dass sie gleich sind und miteinander verrechnet werden können.\ndepo &lt;- depo |&gt; \n  mutate(datetime = paste(Datum, Zeit)) |&gt; \n  mutate(datetime = as.POSIXct(datetime, format = \"%Y-%m-%d  %H:%M:%S\"))|&gt; \n  mutate(nightEnd = as.POSIXct(nightEnd)) |&gt; \n  mutate(goldenHourEnd = as.POSIXct(goldenHourEnd)) |&gt; \n  mutate(goldenHourEnd = goldenHourEnd + hours(1)) |&gt; \n  mutate(goldenHour = as.POSIXct(goldenHour)) |&gt; \n  mutate(goldenHour = goldenHour - hours(1)) |&gt; \n  mutate(night = as.POSIXct(night))\n\n# im naechsten Schritt weise ich den Stunden die Tageszeiten Morgen, Tag, Abend und Nacht zu.\n# diese Zuweisung basiert auf der Einteilung gem. suncalc und eigener Definition.\ndepo &lt;- depo|&gt;\n  mutate(Tageszeit = if_else(datetime &gt;= nightEnd & datetime &lt;= goldenHourEnd, \"Morgen\",\n                             ifelse(datetime &gt; goldenHourEnd & datetime &lt; goldenHour, \"Tag\",\n                                    ifelse(datetime &gt;= goldenHour & datetime &lt;= night,\n                                           \"Abend\",\n                                           \"Nacht\")))) |&gt;\n  mutate(Tageszeit = factor(Tageszeit, levels = c(\n    \"Morgen\", \"Tag\", \"Abend\", \"Nacht\")))\n\n# # behalte die relevanten Var\ndepo &lt;- depo |&gt; dplyr::select(-nightEnd, -goldenHourEnd, -goldenHour, -night)\n\n#Plotte zum pruefn ob das funktioniert hat\np &lt;- ggplot(depo, aes(y = Datum, color = Tageszeit, x = Stunde))+\n  geom_jitter()+\n  scale_color_manual(values=mycolors)\n\nplotly::ggplotly(p)\n\nBei mir hat der Zusatz der Tageszeit noch zu einigen NA-Wertren geführt. Diese lösche ich einfach:\n\ndepo &lt;- na.omit(depo)\n# hat das funktioniert?\nsum(is.na(depo))"
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-3",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-3",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "4a)",
    "text": "4a)\nUnsere Daten liegen im Stundenformat vor. Für einige Auswertungen müssen wir aber auf ganze Tage zurückgreifen können.\n\nDie Stundendaten müssen zu ganzen Tagen aggregiert werden. Macht das wiederum einer Pipe. Bezieht folgende Gruppierungen (group_by()) mit ein: Datum, Wochentag, Wochenende, KW, Monat, Jahr, Phase. Summiert die Zählmengen separat (Total, IN, OUT) auf und speichert das Resultat unter depo_d.\n\nTipp: Wenn man die Convinience Variablen als grouping variable einspeisst, dann werden sie in das neue df übernommen und müssen nicht nochmals hinzugefügt werden\n\ndepo_d &lt;- depo |&gt; \n  group_by(VARIABLE1, VARIABLE2, ...) |&gt;   # Gruppieren nach den Variablen\n  summarise(Total = sum(Fuss_IN + Fuss_OUT),# Berechnen der gewünschten Werte\n            Fuss_IN = sum(Fuss_IN),\n            ...\n\n\nErstellt nun einen Datensatz depo_daytime, in welchem ihr obrigen Schritt wiederholt aber zusätzlich noch die Gruppierung “Tageszeit” nutzt."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-3",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-3",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "4b)",
    "text": "4b)\n\nAggregiere die Stundenwerte nach dem Monat (Gruppierungen Monat, Jahr) und speichert das neue df unter depo_m.\n\nTipp: Braucht wiederum group_by() und summarise(). Nun brauchen wir nur noch das Total, keine Richtungstrennung mehr.\n\nFügt den neu erstellten df eine Spalte mit Jahr + Monat hinzu. Das ist etwas mühsam, darum hier der fertige Code dazu:\n\n\n# vergewissere, dass sicher df\ndepo_m &lt;- as.data.frame(depo_m)\n# sortiere das df anhand zwei Spalten aufsteigend (damit die Reihenfolge sicher stimmt)\ndepo_m[\n  with(depo_m, order(Jahr, Monat)),]\n\n# Speichere dann Jahr und Monat in einer Spalte und formatiere diese als Datum \ndepo_m &lt;- depo_m |&gt; \n  mutate(Ym = paste(Jahr, Monat)) |&gt;\n  mutate(Ym= lubridate::ym(Ym)) \n\n\nWiederholt diesen Schritt, diesmal aber mit der Gruppierung “Tageszeit” neben “Jahr” und “Monat” und speichert das Resultat unter “depo_m_daytime”."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-3",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-3",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "4c)",
    "text": "4c)\nMacht euch mit den Daten vertraut. Plottet sie, seht euch die df’s an, versteht, was sie repräsentieren.\nZ.B. sind folgende Befehle und Plots wichtig:\n\nstr()\nsummarize()\nhead()\nScatterplot, x = Datum, y = Anzahl pro Zeiteinheit\nHistrogram\nusw.\n\nHinweis: Geht noch nicht zu weit mit euren Plots. Die Idee ist, dass man sich einen Überblick über die Daten verschafft und noch keine “analysierenden” Plots erstellt.\n–&gt; Erklärt dem Plenum am 25.10.2021 was ihr gemacht habt, was eure Daten zeigen und präsentiert diese einfachen Plots. \nNachdem nun alle Daten vorbereitet sind folgt im nächsten Schritt die deskriptive Analyse."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Loesung.html",
    "href": "fallstudie_s/5_Vorverarbeitung_Loesung.html",
    "title": "KW 42+43: Lösung Datenvorverarbeitung",
    "section": "",
    "text": "Aufgabe 1: Zähldaten\n\n# lese die Daten ein \n# Je nach Bedarf muss der Speicherort sowie der Dateiname angepasst werden\ndepo &lt;- read.csv(\"datasets/fallstudie_s/WPZ/211_sihlwaldstrasse_2017_2022.csv\", sep = \";\")\n\n# Hinweis zu den Daten:\n# In hourly analysis format, the data at 11:00 am corresponds to the counts saved between \n# 11:00 am and 12:00 am.\n\n# Anpassen der Datentypen und erstes Sichten\nstr(depo)\n\ndepo &lt;- depo |&gt;\n  mutate(Datum = as.character(Datum)) |&gt; \n  mutate(Datum = as.Date(Datum, format = \"%Y%m%d\")) #|&gt; \n  # Schneide das df auf den gewuenschten Zeitraum zu\n  # filter(Datum &gt;= depo_start, Datum &lt;=  depo_end) # das Komma hat die gleiche Funktion wie ein &\n\n# In dieser Auswertung werden nur Personen zu Fuss betrachtet!\n# it select werden spalten ausgewaehlt oder eben fallengelassen\ndepo &lt;- depo |&gt; dplyr::select(-c(Velo_IN, Velo_OUT))\n\n# Berechnen des Totals, da dieses in den Daten nicht vorhanden ist\ndepo &lt;- depo|&gt;\n  mutate(Total = Fuss_IN + Fuss_OUT)\n\n# Entferne die NA's in dem df.\ndepo &lt;- na.omit(depo)\n\n\n\nAufgabe 2: Meteodaten\n\n# Einlesen\nmeteo &lt;- read.csv(\"datasets/fallstudie_s/WPZ/order_105742_data.txt\", sep = \";\")\n\n# Datentypen setzen\n# Das Datum wird als Integer erkannt. Zuerst muss es in Text umgewaldelt werden aus dem dann\n# das eigentliche Datum herausgelesen werden kann\nmeteo &lt;- transform(meteo, time = as.Date(as.character(time), \"%Y%m%d\"))\n\n# Die eigentlichen Messwerte sind alle nummerisch\nmeteo &lt;- meteo|&gt;\n  mutate(tre200jx = as.numeric(tre200jx))|&gt;\n  mutate(rre150j0 = as.numeric(rre150j0))|&gt;\n  mutate(sremaxdv = as.numeric(sremaxdv)) |&gt; \n  filter(time &gt;= depo_start, time &lt;=  depo_end) # schneide dann auf Untersuchungsdauer\n\n# Was ist eigentlich Niederschlag:\n# https://www.meteoschweiz.admin.ch/home/wetter/wetterbegriffe/niederschlag.html\n\n# Filtere Werte mit NA\nmeteo &lt;- meteo |&gt;\n  filter(!is.na(stn)) |&gt;\n  filter(!is.na(time))|&gt;\n  filter(!is.na(tre200jx))|&gt;\n  filter(!is.na(rre150j0))|&gt;\n  filter(!is.na(sremaxdv))\n# Pruefe ob alles funktioniert hat\nstr(meteo)\nsum(is.na(meteo)) # zeigt die Anzahl NA's im data.frame an\n\n\n\nAufgabe 3: Datenvorverarbeitung (Mutationen)\n\n#.################################################################################################\n# 2. VORBEREITUNG DER DATEN #####\n#.################################################################################################\n\n# 2.1 Convinience Variablen ####\n# fuege dem Dataframe (df) die Wochentage hinzu\ndepo &lt;- depo |&gt; \n  mutate(Wochentag = weekdays(Datum)) |&gt; \n  # R sortiert die Levels aplhabetisch. Da das in unserem Fall aber sehr unpraktisch ist,\n  # muessen die Levels manuell manuell bestimmt werden\n  mutate(Wochentag = base::factor(Wochentag, \n                            levels = c(\"Montag\", \"Dienstag\", \"Mittwoch\", \n                                       \"Donnerstag\", \"Freitag\", \"Samstag\", \"Sonntag\"))) |&gt; \n  # Werktag oder Wochenende hinzufuegen\n  mutate(Wochenende = if_else(Wochentag == \"Montag\" | Wochentag == \"Dienstag\" | \n                           Wochentag == \"Mittwoch\" | Wochentag == \"Donnerstag\" | \n                           Wochentag == \"Freitag\", \"Werktag\", \"Wochenende\"))|&gt;\n  #Kalenderwoche hinzufuegen\n  mutate(KW= isoweek(Datum))|&gt;\n  # monat und Jahr\n  mutate(Monat = month(Datum)) |&gt; \n  mutate(Jahr = year(Datum))\n\n#Lockdown \n# Hinweis: ich mache das nachgelagert, da ich die Erfahrung hatte, dass zu viele \n# Operationen in einem Schritt auch schon mal durcheinander erzeugen koennen.\n# Hinweis II: Wir packen alle Phasen (normal, die beiden Lockdowns und Covid aber ohne Lockdown)\n# in eine Spalte --&gt; long ist schoener als wide\ndepo &lt;- depo |&gt;\n  mutate(Phase = if_else(Datum &gt;= lock_1_start_2020 & Datum &lt;= lock_1_end_2020,\n                         \"Lockdown_1\",\n                         if_else(Datum &gt;= lock_2_start_2021 & Datum &lt;= lock_2_end_2021,\n                                 \"Lockdown_2\",\n                                 if_else(Datum&gt;= (lock_1_start_2020 - years(1)) & Datum &lt; lock_1_start_2020,\n                                         \"Normal\", \n                                         if_else(Datum &gt; lock_2_end_2021,\n                                                 \"Post\", \"Pre\")))))\n\n# hat das gepklappt?!\nunique(depo$Phase)\n\n# aendere die Datentypen\ndepo &lt;- depo |&gt; \n  mutate(Wochenende = as.factor(Wochenende)) |&gt; \n  mutate(KW = factor(KW)) |&gt; \n  # mit factor() koennen die levels direkt einfach selbst definiert werden.\n  # wichtig: speizfizieren, dass aus R base, ansonsten kommt es zu einem \n  # mix-up mit anderen packages\n  mutate(Phase = base::factor(Phase, levels = c(\"Pre\", \"Normal\", \"Lockdown_1\", \"Lockdown_2\", \"Post\")))\n\nstr(depo)\n  \n# Fuer einige Auswertungen muss auf die Stunden als nummerischer Wert zurueckgegriffen werden\ndepo$Stunde &lt;- as.numeric(format(as.POSIXct(depo$Zeit,format=\"%H:%M:%S\"),\"%H\"))\n\n# ersetze 0 Uhr mit 24 Uhr (damit wir besser rechnen können)\ndepo$Stunde[depo$Stunde == 0] &lt;- 24\nunique(depo$Stunde)\ntypeof(depo$Stunde)\n\n# Die Daten wurden kalibriert. Wir runden sie fuer unserer Analysen auf Ganzzahlen\ndepo$Total &lt;- round(depo$Total, digits = 0)\ndepo$Fuss_IN &lt;- round(depo$Fuss_IN, digits = 0)\ndepo$Fuss_OUT &lt;- round(depo$Fuss_OUT, digits = 0)\n\n# 2.2 Tageszeit hinzufuegen ####\n\n# Einteilung Standort Zuerich\nLatitude &lt;- 47.38598\nLongitude &lt;- 8.50806\n\n# Zur Berechnung der Tageslaege muessen wir zuerst den Start und das Ende der Sommer-\n# zeit definieren\n# https://www.schulferien.org/schweiz/zeit/zeitumstellung/\n\nSo_start_2017 &lt;- as.Date(\"2017-03-26\") \nSo_end_2017 &lt;- as.Date(\"2017-10-29\") \nSo_start_2018 &lt;- as.Date(\"2018-03-25\") \nSo_end_2018 &lt;- as.Date(\"2018-10-28\") \nSo_start_2019 &lt;- as.Date(\"2019-03-31\") \nSo_end_2019 &lt;- as.Date(\"2019-10-27\") \nSo_start_2020 &lt;- as.Date(\"2020-03-29\")\nSo_end_2020 &lt;- as.Date(\"2020-10-25\")\nSo_start_2021 &lt;- as.Date(\"2021-03-28\")\nSo_end_2021 &lt;- as.Date(\"2021-10-31\")\nSo_start_2022 &lt;- as.Date(\"2022-03-27\")\nSo_end_2022 &lt;- as.Date(\"2022-10-30\")\n\n# Welche Zeitzone haben wir eigentlich?\n# Switzerland uses Central European Time (CET) during the winter as standard time, \n# which is one hour ahead of Coordinated Universal Time (UTC+01:00), and \n# Central European Summer Time (CEST) during the summer as daylight saving time, \n# which is two hours ahead of Coordinated Universal Time (UTC+02:00).\n# https://en.wikipedia.org/wiki/Time_in_Switzerland\n\n# Was sind Astronomische Dämmerung und Golden Hour ueberhaupt?\n# https://sunrisesunset.de/sonne/schweiz/zurich-kreis-1-city/\n# https://www.rdocumentation.org/packages/suncalc/versions/0.5.0/topics/getSunlightTimes\n\n# Wir arbeiten mit folgenden Variablen:\n# \"nightEnd\" : night ends (morning astronomical twilight starts)\n# \"goldenHourEnd\" : morning golden hour (soft light, best time for photography) ends\n# \"goldenHour\" : evening golden hour starts\n# \"night\" : night starts (dark enough for astronomical observations)\n\nlumidata &lt;-\n  getSunlightTimes(\n    date = seq.Date(depo_start, depo_end, by = 1),\n    keep = c(\"nightEnd\", \"goldenHourEnd\", \"goldenHour\", \"night\"),\n    lat = Latitude,\n    lon = Longitude,\n    tz = \"CET\")\n\nlumidata &lt;- lumidata |&gt; \n  mutate(Jahreszeit = ifelse(date &gt;= So_start_2017 & date &lt;=  So_end_2017 |\n                               date &gt;= So_start_2018 & date &lt;=  So_end_2018 |\n                               date &gt;= So_start_2019 & date &lt;=  So_end_2019 |\n                               date &gt;= So_start_2020 & date &lt;= So_end_2020 |\n                               date &gt;= So_start_2021 & date &lt;= So_end_2021 |\n                               date &gt;= So_start_2022 & date &lt;= So_end_2022, \n                               \"Sommerzeit\", \"Winterzeit\"))\n\n# CH ist im Im Sommer CET + 1. \n# Darum auf alle relevanten Spalten eine Stunde addieren\n# hinweis: ich verzichte hier auf ifelse, da es einfacher und nachvollziehbarer scheint,\n# hier mit einem filter die betreffenden Spalten zu waehlen\nlumidata_So &lt;- lumidata |&gt; \n  filter(Jahreszeit==\"Sommerzeit\") |&gt; \n  mutate(nightEnd = nightEnd + hours(1),\n         goldenHourEnd =  goldenHourEnd + hours(1),\n         goldenHour = goldenHour + hours(1),\n         night = night + hours(1))\n\nlumidata_Wi &lt;- lumidata |&gt; \n  filter(Jahreszeit==\"Winterzeit\") \n# verbinde sommer- und winterzeit wieder\nlumidata &lt;- rbind(lumidata_So, lumidata_Wi) |&gt; \n  arrange(date)\n\n# change data type\nlumidata$date &lt;- as.Date(lumidata$date, format= \"%Y-%m-%d\")\n\n# drop unnecessary cols\nlumidata &lt;- lumidata |&gt; dplyr::select(-lat, -lon)\n\n# jetzt haben wir alle noetigen Angaben zu Sonnenaufgang, Tageslaenge usw. \n# diese Angaben koennen wir nun mit unseren Zaehldaten verbinden:\ndepo &lt;- left_join(depo,lumidata, by = c(\"Datum\" =\"date\"))\n\n# aendere alle Zeit- und Datumsangaben so, dass sie gleich sind und miteinander verrechnet werden können.\ndepo &lt;- depo |&gt; \n  mutate(datetime = paste(Datum, Zeit)) |&gt; \n  mutate(datetime = as.POSIXct(datetime, format = \"%Y-%m-%d  %H:%M:%S\"))|&gt; \n  mutate(nightEnd = as.POSIXct(nightEnd)) |&gt; \n  mutate(goldenHourEnd = as.POSIXct(goldenHourEnd)) |&gt; \n  mutate(goldenHourEnd = goldenHourEnd + hours(1)) |&gt; \n  mutate(goldenHour = as.POSIXct(goldenHour)) |&gt; \n  mutate(goldenHour = goldenHour - hours(1)) |&gt; \n  mutate(night = as.POSIXct(night))\n\n# im naechsten Schritt weise ich den Stunden die Tageszeiten Morgen, Tag, Abend und Nacht zu.\n# diese Zuweisung basiert auf der Einteilung gem. suncalc und eigener Definition.\ndepo &lt;- depo|&gt;\n  mutate(Tageszeit = if_else(datetime &gt;= nightEnd & datetime &lt;= goldenHourEnd, \"Morgen\",\n                             ifelse(datetime &gt; goldenHourEnd & datetime &lt; goldenHour, \"Tag\",\n                                    ifelse(datetime &gt;= goldenHour & datetime &lt;= night,\n                                           \"Abend\",\n                                           \"Nacht\")))) |&gt;\n  mutate(Tageszeit = factor(Tageszeit, levels = c(\n    \"Morgen\", \"Tag\", \"Abend\", \"Nacht\")))\n\n# # behalte die relevanten Var\ndepo &lt;- depo |&gt; dplyr::select(-nightEnd, -goldenHourEnd, -goldenHour, -night)\n\n#Plotte zum pruefn ob das funktioniert hat\np &lt;- ggplot(depo, aes(y = Datum, color = Tageszeit, x = Stunde))+\n  geom_jitter()+\n  scale_color_manual(values=mycolors)\n\nplotly::ggplotly(p)\n\n\n# bei mir hat der Zusatz der Tageszeit noch zu einigen NA-Wertren gefueht. \n# Diese loesche ich einfach:\ndepo &lt;- na.omit(depo)\n# hat das funktioniert?\nsum(is.na(depo))\n\n\n\nAufgabe 4: Aggregierung der Stundendaten\n\n# 2.4 Aggregierung der Stundendaten zu ganzen Tagen ####\n# Zur Berechnung von Kennwerten ist es hilfreich, wenn neben den Stundendaten auch auf Ganztagesdaten\n# zurueckgegriffen werden kann\n# hier werden also pro Nutzergruppe und Richtung die Stundenwerte pro Tag aufsummiert\ndepo_d &lt;- depo |&gt; \n  group_by(Datum, Wochentag, Wochenende, KW, Monat, Jahr, Phase) |&gt; \n  summarise(Total = sum(Fuss_IN + Fuss_OUT), \n            Fuss_IN = sum(Fuss_IN),\n            Fuss_OUT = sum(Fuss_OUT)) \n# Wenn man die Convinience Variablen als grouping variable einspeisst, dann werden sie in \n# das neue df uebernommen und muessen nicht nochmals hinzugefuegt werden\n# pruefe das df\nhead(depo_d)\n\n# nun gruppieren wir nicht nur nach Tag sondern auch noch nach Tageszeit\ndepo_daytime &lt;- depo |&gt; \n  group_by(Datum, Wochentag, Wochenende, KW, Monat, Jahr, Phase, Tageszeit) |&gt; \n  summarise(Total = sum(Fuss_IN + Fuss_OUT), \n            Fuss_IN = sum(Fuss_IN),\n            Fuss_OUT = sum(Fuss_OUT)) \n\n\n# Gruppiere die Werte nach Monat\ndepo_m &lt;- depo |&gt; \n  group_by(Jahr, Monat) |&gt; \n  summarise(Total = sum(Total)) \n# sortiere das df aufsteigend (nur das es sicher stimmt)\ndepo_m &lt;- as.data.frame(depo_m)\ndepo_m[\n  with(depo_m, order(Jahr, Monat)),]\ndepo_m &lt;- depo_m |&gt; \n  mutate(Ym = paste(Jahr, Monat)) |&gt; # und mache eine neue Spalte, in der Jahr und\n  mutate(Ym= lubridate::ym(Ym)) # formatiere als Datum\n\n# Gruppiere die Werte nach Monat und TAGESZEIT\ndepo_m_daytime &lt;- depo |&gt; \n  group_by(Jahr, Monat, Tageszeit) |&gt; \n  summarise(Total = sum(Total)) \n# sortiere das df aufsteigend (nur das es sicher stimmt)\ndepo_m_daytime &lt;- as.data.frame(depo_m_daytime)\ndepo_m_daytime[\n  with(depo_m_daytime, order(Jahr, Monat)),]\ndepo_m_daytime &lt;- depo_m_daytime |&gt; \n  mutate(Ym = paste(Jahr, Monat)) |&gt; # und mache eine neue Spalte, in der Jahr und\n  mutate(Ym= lubridate::ym(Ym)) # formatiere als Datum"
  },
  {
    "objectID": "fallstudie_n/1_Vorbemerkung.html",
    "href": "fallstudie_n/1_Vorbemerkung.html",
    "title": "1. Vorbemerkung",
    "section": "",
    "text": "Aktuell dient diese Plattform für die BiEc Fallstudie - Profil N einzig der Bereitstellung von Aufgaben die von euch im Rahmen dieses Fallstudienprojekts erarbeitet werden sollen. Die Aufgaben werden in den meisten Fällen mit Code-Beispielen erläutert oder benötigten Code-snippets resp. Funktionen werden mitgeliefert. Im Laufe des Semesters werden hier ausserdem häppchenweise (mögliche) Lösungen zu den Aufgaben aufgeschaltet. Alles grundlegende Material und alle Unterlagen zu den theoretischen Inputs sind weiterhin und ausschliesslich im Moodlekurs Research Methods - Fallstudie BiEc zu finden. Die für die Aufgaben benötigten Datengrundlagen sind ebenfalls im entsprechenden Abschnitt auf Moodle zu finden. Frohes Schaffen!\n\nIm Rahmen der Fallstudie werden wir einige R Packages brauchen. Wir empfehlen, diese bereits vor der ersten Lektion zu installieren. Analog Kapitel 1 könnt ihr mit nachstehendem Code alle noch nicht installierten packages automatisch installieren.\n\n\nipak &lt;- function(pkg){\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if(length(new.pkg)){install.packages(new.pkg, dependencies = TRUE)}\n}\n\npackages &lt;- c(\"adehabitatHR\", \"bbmle\", \"car\", \"cowplot\", \"DHARMa\", \"dplyr\", \n\"ggeffects\", \"ggplot2\", \"ggspatial\", \"glmmTMB\", \"gstat\", \"kableExtra\", \"lme4\", \n\"MASS\", \"MuMIn\", \"pastecs\", \"performance\", \"PerformanceAnalytics\", \"psych\", \n\"readr\", \"rms\", \"ROCR\", \"sf\", \"sjPlot\", \"sjstats\", \"terra\") \n\nipak(packages)"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#projektaufbau-rstudio-projekte",
    "href": "fallstudie_n/2_Datenverarbeitung.html#projektaufbau-rstudio-projekte",
    "title": "2. Daten(vor)verarbeitung",
    "section": "Projektaufbau RStudio-Projekte",
    "text": "Projektaufbau RStudio-Projekte\nVor den eigentlichen Auswertungen müssen einige vorbereitende Arbeiten unternommen werden. Die Zeit, die man hier investiert, wird in der späteren Projektphase um ein vielfaches eingespart. Im Skript soll die Ordnerstruktur des Projekts genannt werden, damit der Arbeitsvorgang auf verschiedenen Rechnern reproduzierbar ist.\nArbeitet mit Projekten, da diese sehr einfach ausgetauscht und somit auch reproduziert werden önnen; es gibt keine absoluten Arbeitspfade sondern nur relative. Der Datenimport (und auch der Export) kann mithilfe dieser relativen Pfaden stark vereinfacht werden. Projekte helfen alles am richtigen Ort zu behalten. (mehr zur Arbeit mit Projekten: Link)"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufbau-von-r-skripten",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufbau-von-r-skripten",
    "title": "2. Daten(vor)verarbeitung",
    "section": "Aufbau von R-Skripten",
    "text": "Aufbau von R-Skripten\nIm Kopf des Skripts zuerst immer den Titel des Projekts sowie die Autor:innen des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklärt werden, wer die Datenherrschaft hat (Rehdaten: Forschungsgruppe WILMA).\n\n#.##################################################################################\n# Daten(vor)verarbeitung Fallstudie WPZ  ####\n# Modul Research Methods, HS22. Autor/in ####\n#.##################################################################################\n\nBeschreibt zudem folgendes:\n\nOrdnerstruktur; ich verwende hier den Projektordner mit den Unterordnern:\n\nSkripts\nData\nResults\nPlots\n\nVerwendete Daten\n\nEin Skript soll in R eigentlich immer nach dem selbem Schema aufgebaut sein. Dieses Schema beinhaltet (nach dem bereits erwähnten Kopf des Skripts) 4 Kapitel:\n\nDatenimport\nDatenvorverarbeitung\nAnalyse\nVisualisierung\n\nBereitet euer Skript also nach dieser Struktur vor. Nutzt für den Text, welcher nicht Code ist, vor dem Text das Symbol #. Wenn ihr den Text als Titel definieren wollt, der die grobe Struktur des Skripts absteckt, baut in wie in folgendem Beispiel auf:\n\n\n#.###################################################################################\n# METADATA ####\n#.###################################################################################\n# Datenherkunft ####\n# ...\n\n#.###################################################################################\n# 1. DATENIMPORT ####\n#.###################################################################################"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#libraries-laden",
    "href": "fallstudie_n/2_Datenverarbeitung.html#libraries-laden",
    "title": "2. Daten(vor)verarbeitung",
    "section": "Libraries laden",
    "text": "Libraries laden\n\n\nlibrary(\"readr\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#daten-laden",
    "href": "fallstudie_n/2_Datenverarbeitung.html#daten-laden",
    "title": "2. Daten(vor)verarbeitung",
    "section": "Daten laden",
    "text": "Daten laden\nHerunterladen der Daten der Feldaufnahmen von Moodle (Aufgabe3_Feldaufnahmen_alle_Gruppen.zip), Einlesen, Sichtung der Datensätze und der Datentypen.\n\n\nCode\ndf_team1 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Felderhebungen_Waldstruktur.csv\", delim = \";\")\n\ndf_team2 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Felderhebung_11102022_gr3.csv\", delim = \";\")\n\ndf_team3 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Felderhebung_Waldstruktur_TEAM3_pink_Gruppe 7.csv\", delim = \";\")\n\ndf_team4 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Felderhebungen_Team4_Blau_221011.csv\",delim = \";\")\n\ndf_team5 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Felderhebung TEAM 5.csv\", delim = \";\")\n\ndf_team6 &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Feldaufnahmen_alle_Gruppen/Team6_Felderhebung.csv\", delim = \";\")\n\n# hier können die Probekreise mit den Angaben zur Anzahl Rehlokalisationen und der \n# LIDAR-basierten Ableitung der Waldstruktur eingelesen werden\n\ndf_reh &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Reh_Waldstruktur_221013.csv\", delim = \";\")\nstr(df_reh)\n## spc_tbl_ [305 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ Anz_reh_lokalisationen: num [1:305] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ x                     : num [1:305] 684900 684900 684900 684900 684875 ...\n##  $ y                     : num [1:305] 237100 237125 237150 237175 237075 ...\n##  $ DG_us                 : num [1:305] 0.0903 0.2717 0.468 0.7407 0.1811 ...\n##  $ DG_os                 : num [1:305] 0.908 0.959 0.871 0.986 0.86 ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   Anz_reh_lokalisationen = col_double(),\n##   ..   x = col_double(),\n##   ..   y = col_double(),\n##   ..   DG_us = col_double(),\n##   ..   DG_os = col_double()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n# Die eingelesenen Datensätze anschauen und versuchen zu einem Gesamtdatensatz  \n# verbinden. Ist der Output zufriedenstellend?\n\ndf_gesamt &lt;- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6)\nstr(df_gesamt)\n## spc_tbl_ [150 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ Kreis (r 12.5m)               : num [1:150] 0 1 2 3 4 5 6 7 8 9 ...\n##  $ X                             : num [1:150] 684900 684875 684875 684875 684850 ...\n##  $ Y                             : num [1:150] 237175 237125 237175 237250 237225 ...\n##  $ Deckungsgrad Rubus sp. [%]    : num [1:150] 1 72.5 15 25 15 25 30 60 85 65 ...\n##  $ DG Strauchschicht [%] (0.5-3m): num [1:150] 50 57.5 65 45 65 70 75 65 65 35 ...\n##  $ DG Baumschicht [%] (ab 3m)    : num [1:150] 90 55 85 65 70 80 80 50 60 70 ...\n##  $ Kreis                         : num [1:150] NA NA NA NA NA NA NA NA NA NA ...\n##  $ x                             : num [1:150] NA NA NA NA NA NA NA NA NA NA ...\n##  $ y                             : num [1:150] NA NA NA NA NA NA NA NA NA NA ...\n##  $ Deckungsgrad Rubus sp [%]     : num [1:150] NA NA NA NA NA NA NA NA NA NA ...\n##  $ DG Rubus sp. [%]              : num [1:150] NA NA NA NA NA NA NA NA NA NA ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   `Kreis (r 12.5m)` = col_double(),\n##   ..   X = col_double(),\n##   ..   Y = col_double(),\n##   ..   `Deckungsgrad Rubus sp. [%]` = col_double(),\n##   ..   `DG Strauchschicht [%] (0.5-3m)` = col_double(),\n##   ..   `DG Baumschicht [%] (ab 3m)` = col_double()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-1",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-1",
    "title": "2. Daten(vor)verarbeitung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\n\n1.1 Einfügen zusätzliche Spalte pro Datensatz mit der Gruppenzugehörigkeit (Team1-6)\n1.2 Spaltenumbenennung damit die Bezeichungen in allen Datensätzen gleich sind und der Gesamtdatensatz zusammengefügt werden kann\n\n→ Befehle mutate und rename, mit pipes (alt: |&gt;, neu: |&gt;) in einem Schritt möglich\n\n\n\n\nCode\n#.#################################################################################\n# 2. DATENVORVERARBEITUNG #####\n#.#################################################################################\n\ndf_team1 &lt;- df_team1 |&gt;\n  mutate(team = \"team1\") |&gt;\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team2 &lt;- df_team2 |&gt;\n  mutate(team = \"team2\") |&gt;\n  rename(KreisID = \"Kreis\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team3 &lt;- df_team3 |&gt;\n  mutate(team = \"team3\") |&gt;\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         X = \"x\",\n         Y = \"y\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team4 &lt;- df_team4 |&gt;\n  mutate(team = \"team4\") |&gt;\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team5 &lt;- df_team5 |&gt;\n  mutate(team = \"team5\") |&gt;\n  rename(KreisID = \"Kreis\",\n         X = \"x\",\n         Y = \"y\",\n         DG_Rubus = \"Deckungsgrad Rubus sp [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team6 &lt;- df_team6 |&gt;\n  mutate(team = \"team6\") |&gt;\n  rename(KreisID = \"Kreis\",\n         X = \"x\",\n         Y = \"y\",\n         DG_Rubus = \"DG Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-2",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-2",
    "title": "2. Daten(vor)verarbeitung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nZusammenführen der Teildatensätze zu einem Datensatz\n\n\nCode\ndf_gesamt &lt;- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6)"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-3",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-3",
    "title": "2. Daten(vor)verarbeitung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nVerbinden (join) des Datensatzes der Felderhebungen mit dem Datensatz der Rehe.\nZiel: ein Datensatz mit allen Kreisen der Felderhebung, angereichert mit den Umweltvariablen Understory und Overstory aus den LIDAR-Daten (DG_us, DG_os) aus dem Rehdatensatz. –&gt; Welche Art von join? Welche Spalten zum Verbinden (by = ?) der Datensätze\n\n\nCode\n\ndf_with_LIDAR &lt;- left_join(df_gesamt,df_reh, by = c(\"X\" = \"x\", \"Y\" = \"y\"))"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-4",
    "href": "fallstudie_n/2_Datenverarbeitung.html#aufgabe-4",
    "title": "2. Daten(vor)verarbeitung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nScatterplot der korrespondondierenden Umweltvariablen aus den Felderhebungen gegen die Umweltvariablen aus den LIDAR-Daten erstellen (zusätzlich Einfärben der Gruppen und Regressionslinie darüberlegen).\n\n\nCode\n#.#####################################################################################\n# 4. VISUALISERUNG #####\n#.#####################################################################################\n\nggplot(df_with_LIDAR, aes(DG_us, DG_Strauchschicht, color = team)) + \n  geom_point() + \n  stat_smooth(method = \"lm\")\n\n\n\n\n\nCode\n\nwrite_delim(df_with_LIDAR, \"df_with_lidar.csv\", delim = \";\")"
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#libraries-laden",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#libraries-laden",
    "title": "3. Berechnung Homeranges",
    "section": "Libraries laden",
    "text": "Libraries laden\n\n\nlibrary(\"sf\")\nlibrary(\"terra\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"terra\")\nlibrary(\"adehabitatHR\")\nlibrary(\"ggspatial\")"
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#daten-einlesen",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#daten-einlesen",
    "title": "3. Berechnung Homeranges",
    "section": "Daten einlesen",
    "text": "Daten einlesen\nEinlesen des Gesamtdatensatzes von Moodle, Sichtung des Datensatzes und der Datentypen\n\nRehe &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe3_Homeranges_Rehe_landforst_20221024.csv\", delim = \";\")\n\nstr(Rehe)"
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-1",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-1",
    "title": "3. Berechnung Homeranges",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nIm Datensatz Rehe eine neue Spalte mit Datum und Zeit in einer Spalte kreieren. Beim Format hat sich ein Fehler eingeschlichen. Findet ihr ihn?\n\n\nRehe &lt;- Rehe |&gt;\n  mutate(UTC_DateTime = as.POSIXct(paste(UTC_Date, UTC_Time), \n                                   format = \"%Y-%m-%d %H:%M:%S\"))"
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-2",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-2",
    "title": "3. Berechnung Homeranges",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nMit den folgenden Zeilen können die GPS-Punkte visualisiert werden\n\nRehe_sf &lt;- st_as_sf(Rehe, coords = c(\"X\", \"Y\"), crs = 21781)\n\nRE13 &lt;- filter(Rehe_sf, TierID == \"RE13\")\n\nplot(RE13[\"TierID\"])\n\n\n\n\nHier einige Zeilen Code, um eine HomeRange zu berechnen.\nHerumschrauben an den Einstellungen von:\n\nan der Ausdehung, resp. prozentualer Anteil Punkte in der HR (Funktion getverticeshr)\nhref (in der Funktion kernelUD)\n\n→ Ziel: eine Karte erstellen mit der Visualiserung mindestens einer HR\nBerechnung von href nach: Pebsworth, Morgan, und Huffman (2012).\n\n\nRE13_xy &lt;- st_coordinates(RE13)\n\nRE13_sp &lt;- as(RE13[\"TierID\"], \"Spatial\")\n\nsigma &lt;- 0.5*(sd(RE13_xy[,1])+sd(RE13_xy[,2]))                              \nn &lt;- nrow(RE13)\nhref &lt;- sigma * n^(-1/6)*0.9  \n\n# scaled reference: href * 0.9\n\nkud &lt;- kernelUD(RE13_sp, h=href, grid=25)             \n\n# Berechnung der Home Range (95% Isopleth)\n\nhomerange &lt;- getverticeshr(kud, percent=95)             \n\n# Schreibt HR in den oben beschriebenen Ordner (als Shapefile)\n\nhr &lt;- st_as_sf(homerange)\n\nst_write(hr, dsn= \"Results\", layer=\"HR_RE13\", driver=\"ESRI Shapefile\", delete_layer = TRUE)\n## Deleting layer `HR_RE13' using driver `ESRI Shapefile'\n## Writing layer `HR_RE13' to data source `Results' using driver `ESRI Shapefile'\n## Writing 1 features with 2 fields and geometry type Polygon.\n\n\n# mit diesem Befehl kann die HR geplottet werden\n\nggplot(hr) + \n  geom_sf(size = 1, alpha = 0.3, color = \"red\", fill=\"red\") +\n  coord_sf(datum = sf::st_crs(21781))+\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position=\"none\"\n  )\n\n\n\n\n# und die Punkte der GPS-Lokalisationen darüber gelegt werden \n\nggplot(hr) + \n  geom_sf(size = 1, alpha = 0.3, color = \"red\", fill=\"red\") +\n  geom_sf(data = RE13, aes(fill = \"red\")) +\n  coord_sf(datum = sf::st_crs(21781))+\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position=\"none\"\n)\n\n\n\n\nCode um die Homerange auf der Landeskarte 1:25000 zu plotten. Transparenz kann mit alpha angepasst werden.\n\n\npk25_wpz &lt;- rast(\"datasets/fallstudie_n/pk25_wpz.tif\")\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) +\n  annotation_spatial(pk25_wpz) +\n  geom_sf(size = 1, alpha = 0.3) +\n  geom_sf(data = RE13, aes(fill = \"red\")) +\n  coord_sf(datum = sf::st_crs(21781))+\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position=\"none\"\n  )\n\n\n\n## \n|---------|---------|---------|---------|\n=========================================\n                                          \n\nNachbauen des Sampling Grids mit den Kreisen (Wird als Grundlage für Extraktion der Umweltvariablen innerhalb der Homeranges benötigt)\n\nAusdehnung des Grids basiert auf hr\nCellsize des Grids: 25m\n\n\n\nx25 &lt;- st_make_grid(hr, 25, what = \"centers\")\ngrid_plot &lt;- st_buffer(x25, 12.5)\n\nggplot(grid_plot, color = \"black\", fill=NA) + \n  geom_sf() +\n  geom_sf(data = RE13, color = \"blue\",  ) +\n  geom_sf(data = hr, color = \"red\", fill = NA, size = 2) +\n  coord_sf(datum = 21781)+\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position=\"none\"\n  )"
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-3",
    "href": "fallstudie_n/3_Berechnung_Homeranges.html#aufgabe-3",
    "title": "3. Berechnung Homeranges",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nTesten der Variablen der Vegetationsschichten von letzter Woche auf einen linearen Zusammenhang (Korrelation; Funktion cor.test). DG_Baumschicht vs. DG_os / DG_Strauchschicht vs. DG_us aus dem Datensatz df_with_lidar den wir letzte Woche erstellt haben\nDie Theorie zu Korrelation folgt erst ab 31.10.\n\n\n\ndf_with_lidar &lt;- read_delim(\"datasets/fallstudie_n/df_with_lidar.csv\", delim =\";\")\n\ncor.test(~ DG_Strauchschicht+DG_us, data = df_with_lidar, method=\"pearson\")\n## \n##  Pearson's product-moment correlation\n## \n## data:  DG_Strauchschicht and DG_us\n## t = 2.6184, df = 148, p-value = 0.009753\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.05190297 0.35858387\n## sample estimates:\n##       cor \n## 0.2104143\n\n\n\n\n\nPebsworth, Paula A, Hanna R Morgan, und Michael A Huffman. 2012. „Evaluating home range techniques: use of Global Positioning System (GPS) collar data from chacma baboons“. Primates 53: 345–55."
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html#libraries-laden",
    "href": "fallstudie_n/4_Multivariate_Modelle.html#libraries-laden",
    "title": "4. Einstieg Multivariate Modelle - Übung",
    "section": "Libraries laden",
    "text": "Libraries laden\n\nlibrary(\"sf\")\nlibrary(\"terra\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"PerformanceAnalytics\")\nlibrary(\"pastecs\")\nlibrary(\"car\")\nlibrary(\"psych\")"
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-1",
    "href": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-1",
    "title": "4. Einstieg Multivariate Modelle - Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nEinlesen des Gesamtdatensatzes für die Multivariate Analyse von Moodle\n\nSichtung des Datensatzes, der Variablen und der Datentypen\nKontrolle wieviele Rehe in diesem Datensatz enthalten sind\n\n\n\nCode\n\nDF_mod &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe4_Datensatz_Habitatnutzung_Modelle_20221031_moodle.csv\", delim = \";\")\n\nstr(DF_mod)\n\nclass(DF_mod$time_of_day)\n\ntable(DF_mod$id)\n\nDF_mod |&gt; \n  group_by(id) |&gt; \n  summarize(anzahl = n())\n\nlength(unique(DF_mod$id))"
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-2",
    "href": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-2",
    "title": "4. Einstieg Multivariate Modelle - Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nUnterteilung des Datensatzes in Teildatensätze entsprechend der Tageszeit\n\n\nCode\nDF_mod_night &lt;- DF_mod |&gt;\n  filter(time_of_day == \"night\")\n\nDF_mod_day &lt;- DF_mod |&gt;\n  filter(time_of_day == \"day\")\n\n# Kontrolle\ntable(DF_mod_night$time_of_day)\n## \n## night \n##  4185\n\ntable(DF_mod_day$time_of_day)\n## \n##  day \n## 4185"
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-3",
    "href": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-3",
    "title": "4. Einstieg Multivariate Modelle - Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nErstellen von Density Plots der Präsenz / Absenz in Abhängigkeit der unabhängigen Variablen. Diese Übung dient einer ersten groben Einschätzung der Wirkung der Umweltvariablen auf die abhängige Variable (Präsenz/Absenz in unserem Fall)\n\n# Ein Satz Density Plots für den Tagesdatensatz und einer für den Nachtdatensatz \n\npar(mfrow=c(3,3), mar=c(4, 4, 3, 3)) # Vorbereitung Raster für Plots\n\n# innerhalb des for()-loops die Nummern der gewünschten Spalten einstellen\n\nfor (i in 6:12) {                           \n  dp  &lt;-  DF_mod_day |&gt; filter(pres_abs == 1) |&gt; pull(i)\n  dp &lt;- density(dp)\n  da  &lt;-  DF_mod_day |&gt; filter(pres_abs == 0) |&gt; pull(i)\n  da &lt;- density(da)\n  plot(0,0, type=\"l\", xlim=range(c(dp$x,da$x)), ylim=range(dp$y,da$y), \n       xlab=names(DF_mod_day[i]), ylab=\"Density\")\n  lines(dp$x, dp$y, col=\"blue\")             # Präsenz\n  lines(da$x, da$y, col=\"red\")              # Absenz\n}"
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-4",
    "href": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-4",
    "title": "4. Einstieg Multivariate Modelle - Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nTesten eurer erklärenden Variablen auf Normalverteilung (nur kontinuierliche)\n\n\nCode\n\n# klassischer Weg mit shapiro-wilk (mehrere Spalten, verschiedenene statistische\n# Kenngrössen werden angezeigt. Normalverteilung: Wert ganz unten. p&gt;0.05 = ja)\n\nround(stat.desc(DF_mod_day[6:12], basic= F, norm= T), 3)\n##                slope dist_road_all dist_road_only dist_build forest_prop     us\n## median        13.442        28.248         33.693    129.838       0.632  0.059\n## mean          15.055        41.289         46.374    154.502       0.590  0.120\n## SE.mean        0.158         0.642          0.665      1.625       0.005  0.002\n## CI.mean.0.95   0.311         1.259          1.303      3.186       0.010  0.005\n## var          104.994      1725.368       1848.647  11050.895       0.107  0.023\n## std.dev       10.247        41.538         42.996    105.123       0.328  0.151\n## coef.var       0.681         1.006          0.927      0.680       0.555  1.259\n## skewness       0.753         1.914          1.666      0.631      -0.366  1.674\n## skew.2SE       9.945        25.285         22.003      8.341      -4.832 22.115\n## kurtosis      -0.042         4.250          3.147     -0.537      -1.094  3.061\n## kurt.2SE      -0.279        28.079         20.790     -3.545      -7.226 20.226\n## normtest.W     0.942         0.800          0.837      0.943       0.919  0.792\n## normtest.p     0.000         0.000          0.000      0.000       0.000  0.000\n##                   os\n## median         0.754\n## mean           0.586\n## SE.mean        0.006\n## CI.mean.0.95   0.013\n## var            0.173\n## std.dev        0.416\n## coef.var       0.710\n## skewness      -0.388\n## skew.2SE      -5.124\n## kurtosis      -1.586\n## kurt.2SE     -10.481\n## normtest.W     0.791\n## normtest.p     0.000\n\n# empfohlener Weg\n\nggplot(DF_mod_day, aes(slope)) + geom_histogram(aes(y=..density..), color = \"black\", fill = \"white\") + \n  stat_function(fun = dnorm, args = list(mean = mean(DF_mod_day$slope, na.rm = T),sd = sd(DF_mod_day$slope, na.rm = T)), color = \"black\",size = 1)\n\n\n\n\n\n\n\nCode\n\n# Aufgabe 4: die Verteilung bei einem Teildatensatz zu testen reicht, \n# denn die verwendeten Kreise sind die selben am Tag und in der Nacht, \n# nur die Nutzung durch das Reh nicht"
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-5",
    "href": "fallstudie_n/4_Multivariate_Modelle.html#aufgabe-5",
    "title": "4. Einstieg Multivariate Modelle - Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nExplorative Analysen der Variablen mit Scatterplots / Scatterplotmatrizen\n\nZu Scatterplots und Scatterplotmatrizen gibt es viele verschiedene Funktionen / Packages, schaut im Internet und sucht euch eines welches euch passt.\nTesten der Korrelation zwischen den Variablen (Parametrisch oder nicht-parametrische Methode? Ausserdem: gewisse Scatterplotmatrizen zeigen euch die Koeffizenten direkt an)\n\n\n\nCode\n\nchart.Correlation(DF_mod_day[6:12], histogram=TRUE, pch=19, method = \"kendall\")\n\n\n\n\n\nCode\n\n#?chart.Correlation\n\npairs.panels(DF_mod_day[6:12], \n             method = \"kendall\", # correlation method\n             hist.col = \"#00AFBB\",\n             density = TRUE,  # show density plots\n             ellipses = TRUE # show correlation ellipses\n             )\n\n\n\n\n\nCode\n\n# Aufgabe 5: die Korrelation bei einem Teildatensatz zu testen reicht, \n# denn die verwendeten Kreise sind die selben am Tag und in der Nacht, \n# nur die Nutzung durch das Reh nicht."
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#libraries-laden",
    "href": "fallstudie_n/5_Variablenselektion.html#libraries-laden",
    "title": "5. Variablenselektion Multivariate Modelle / Habitatselektionsmodell",
    "section": "Libraries laden",
    "text": "Libraries laden\n\n\nlibrary(\"sp\")\nlibrary(\"raster\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"PerformanceAnalytics\")\nlibrary(\"pastecs\")\nlibrary(\"lme4\")\nlibrary(\"bbmle\")\nlibrary(\"MuMIn\")\nlibrary(\"MASS\")"
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#variablenselektion",
    "href": "fallstudie_n/5_Variablenselektion.html#variablenselektion",
    "title": "5. Variablenselektion Multivariate Modelle / Habitatselektionsmodell",
    "section": "Variablenselektion",
    "text": "Variablenselektion\n→ Vorgehen analog Coppes u. a. (2017)"
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#aufgabe-1",
    "href": "fallstudie_n/5_Variablenselektion.html#aufgabe-1",
    "title": "5. Variablenselektion Multivariate Modelle / Habitatselektionsmodell",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nMit dem folgenden Code kann eine simple Korrelationsmatrix aufgebaut werden.\n\n\nDF_mod &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe4_Datensatz_Habitatnutzung_Modelle_20221031_moodle.csv\", delim = \";\")\n\nDF_mod_day &lt;- DF_mod |&gt;\n  filter(time_of_day == \"day\")\n\n\nround(cor(DF_mod_day[,6:12], method = \"kendall\"),2)\n##                slope dist_road_all dist_road_only dist_build forest_prop    us\n## slope           1.00          0.13           0.16       0.11        0.18  0.22\n## dist_road_all   0.13          1.00           0.84       0.02       -0.08 -0.06\n## dist_road_only  0.16          0.84           1.00       0.03       -0.08 -0.04\n## dist_build      0.11          0.02           0.03       1.00        0.42  0.12\n## forest_prop     0.18         -0.08          -0.08       0.42        1.00  0.31\n## us              0.22         -0.06          -0.04       0.12        0.31  1.00\n## os              0.34         -0.06          -0.04       0.22        0.53  0.42\n##                   os\n## slope           0.34\n## dist_road_all  -0.06\n## dist_road_only -0.04\n## dist_build      0.22\n## forest_prop     0.53\n## us              0.42\n## os              1.00\n\n# hier kann die Schwelle für die Korrelation gesetzt werden, 0.7 ist liberal / \n# 0.5 konservativ\n\ncor &lt;- round(cor(DF_mod_day[,6:12], method = \"kendall\"),2) \ncor[abs(cor)&lt;0.7] &lt;-0\ncor\n##                slope dist_road_all dist_road_only dist_build forest_prop us os\n## slope              1          0.00           0.00          0           0  0  0\n## dist_road_all      0          1.00           0.84          0           0  0  0\n## dist_road_only     0          0.84           1.00          0           0  0  0\n## dist_build         0          0.00           0.00          1           0  0  0\n## forest_prop        0          0.00           0.00          0           1  0  0\n## us                 0          0.00           0.00          0           0  1  0\n## os                 0          0.00           0.00          0           0  0  1"
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#aufgabe-2",
    "href": "fallstudie_n/5_Variablenselektion.html#aufgabe-2",
    "title": "5. Variablenselektion Multivariate Modelle / Habitatselektionsmodell",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nSkalieren der Variablen, damit ihr Einfluss vergleichbar wird (Befehl scale(); Problem verschiedene Skalen der Variablen (bspw. Neigung in Grad, Distanz in Metern)); Umwandeln der Reh-ID in einen Faktor, damit dieser als Random Factor ins Model eingespiesen werden kann.\n\n\nDF_mod_day &lt;- DF_mod_day\n  mutate(slope_scaled = scale(slope),\n         us_scaled = scale(us),\n         os_scaled = scale(os),\n         forest_prop_scaled = scale(forest_prop),\n         dist_road_all_scaled = scale(dist_road_all),\n         dist_road_only_scaled = scale(dist_road_only),\n         dist_build_scaled = scale(dist_build),\n         id = as.factor(id))\n## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'scale': object 'slope' not found"
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#aufgabe-3",
    "href": "fallstudie_n/5_Variablenselektion.html#aufgabe-3",
    "title": "5. Variablenselektion Multivariate Modelle / Habitatselektionsmodell",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nSelektion der Variablen in einem univariaten Model\nEin erstes GLMM (Generalized Linear Mixed Effects Modell) aufbauen: Funktion und Modelformel\n\nwichtige Seite auf der man viele Hilfestellungen zu GLMM’s finden kann.\n\n\n\n# wir werden das package lme4 mit der Funktion glmer verwenden \n\n# die Hilfe von glmer aufrufen: ?glmer\n\n# glmer(formula, data = , family = binomial)\n\n# 1) formula: \n# Abhängige Variable ~ Erklärende Variable + Random Factor \n# In unseren Modellen kontrollieren wir für individuelle Unterschiede bei den Rehen \n# indem wir einen Random Factor definieren =&gt; (1 | id) \n\n# 2) data: \n# euer Datensatz\n\n# 3) family: \n# hier binomial\n\n# warum binomial? Verteilung Daten der Abhängigen Variable Präsenz/Absenz \n\nggplot(DF_mod_day, aes(pres_abs)) + \n  geom_histogram()\n\n\n\n\n# --&gt; Binäre Verteilung =&gt; Binomiale Verteilung mit n = 1 \n\n# und wie schaut die Verteilung der Daten der Abhängigen Variable Nutzungsintensität \n# (nmb, werden wir in diesem Kurs aber nicht genauer anschauen) aus?"
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#aufgabe-4",
    "href": "fallstudie_n/5_Variablenselektion.html#aufgabe-4",
    "title": "5. Variablenselektion Multivariate Modelle / Habitatselektionsmodell",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nMit der GLMM Formel bauen wir in einem ersten Schritt eine univariate Variablenselektion auf.\nAls abhängige Variable verwenden wir die Präsenz/Absenz der Rehe in den Kreisen\n\n# Die erklärende Variable in m1 ist die erste Variable der korrelierenden Variablen\n# Die erklärende Variable in m2 ist die zweite Variable der korrelierenden Variablen\n\nm1 &lt;- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id), data = DF_mod_day, \n            family = binomial)\nm2 &lt;- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id), data = DF_mod_day, \n            family = binomial)\n\n# mit dieser Funktion können die Modellergebnisse inspiziert werden\nsummary(m1)\n\n# Mit dieser Funktion kann der Informationgehalt der beiden Modelle gegeneinander \n# abgeschätzt werden\nbbmle::AICtab(m1, m2)\n\n# tieferer AIC -&gt; besser (AIC = Akaike information criterion)\n\n# ==&gt; dieses Vorgehen muss nun für alle korrelierten Variablen für jeden Teildatensatz \n# (Tag/Nacht) durchgeführt werden, um nur noch nicht (R &lt; 0.7) korrelierte Variablen \n# in das Modell einfliessen zu lassen \n\n\n\nCode\nm1 &lt;- glmer(pres_abs ~ dist_road_all_scaled + (1 | id), data = DF_mod_day, family = binomial)\n## Error in eval(predvars, data, env): object 'dist_road_all_scaled' not found\nm2 &lt;- glmer(pres_abs ~ dist_road_only_scaled + (1 | id), data = DF_mod_day, family = binomial)\n## Error in eval(predvars, data, env): object 'dist_road_only_scaled' not found\n\nsummary(m1)\n## Error in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'summary': object 'm1' not found\n\nbbmle::AICtab(m1, m2)\n## Error in ICtab(..., mnames = mnames, type = \"AIC\"): object 'm1' not found\n\n# tieferer AIC -&gt; besser (AIC = Akaike information criterion) -&gt; als deltaAIC\n# ausgewiesen besser == Distanz zu Strassen\n\n# ==&gt; dieses Vorgehen muss nun für alle korrelierten Variablen für jeden\n# Teildatensatz (geringe Störung/starke Störung) durchgeführt werden, um nur\n# noch nicht (R &lt; 0.7) korrelierte Variablen in das Modell einfliessen zu\n# lassen"
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion.html#aufgabe-5",
    "href": "fallstudie_n/5_Variablenselektion.html#aufgabe-5",
    "title": "5. Variablenselektion Multivariate Modelle / Habitatselektionsmodell",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nSelektion der Variablen in einem multivariaten Model\nMit folgendem Code kann eine automatisierte Variablenselektion (dredge-Funktion) und ein Modelaveraging aufgebaut werden (siehe auch Stats-Skript von J.Dengler & Team)\n\n# hier wird die Formel für die dredge-Funktion vorbereitet (die Variablen V1-V6 \n# sind jene welche nach der univariaten Variablenselektion noch übrig bleiben)  \n\nf &lt;- pres_abs ~ \n  V1 +\n  V2 +\n  V3 +\n  V4 +\n  V5 +\n  V6 \n\n# in diesem Befehl kommt der Random-Factor (das Reh) hinzu und es wird eine Formel \n# daraus gemacht\n\nf_dredge &lt;- paste(c(f, \"+ (1 | id)\"), collapse = \" \") |&gt; as.formula()\n\n# Das Modell mit dieser Formel ausführen\n\nm &lt;- glmer(f_dredge, data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n\n# Das Modell in die dredge-Funktion einfügen (siehe auch unbedingt ?dredge)\n\nall_m &lt;- dredge(m)\n\n# Importance values der einzelnen Variablen (Gibt an, wie bedeutsam eine bestimmte \n# Variable ist, wenn man viele verschiedene Modelle vergleicht (multimodel inference))\n\nsw(all_m)\n\n# Schlussendlich wird ein Modelaverage durchgeführt (Schwellenwert für das delta-AIC = 2)\n\navgmodel &lt;- model.avg(all_m, rank=\"AICc\", subset = delta &lt; 2)\nsummary(avgmodel)\n\n# ==&gt; für den Nachtdatensatz muss der gleiche Prozess der Variablenselektion \n# durchgespielt werden.\n\n\n\nCode\n# hier wird die Formel für die dredge-Funktion vorbereitet (die Variablen V1-V6 \n# sind jene welche nach der univariaten Variablenselektion noch übrig bleiben)  \n\nf &lt;- pres_abs ~ \n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  forest_prop_scaled +\n  dist_road_only_scaled +\n  dist_build_scaled \n\n# inn diesem Befehl kommt der Random-Factor (das Reh) hinzu und es wird eine Formel \n# daraus gemacht\n\nf_dredge &lt;- paste(c(f, \"+ (1 | id)\"), collapse = \" \") |&gt; as.formula()\n\n# Das Modell mit dieser Formel ausführen\n\nm &lt;- glmer(f_dredge, data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n## Error in eval(predvars, data, env): object 'slope_scaled' not found\n\n# Das Modell in die dredge-Funktion einfügen (siehe auch unbedingt ?dredge)\n\nall_m &lt;- dredge(m)\n## Error in nobs(global.model): object 'm' not found\n\n# Importance values der einzelnen Variablen (Gibt an, wie bedeutsam eine bestimmte \n# Variable ist, wenn man viele verschiedene Modelle vergleicht (multimodel inference))\n\nsw(all_m)\n## Error in sw(all_m): object 'all_m' not found\n\n\n# Schlussendlich wird ein Modelaverage durchgeführt (Schwellenwert für das delta-AIC = 2)\n\navgmodel &lt;- model.avg(all_m, rank=\"AICc\", subset = delta &lt; 2)\n## Error in model.avg(all_m, rank = \"AICc\", subset = delta &lt; 2): object 'all_m' not found\nsummary(avgmodel)\n## Error in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'summary': object 'avgmodel' not found\n\n\n\n\n\n\nCoppes, Joy, Judith Ehrlacher, Dominik Thiel, Rudi Suchant, und Veronika Braunisch. 2017. „Outdoor recreation causes effective habitat reduction in capercaillie Tetrao urogallus: a major threat for geographically restricted populations“. Journal of avian biology 48 (12): 1583–94."
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#libraries-laden",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#libraries-laden",
    "title": "6. Modellgüte und -diagnostics MM / Habitatselektionsmodell",
    "section": "Libraries laden",
    "text": "Libraries laden\nNeue packages die wir für die Modelle und die Diagnostics brauchen\n\nlibrary(\"lme4\")\nlibrary(\"bbmle\")\nlibrary(\"MuMIn\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"DHARMa\")\nlibrary(\"car\")\nlibrary(\"MASS\")\nlibrary(\"ROCR\")\nlibrary(\"sjPlot\")\nlibrary(\"ggeffects\")\nlibrary(\"sjstats\")\nlibrary(\"cowplot\")\nlibrary(\"gstat\")"
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#ausgangslage",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#ausgangslage",
    "title": "6. Modellgüte und -diagnostics MM / Habitatselektionsmodell",
    "section": "Ausgangslage",
    "text": "Ausgangslage\n\nDer Modellfit von letzter Woche als Ausgangspunkt für die heutige Übung\n\nHier sollte investiert werden. Backscale Estimates, Modelldiagnostik aufmöbeln,\n\n\nDF_mod_day &lt;- read_delim(\"datasets/fallstudie_n/Aufgabe4_Datensatz_Habitatnutzung_Modelle_20221031_moodle.csv\", delim = \";\") |&gt;\n  filter(time_of_day == \"day\") |&gt;\n  mutate(slope_scaled = scale(slope),\n         us_scaled = scale(us),\n         os_scaled = scale(os),\n         forest_prop_scaled = scale(forest_prop),\n         dist_road_all_scaled = scale(dist_road_all),\n         dist_road_only_scaled = scale(dist_road_only),\n         dist_build_scaled = scale(dist_build),\n         id = as.factor(id))\n\nf &lt;- pres_abs ~\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  forest_prop_scaled +\n  dist_road_only_scaled +\n  dist_build_scaled\n\nf &lt;- paste(c(f, \"+ (1 | id)\"), collapse = \" \") |&gt; as.formula()\n\nm_day &lt;- glmer(f, data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nall_m &lt;- dredge(m_day)\n\navgmodel &lt;- model.avg(all_m, rank=\"AICc\", subset = delta &lt; 2)\nsummary(avgmodel)\n\n\nDie Modellresultate aus dem avgmodel sind grundätzlich die finalen Resultate die bereits interpretiert werden könnten. Allerdings funktionieren die Diagnosetests und die Darstellung der Resultate mit diesem gemittelten Modell nicht sehr gut, weshalb wir einen re-fit mit glmer machen müssen (an den Resultaten ändert sich dadurch nichts)\n\n\n\n# hier zum Vergleich, dass die Resulate sich nur marginal verändern \n\nsummary(avgmodel)\nsummary(m_day)"
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-1",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-1",
    "title": "6. Modellgüte und -diagnostics MM / Habitatselektionsmodell",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nBerechung der AUC (area under the receiver operating characteristic curve) = Mass der Modellgüte\nFür die Berechnung des AUC findet ihr weiterführende Informationen unter: Link"
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-2",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-2",
    "title": "6. Modellgüte und -diagnostics MM / Habitatselektionsmodell",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nInterpretieren der Modell-Residuen mittels Tests auf verschiedene Aspekte\n\nModel testing for over/underdispersion, zeroinflation and spatial autocorrelation following the DHARMa package.\nunbedingt die Vignette des DHARMa-Package konsultieren: Link"
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-3",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-3",
    "title": "6. Modellgüte und -diagnostics MM / Habitatselektionsmodell",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nGraphische Darstellung der Modellresultate"
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-4",
    "href": "fallstudie_n/6_Guete_und_Diagnostics.html#aufgabe-4",
    "title": "6. Modellgüte und -diagnostics MM / Habitatselektionsmodell",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nErmittlung des individuellen Beitrags der einzelen Variablen im Gesamtmodell\n\nBestimmen delta AIC nach Coppes u. a. (2017) → Vergleich des Gesamtmodells gegenüber einem Modell ohne die entsprechende Variable.\nAuftrag auf den 21.11.2022: Kurze Vorstellung der Modellresultate & Diagnostics im Plenum und Diskussion der Ergebnisse (keine PP-Präsentation nötig)\n\n\n\n\n\nCoppes, Joy, Judith Ehrlacher, Dominik Thiel, Rudi Suchant, und Veronika Braunisch. 2017. „Outdoor recreation causes effective habitat reduction in capercaillie Tetrao urogallus: a major threat for geographically restricted populations“. Journal of avian biology 48 (12): 1583–94."
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "Literaturverzeichnis",
    "section": "",
    "text": "Bochud, Murielle, Angéline Chatelan, Juan-Manuel Blanco, and Sigrid\nMaria Beer-Borst. 2017. “Anthropometric Characteristics and\nIndicators of Eating and Physical Activity Behaviors in the Swiss Adult\nPopulation: Results from menuCH 2014-2015.”\n\n\nBorcard, Daniel, François Gillet, Pierre Legendre, et al. 2011.\nNumerical Ecology with r. Vol. 2. Springer.\n\n\nCoppes, Joy, Judith Ehrlacher, Dominik Thiel, Rudi Suchant, and Veronika\nBraunisch. 2017. “Outdoor Recreation Causes Effective Habitat\nReduction in Capercaillie Tetrao Urogallus: A Major Threat for\nGeographically Restricted Populations.” Journal of Avian\nBiology 48 (12): 1583–94.\n\n\nGilgen, Kurt, and Alma Sartoris. 2010. “Empfehlung Zur Planung von\nWindenergieanlagen: Die Anwendung von Raumplanungsinstrumenten Und\nKriterien Zur Standortwahl.” Eidgenössisches Departement für\nUmwelt, Verkehr, Energie und Kommunikation UVEK.\n\n\nKovic, Marko. 2014. “Je Weniger Ausländer, Desto Mehr Ja-Stimmen?\nWirklich?” Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich.\n\n\nPebsworth, Paula A, Hanna R Morgan, and Michael A Huffman. 2012.\n“Evaluating Home Range Techniques: Use of Global Positioning\nSystem (GPS) Collar Data from Chacma Baboons.” Primates\n53: 345–55.\n\n\nScherler, Patrick. 2020. “Drivers of Departure and Prospecting in\nDispersing Juvenile Red Kites (Milvus Milvus).” PhD thesis,\nUniversity of Zurich.\n\n\nTegou, Leda-Ioanna, Heracles Polatidis, and Dias A. Haralambopoulos.\n2010. “Environmental Management Framework for Wind Farm Siting:\nMethodology and Case Study.” Journal of Environmental\nManagement 91 (11): 2134–47. https://doi.org/10.1016/j.jenvman.2010.05.010.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data\nScience. O’Reilly. https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093."
  }
]